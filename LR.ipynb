{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import api\n",
    "import helpers\n",
    "import evaluation\n",
    "import implementations\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, x, ids = helpers.load_csv_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_iters = 50000\n",
    "gamma = 0.000001\n",
    "lambda_ = 0.01\n",
    "initial_w = np.zeros(90)\n",
    "cut = 0.43\n",
    "poly = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=18107.08379776745, gradient=18829.915146286345\n",
      "Current iteration=1000, the loss=5979.582465774494, gradient=206.28658199850702\n",
      "Current iteration=2000, the loss=5941.561079214698, gradient=189.9607103662368\n",
      "Current iteration=3000, the loss=5906.767085513815, gradient=183.30204879305467\n",
      "Current iteration=4000, the loss=5874.206949738064, gradient=177.7389436616125\n",
      "Current iteration=5000, the loss=5843.476112672788, gradient=172.98469990547457\n",
      "Current iteration=6000, the loss=5814.283316112961, gradient=168.8286014140079\n",
      "Current iteration=7000, the loss=5786.4168491627015, gradient=165.11457512145014\n",
      "Current iteration=8000, the loss=5759.721204985001, gradient=161.72768539860851\n",
      "Current iteration=9000, the loss=5734.08065203965, gradient=158.58400716668447\n",
      "Current iteration=10000, the loss=5709.407545396087, gradient=155.6229979071434\n",
      "Current iteration=11000, the loss=5685.633953321424, gradient=152.8017054416106\n",
      "Current iteration=12000, the loss=5662.705648076034, gradient=150.09032830780535\n",
      "Current iteration=13000, the loss=5640.577812039431, gradient=147.46878114633193\n",
      "Current iteration=14000, the loss=5619.212005490234, gradient=144.92401780371054\n",
      "Current iteration=15000, the loss=5598.574069921707, gradient=142.44793950587638\n",
      "Current iteration=16000, the loss=5578.632724912977, gradient=140.03577054934314\n",
      "Current iteration=17000, the loss=5559.358672984905, gradient=137.6848216095404\n",
      "Current iteration=18000, the loss=5540.724066437051, gradient=135.39358200987323\n",
      "Current iteration=19000, the loss=5522.702220463177, gradient=133.1610901848373\n",
      "Current iteration=20000, the loss=5505.267482481063, gradient=130.98653175628877\n",
      "Current iteration=21000, the loss=5488.395190540902, gradient=128.86901337133912\n",
      "Current iteration=22000, the loss=5472.06167399325, gradient=126.80746206532864\n",
      "Current iteration=23000, the loss=5456.24426661979, gradient=124.80060574069549\n",
      "Current iteration=24000, the loss=5440.921315581228, gradient=122.84699930651144\n",
      "Current iteration=25000, the loss=5426.072178769693, gradient=120.945071020673\n",
      "Current iteration=26000, the loss=5411.677208949201, gradient=119.09317275351452\n",
      "Current iteration=27000, the loss=5397.717726208266, gradient=117.2896251720242\n",
      "Current iteration=28000, the loss=5384.175981568512, gradient=115.53275394385189\n",
      "Current iteration=29000, the loss=5371.035114809828, gradient=113.82091621988279\n",
      "Current iteration=30000, the loss=5358.27910921905, gradient=112.1525183412914\n",
      "Current iteration=31000, the loss=5345.892745393422, gradient=110.52602641889179\n",
      "Current iteration=32000, the loss=5333.86155563252, gradient=108.93997155054983\n",
      "Current iteration=33000, the loss=5322.171779930735, gradient=107.39295126224279\n",
      "Current iteration=34000, the loss=5310.810324170807, gradient=105.8836284611708\n",
      "Current iteration=35000, the loss=5299.764720817572, gradient=104.4107288774655\n",
      "Current iteration=36000, the loss=5289.023092203914, gradient=102.97303769413571\n",
      "Current iteration=37000, the loss=5278.574116367706, gradient=101.56939584098751\n",
      "Current iteration=38000, the loss=5268.406995319531, gradient=100.19869625839361\n",
      "Current iteration=39000, the loss=5258.511425579643, gradient=98.8598803141692\n",
      "Current iteration=40000, the loss=5248.877570806292, gradient=97.5519344721019\n",
      "Current iteration=41000, the loss=5239.496036336797, gradient=96.27388725465933\n",
      "Current iteration=42000, the loss=5230.357845471052, gradient=95.02480650715103\n",
      "Current iteration=43000, the loss=5221.4544173401955, gradient=93.80379694985223\n",
      "Current iteration=44000, the loss=5212.77754621807, gradient=92.60999799357958\n",
      "Current iteration=45000, the loss=5204.319382148217, gradient=91.4425817894953\n",
      "Current iteration=46000, the loss=5196.072412773479, gradient=90.30075148311802\n",
      "Current iteration=47000, the loss=5188.0294462682805, gradient=89.18373964398397\n",
      "Current iteration=48000, the loss=5180.183595285314, gradient=88.0908068450575\n",
      "Current iteration=49000, the loss=5172.528261838419, gradient=87.02124036914411\n",
      "Current iteration=0, the loss=51147.33045351836, gradient=22369.076308761178\n",
      "Current iteration=1000, the loss=41922.11559905679, gradient=1594.1805961658906\n",
      "Current iteration=2000, the loss=40075.854892388204, gradient=1164.1323183249965\n",
      "Current iteration=3000, the loss=39003.390580117586, gradient=919.7571271600325\n",
      "Current iteration=4000, the loss=38309.65818995789, gradient=752.6648342425578\n",
      "Current iteration=5000, the loss=37834.04391703732, gradient=630.9205174091862\n",
      "Current iteration=6000, the loss=37493.38361327895, gradient=539.467617146891\n",
      "Current iteration=7000, the loss=37240.05384637546, gradient=469.48080014835784\n",
      "Current iteration=8000, the loss=37045.16919325936, gradient=415.23665799076105\n",
      "Current iteration=9000, the loss=36890.48522903308, gradient=372.799901304945\n",
      "Current iteration=10000, the loss=36764.11002491478, gradient=339.3521545315752\n",
      "Current iteration=11000, the loss=36658.09120555136, gradient=312.8132283957497\n",
      "Current iteration=12000, the loss=36566.995774264295, gradient=291.61426279347876\n",
      "Current iteration=13000, the loss=36487.04350356652, gradient=274.55509470979075\n",
      "Current iteration=14000, the loss=36415.56199874521, gradient=260.7100856873381\n",
      "Current iteration=15000, the loss=36350.63520325365, gradient=249.3627012991551\n",
      "Current iteration=16000, the loss=36290.87167631718, gradient=239.95786091403585\n",
      "Current iteration=17000, the loss=36235.2489201691, gradient=232.06592293012827\n",
      "Current iteration=18000, the loss=36183.00706614004, gradient=225.3548195493415\n",
      "Current iteration=19000, the loss=36133.575218663886, gradient=219.56825567220267\n",
      "Current iteration=20000, the loss=36086.51977666215, gradient=214.5086096255933\n",
      "Current iteration=21000, the loss=36041.507768261676, gradient=210.02355405832378\n",
      "Current iteration=22000, the loss=35998.280578865415, gradient=205.99563518526924\n",
      "Current iteration=23000, the loss=35956.634959848474, gradient=202.33419648889432\n",
      "Current iteration=24000, the loss=35916.40919134866, gradient=198.9691474026508\n",
      "Current iteration=25000, the loss=35877.472928136616, gradient=195.84617276172395\n",
      "Current iteration=26000, the loss=35839.71969954028, gradient=192.9230595188229\n",
      "Current iteration=27000, the loss=35803.06133630572, gradient=190.16688491049558\n",
      "Current iteration=28000, the loss=35767.42380594479, gradient=187.5518659491186\n",
      "Current iteration=29000, the loss=35732.744083893915, gradient=185.05771505077743\n",
      "Current iteration=30000, the loss=35698.96779063688, gradient=182.66838226357592\n",
      "Current iteration=31000, the loss=35666.047398116745, gradient=180.3710924751506\n",
      "Current iteration=32000, the loss=35633.9408612462, gradient=178.1556076107019\n",
      "Current iteration=33000, the loss=35602.61056823937, gradient=176.013660472159\n",
      "Current iteration=34000, the loss=35572.02253105293, gradient=173.93851960006987\n",
      "Current iteration=35000, the loss=35542.14575738315, gradient=171.92465424576895\n",
      "Current iteration=36000, the loss=35512.95176048328, gradient=169.9674759246286\n",
      "Current iteration=37000, the loss=35484.414174011225, gradient=168.0631386308955\n",
      "Current iteration=38000, the loss=35456.50844723569, gradient=166.20838405488433\n",
      "Current iteration=39000, the loss=35429.21160197582, gradient=164.40042137927827\n",
      "Current iteration=40000, the loss=35402.50203716722, gradient=162.63683369062045\n",
      "Current iteration=41000, the loss=35376.35937033664, gradient=160.9155049127724\n",
      "Current iteration=42000, the loss=35350.76430781598, gradient=159.23456259363712\n",
      "Current iteration=43000, the loss=35325.69853744935, gradient=157.5923329625072\n",
      "Current iteration=44000, the loss=35301.144639002036, gradient=155.98730550453405\n",
      "Current iteration=45000, the loss=35277.0860085834, gradient=154.4181049326812\n",
      "Current iteration=46000, the loss=35253.506794235574, gradient=152.8834689227788\n",
      "Current iteration=47000, the loss=35230.391840480006, gradient=151.38223034934347\n",
      "Current iteration=48000, the loss=35207.72664010286, gradient=149.91330304546742\n",
      "Current iteration=49000, the loss=35185.49729183668, gradient=148.47567032971781\n",
      "Current iteration=0, the loss=5241.578979394306, gradient=5369.356062091761\n",
      "Current iteration=1000, the loss=2401.3253624879017, gradient=247.64904944786386\n",
      "Current iteration=2000, the loss=2371.2719963194604, gradient=142.82490683911297\n",
      "Current iteration=3000, the loss=2352.5460520765428, gradient=132.10461705718944\n",
      "Current iteration=4000, the loss=2336.0980145987173, gradient=124.56473890178414\n",
      "Current iteration=5000, the loss=2321.3983099289776, gradient=118.0523932507109\n",
      "Current iteration=6000, the loss=2308.1372880728754, gradient=112.37763932544118\n",
      "Current iteration=7000, the loss=2296.072257563551, gradient=107.40817035286506\n",
      "Current iteration=8000, the loss=2285.0107154477705, gradient=103.03243171597869\n",
      "Current iteration=9000, the loss=2274.799055368525, gradient=99.15668210731128\n",
      "Current iteration=10000, the loss=2265.313930590766, gradient=95.70243647703825\n",
      "Current iteration=11000, the loss=2256.45564147883, gradient=92.60412889480442\n",
      "Current iteration=12000, the loss=2248.143059497703, gradient=89.8070631884949\n",
      "Current iteration=13000, the loss=2240.309715164669, gradient=87.26564849794242\n",
      "Current iteration=14000, the loss=2232.900769458198, gradient=84.94189683760682\n",
      "Current iteration=15000, the loss=2225.870658106527, gradient=82.80415358712895\n",
      "Current iteration=16000, the loss=2219.1812505348603, gradient=80.82603080088519\n",
      "Current iteration=17000, the loss=2212.8004042817956, gradient=78.98551464033855\n",
      "Current iteration=18000, the loss=2206.700824768372, gradient=77.26422080448874\n",
      "Current iteration=19000, the loss=2200.8591619844847, gradient=75.64677485248055\n",
      "Current iteration=20000, the loss=2195.2552918695665, gradient=74.12029737558787\n",
      "Current iteration=21000, the loss=2189.8717423321136, gradient=72.67397686484138\n",
      "Current iteration=22000, the loss=2184.6932330260142, gradient=71.29871572948878\n",
      "Current iteration=23000, the loss=2179.7063049526264, gradient=69.98683721233132\n",
      "Current iteration=24000, the loss=2174.8990212534263, gradient=68.731842922338\n",
      "Current iteration=25000, the loss=2170.2607246155812, gradient=67.52821238569283\n",
      "Current iteration=26000, the loss=2165.7818398391687, gradient=66.37123743599976\n",
      "Current iteration=27000, the loss=2161.453712536755, gradient=65.25688545776293\n",
      "Current iteration=28000, the loss=2157.268476822039, gradient=64.18168649797695\n",
      "Current iteration=29000, the loss=2153.2189463196205, gradient=63.142640098808776\n",
      "Current iteration=30000, the loss=2149.2985239869495, gradient=62.13713840594317\n",
      "Current iteration=31000, the loss=2145.5011271531193, gradient=61.16290269415416\n",
      "Current iteration=32000, the loss=2141.821124901605, gradient=60.21793094246096\n",
      "Current iteration=33000, the loss=2138.253285496681, gradient=59.30045450110713\n",
      "Current iteration=34000, the loss=2134.792732008169, gradient=58.408902234389046\n",
      "Current iteration=35000, the loss=2131.434904651151, gradient=57.54187080778982\n",
      "Current iteration=36000, the loss=2128.1755286458456, gradient=56.69810002403784\n",
      "Current iteration=37000, the loss=2125.010586633102, gradient=55.8764523083085\n",
      "Current iteration=38000, the loss=2121.9362948650955, gradient=55.07589560438967\n",
      "Current iteration=39000, the loss=2118.9490825381113, gradient=54.29548907681866\n",
      "Current iteration=40000, the loss=2116.045573752416, gradient=53.53437112351968\n",
      "Current iteration=41000, the loss=2113.222571679053, gradient=52.79174929334634\n",
      "Current iteration=42000, the loss=2110.4770445896384, gradient=52.066891776565704\n",
      "Current iteration=43000, the loss=2107.806113466716, gradient=51.359120196547195\n",
      "Current iteration=44000, the loss=2105.207040961827, gradient=50.66780348013539\n",
      "Current iteration=45000, the loss=2102.677221508629, gradient=49.99235262436457\n",
      "Current iteration=46000, the loss=2100.2141724309918, gradient=49.332216209959974\n",
      "Current iteration=47000, the loss=2097.8155259125065, gradient=48.68687653881299\n",
      "Current iteration=48000, the loss=2095.4790217154773, gradient=48.05584629443919\n",
      "Current iteration=49000, the loss=2093.202500555187, gradient=47.43866564223013\n",
      "Current iteration=0, the loss=48507.82598994609, gradient=13992.173164920536\n",
      "Current iteration=1000, the loss=42492.11651496151, gradient=1130.0842589218646\n",
      "Current iteration=2000, the loss=41642.317908126315, gradient=773.2263903861497\n",
      "Current iteration=3000, the loss=41161.07859727618, gradient=627.0983046466237\n",
      "Current iteration=4000, the loss=40822.36053934109, gradient=542.2277031136239\n",
      "Current iteration=5000, the loss=40560.19750038311, gradient=484.3454979783788\n",
      "Current iteration=6000, the loss=40347.17515576793, gradient=440.09307361888426\n",
      "Current iteration=7000, the loss=40169.52459935251, gradient=403.7119787581505\n",
      "Current iteration=8000, the loss=40019.11077894857, gradient=372.54648611959095\n",
      "Current iteration=9000, the loss=39890.47243472965, gradient=345.2527273972189\n",
      "Current iteration=10000, the loss=39779.614359940126, gradient=321.06453690084504\n",
      "Current iteration=11000, the loss=39683.45822967078, gradient=299.486236441437\n",
      "Current iteration=12000, the loss=39599.55975791139, gradient=280.1602882628708\n",
      "Current iteration=13000, the loss=39525.943273628436, gradient=262.80778382934056\n",
      "Current iteration=14000, the loss=39460.9938379825, gradient=247.19996993058228\n",
      "Current iteration=15000, the loss=39403.381075401434, gradient=233.14342466624478\n",
      "Current iteration=16000, the loss=39352.00253781768, gradient=220.47151421128902\n",
      "Current iteration=17000, the loss=39305.9402172721, gradient=209.03893201467199\n",
      "Current iteration=18000, the loss=39264.42647702214, gradient=198.71788599747285\n",
      "Current iteration=19000, the loss=39226.81700673796, gradient=189.395259205234\n",
      "Current iteration=20000, the loss=39192.56915160378, gradient=180.97040458259136\n",
      "Current iteration=21000, the loss=39161.22442174328, gradient=173.3533873742091\n",
      "Current iteration=22000, the loss=39132.39429111803, gradient=166.4635617362428\n",
      "Current iteration=23000, the loss=39105.74860725794, gradient=160.2284055266235\n",
      "Current iteration=24000, the loss=39081.00608760715, gradient=154.58255814857736\n",
      "Current iteration=25000, the loss=39057.92649343737, gradient=149.46701929674927\n",
      "Current iteration=26000, the loss=39036.30415960841, gradient=144.82847538705897\n",
      "Current iteration=27000, the loss=39015.962625454995, gradient=140.61872715375034\n",
      "Current iteration=28000, the loss=38996.75016394489, gradient=136.7941972353772\n",
      "Current iteration=29000, the loss=38978.53604670204, gradient=133.31550096681167\n",
      "Current iteration=30000, the loss=38961.207414229786, gradient=130.14706725675578\n",
      "Current iteration=31000, the loss=38944.66664572417, gradient=127.2567994710031\n",
      "Current iteration=32000, the loss=38928.829142744864, gradient=124.61576873050848\n",
      "Current iteration=33000, the loss=38913.6214568619, gradient=122.19793402579208\n",
      "Current iteration=34000, the loss=38898.97970409298, gradient=119.97988510039019\n",
      "Current iteration=35000, the loss=38884.84821916332, gradient=117.94060522399589\n",
      "Current iteration=36000, the loss=38871.17841087383, gradient=116.06125182166578\n",
      "Current iteration=37000, the loss=38857.92778656176, gradient=114.32495350987588\n",
      "Current iteration=38000, the loss=38845.05911909135, gradient=112.71662247127978\n",
      "Current iteration=39000, the loss=38832.539734270096, gradient=111.22278132975251\n",
      "Current iteration=40000, the loss=38820.340900243886, gradient=109.83140381025802\n",
      "Current iteration=41000, the loss=38808.43730343279, gradient=108.5317685205129\n",
      "Current iteration=42000, the loss=38796.80659805516, gradient=107.31432520135786\n",
      "Current iteration=43000, the loss=38785.42901834534, gradient=106.17057278087536\n",
      "Current iteration=44000, the loss=38774.28704427921, gradient=105.09294854783973\n",
      "Current iteration=45000, the loss=38763.365113046784, gradient=104.07472774227207\n",
      "Current iteration=46000, the loss=38752.64936969903, gradient=103.1099328499525\n",
      "Current iteration=47000, the loss=38742.127451392495, gradient=102.1932518861695\n",
      "Current iteration=48000, the loss=38731.78830049096, gradient=101.3199649622896\n",
      "Current iteration=49000, the loss=38721.62200248635, gradient=100.48587844622065\n",
      "Current iteration=0, the loss=2046.1704770129586, gradient=2062.663645171133\n",
      "Current iteration=1000, the loss=1288.6535500894167, gradient=284.3277683853397\n",
      "Current iteration=2000, the loss=1233.9272817186272, gradient=208.59945021958046\n",
      "Current iteration=3000, the loss=1195.0499516095335, gradient=186.68076840502164\n",
      "Current iteration=4000, the loss=1163.5320955729223, gradient=168.67967280399728\n",
      "Current iteration=5000, the loss=1137.6814078976602, gradient=153.16071729859055\n",
      "Current iteration=6000, the loss=1116.2551455417613, gradient=139.85361050548653\n",
      "Current iteration=7000, the loss=1098.2865494437797, gradient=128.47704417444558\n",
      "Current iteration=8000, the loss=1083.0320205317616, gradient=118.75322563461322\n",
      "Current iteration=9000, the loss=1069.92259949878, gradient=110.4275242092232\n",
      "Current iteration=10000, the loss=1058.5230649893342, gradient=103.27602257888417\n",
      "Current iteration=11000, the loss=1048.4995110019506, gradient=97.10668076202236\n",
      "Current iteration=12000, the loss=1039.5944601645126, gradient=91.75738149304321\n",
      "Current iteration=13000, the loss=1031.6080859780452, gradient=87.09264969664592\n",
      "Current iteration=14000, the loss=1024.384162728061, gradient=83.00000590285354\n",
      "Current iteration=15000, the loss=1017.7995836486685, gradient=79.38643675847572\n",
      "Current iteration=16000, the loss=1011.7565369690791, gradient=76.17520045666123\n",
      "Current iteration=17000, the loss=1006.1766497416683, gradient=73.30304278062583\n",
      "Current iteration=18000, the loss=1000.9965861726205, gradient=70.71782694391601\n",
      "Current iteration=19000, the loss=996.1647223814826, gradient=68.37654617439377\n",
      "Current iteration=20000, the loss=991.6386202274389, gradient=66.24367428330137\n",
      "Current iteration=21000, the loss=987.3830968134789, gradient=64.28980643090213\n",
      "Current iteration=22000, the loss=983.3687402442171, gradient=62.49054454540551\n",
      "Current iteration=23000, the loss=979.5707614891064, gradient=60.82558644337812\n",
      "Current iteration=24000, the loss=975.9681008038912, gradient=59.27798307436181\n",
      "Current iteration=25000, the loss=972.5427280411748, gradient=57.83353366137161\n",
      "Current iteration=26000, the loss=969.2790914770802, gradient=56.480293441191215\n",
      "Current iteration=27000, the loss=966.1636810371364, gradient=55.20817306260793\n",
      "Current iteration=28000, the loss=963.1846801284939, gradient=54.00861243984856\n",
      "Current iteration=29000, the loss=960.3316864730223, gradient=52.87431500972221\n",
      "Current iteration=30000, the loss=957.5954869595408, gradient=51.79903096174063\n",
      "Current iteration=31000, the loss=954.9678750069659, gradient=50.77738016919882\n",
      "Current iteration=32000, the loss=952.4415015533535, gradient=49.804707314792175\n",
      "Current iteration=33000, the loss=950.009752777114, gradient=48.876963140947964\n",
      "Current iteration=34000, the loss=947.6666491758772, gradient=47.99060691957974\n",
      "Current iteration=35000, the loss=945.4067617932403, gradient=47.14252617744932\n",
      "Current iteration=36000, the loss=943.2251422808423, gradient=46.329970473134566\n",
      "Current iteration=37000, the loss=941.1172641775362, gradient=45.5504966341246\n",
      "Current iteration=38000, the loss=939.0789733271487, gradient=44.80192335612928\n",
      "Current iteration=39000, the loss=937.1064457776479, gradient=44.08229346435324\n",
      "Current iteration=40000, the loss=935.1961518348581, gradient=43.389842457014545\n",
      "Current iteration=41000, the loss=933.3448252038621, gradient=42.72297220989175\n",
      "Current iteration=42000, the loss=931.5494363567105, gradient=42.08022892936294\n",
      "Current iteration=43000, the loss=929.8071694280642, gradient=41.46028461003756\n",
      "Current iteration=44000, the loss=928.1154020702257, gradient=40.8619213895312\n",
      "Current iteration=45000, the loss=926.4716878027696, gradient=40.284018303496744\n",
      "Current iteration=46000, the loss=924.8737404752447, gradient=39.7255400337393\n",
      "Current iteration=47000, the loss=923.3194205284448, gradient=39.18552731515765\n",
      "Current iteration=48000, the loss=921.8067227939171, gradient=38.66308872661063\n",
      "Current iteration=49000, the loss=920.3337656153077, gradient=38.15739363920259\n",
      "Current iteration=0, the loss=32873.891332416526, gradient=8051.898520842027\n",
      "Current iteration=1000, the loss=27431.68064994274, gradient=865.4383496635651\n",
      "Current iteration=2000, the loss=26992.309542431423, gradient=520.7268996076253\n",
      "Current iteration=3000, the loss=26781.257799704596, gradient=413.39627304036463\n",
      "Current iteration=4000, the loss=26630.714000958134, gradient=367.3658616739796\n",
      "Current iteration=5000, the loss=26505.90441069224, gradient=340.9316266090152\n",
      "Current iteration=6000, the loss=26396.262198619825, gradient=322.07184989000854\n",
      "Current iteration=7000, the loss=26297.5261096322, gradient=306.7850041544692\n",
      "Current iteration=8000, the loss=26207.507861933624, gradient=293.5489592868222\n",
      "Current iteration=9000, the loss=26124.840687587177, gradient=281.70184749069324\n",
      "Current iteration=10000, the loss=26048.545688561444, gradient=270.9130033318366\n",
      "Current iteration=11000, the loss=25977.859541472804, gradient=260.9932165162261\n",
      "Current iteration=12000, the loss=25912.156306160887, gradient=251.82034231399513\n",
      "Current iteration=13000, the loss=25850.907081922545, gradient=243.3073734598258\n",
      "Current iteration=14000, the loss=25793.656626876913, gradient=235.38746357818852\n",
      "Current iteration=15000, the loss=25740.008403715776, gradient=228.00631013538995\n",
      "Current iteration=16000, the loss=25689.614220085165, gradient=221.11800179359565\n",
      "Current iteration=17000, the loss=25642.16658912255, gradient=214.68261700296068\n",
      "Current iteration=18000, the loss=25597.39282127177, gradient=208.66476103579427\n",
      "Current iteration=19000, the loss=25555.05029030846, gradient=203.03262989927546\n",
      "Current iteration=20000, the loss=25514.92254076879, gradient=197.75738166257432\n",
      "Current iteration=21000, the loss=25476.81602676494, gradient=192.8126934259054\n",
      "Current iteration=22000, the loss=25440.557342591303, gradient=188.1744342239442\n",
      "Current iteration=23000, the loss=25405.990847700836, gradient=183.82041289403884\n",
      "Current iteration=24000, the loss=25372.976615005005, gradient=179.73017622952796\n",
      "Current iteration=25000, the loss=25341.38864867473, gradient=175.88484217393335\n",
      "Current iteration=26000, the loss=25311.113329367578, gradient=172.26695838433946\n",
      "Current iteration=27000, the loss=25282.048053157283, gradient=168.8603798478491\n",
      "Current iteration=28000, the loss=25254.10003660483, gradient=165.65016129792284\n",
      "Current iteration=29000, the loss=25227.185265110962, gradient=162.62246147480286\n",
      "Current iteration=30000, the loss=25201.227565372268, gradient=159.76445711087416\n",
      "Current iteration=31000, the loss=25176.157785712163, gradient=157.06426507593946\n",
      "Current iteration=32000, the loss=25151.913070460203, gradient=154.51087149468748\n",
      "Current iteration=33000, the loss=25128.436216536105, gradient=152.09406691265917\n",
      "Current iteration=34000, the loss=25105.67510205017, gradient=149.80438677662193\n",
      "Current iteration=35000, the loss=25083.582178122353, gradient=147.6330566346673\n",
      "Current iteration=36000, the loss=25062.114016301584, gradient=145.5719415660085\n",
      "Current iteration=37000, the loss=25041.230904968634, gradient=143.61349943041748\n",
      "Current iteration=38000, the loss=25020.89648896373, gradient=141.75073758931305\n",
      "Current iteration=39000, the loss=25001.077447414926, gradient=139.97717279930964\n",
      "Current iteration=40000, the loss=24981.74320537684, gradient=138.2867940178512\n",
      "Current iteration=41000, the loss=24962.865675434485, gradient=136.674027891803\n",
      "Current iteration=42000, the loss=24944.41902590083, gradient=135.13370672525443\n",
      "Current iteration=43000, the loss=24926.379472645167, gradient=133.66103874365803\n",
      "Current iteration=44000, the loss=24908.72509194625, gradient=132.25158048879464\n",
      "Current iteration=45000, the loss=24891.435652073735, gradient=130.90121119364252\n",
      "Current iteration=46000, the loss=24874.492461571444, gradient=129.60610899869656\n",
      "Current iteration=47000, the loss=24857.878232452542, gradient=128.36272888201518\n",
      "Current iteration=48000, the loss=24841.576956722554, gradient=127.16778218470095\n",
      "Current iteration=49000, the loss=24825.573794827866, gradient=126.01821762185244\n",
      "Current iteration=0, the loss=1023.7783856870392, gradient=1321.0923046620105\n",
      "Current iteration=1000, the loss=467.87935028030773, gradient=359.0544385588379\n",
      "Current iteration=2000, the loss=406.07971225621, gradient=163.72106017265463\n",
      "Current iteration=3000, the loss=390.32675223833695, gradient=94.16302330655513\n",
      "Current iteration=4000, the loss=384.3334077981735, gradient=63.591705609980316\n",
      "Current iteration=5000, the loss=381.226788397604, gradient=49.375871613559475\n",
      "Current iteration=6000, the loss=379.14257276974047, gradient=42.68127561892022\n",
      "Current iteration=7000, the loss=377.47306736071016, gradient=39.390232073377895\n",
      "Current iteration=8000, the loss=375.9978828684497, gradient=37.5863409554235\n",
      "Current iteration=9000, the loss=374.6312904060292, gradient=36.42343274345085\n",
      "Current iteration=10000, the loss=373.3379663237161, gradient=35.54045651244913\n",
      "Current iteration=11000, the loss=372.1023473623392, gradient=34.785168908201996\n",
      "Current iteration=12000, the loss=370.91685356625067, gradient=34.09344254837347\n",
      "Current iteration=13000, the loss=369.7772037659802, gradient=33.438208276208535\n",
      "Current iteration=14000, the loss=368.68051370171327, gradient=32.8079971875085\n",
      "Current iteration=15000, the loss=367.6245155555063, gradient=32.197901477257865\n",
      "Current iteration=16000, the loss=366.60723648567347, gradient=31.605755044502025\n",
      "Current iteration=17000, the loss=365.62686635083895, gradient=31.030519450385604\n",
      "Current iteration=18000, the loss=364.68170338517183, gradient=30.471605216718274\n",
      "Current iteration=19000, the loss=363.77013173289816, gradient=29.928590046491216\n",
      "Current iteration=20000, the loss=362.89061176723095, gradient=29.401104844947966\n",
      "Current iteration=21000, the loss=362.0416753625138, gradient=28.88878999986848\n",
      "Current iteration=22000, the loss=361.22192295963106, gradient=28.391280540160945\n",
      "Current iteration=23000, the loss=360.43002119128613, gradient=27.908202762187443\n",
      "Current iteration=24000, the loss=359.6647006166179, gradient=27.439175107991552\n",
      "Current iteration=25000, the loss=358.92475342424456, gradient=26.98381038179236\n",
      "Current iteration=26000, the loss=358.2090310792116, gradient=26.541718182105477\n",
      "Current iteration=27000, the loss=357.5164419285972, gradient=26.112507155743995\n",
      "Current iteration=28000, the loss=356.8459487902696, gradient=25.695786965381064\n",
      "Current iteration=29000, the loss=356.1965665485884, gradient=25.291169966458327\n",
      "Current iteration=30000, the loss=355.5673597770725, gradient=24.898272621728736\n",
      "Current iteration=31000, the loss=354.95744040391145, gradient=24.516716687655116\n",
      "Current iteration=32000, the loss=354.3659654325186, gradient=24.146130204130007\n",
      "Current iteration=33000, the loss=353.79213472627583, gradient=23.78614831425316\n",
      "Current iteration=34000, the loss=353.2351888641661, gradient=23.436413936447387\n",
      "Current iteration=35000, the loss=352.6944070720115, gradient=23.096578307570272\n",
      "Current iteration=36000, the loss=352.16910523244576, gradient=22.76630141287185\n",
      "Current iteration=37000, the loss=351.65863397546826, gradient=22.445252316484\n",
      "Current iteration=38000, the loss=351.1623768503849, gradient=22.13310940443178\n",
      "Current iteration=39000, the loss=350.67974857909815, gradient=21.829560550790802\n",
      "Current iteration=40000, the loss=350.2101933900323, gradient=21.534303216479458\n",
      "Current iteration=41000, the loss=349.75318343142806, gradient=21.24704448920263\n",
      "Current iteration=42000, the loss=349.30821726231204, gradient=20.96750107221189\n",
      "Current iteration=43000, the loss=348.8748184191057, gradient=20.69539922878639\n",
      "Current iteration=44000, the loss=348.45253405557634, gradient=20.4304746886512\n",
      "Current iteration=45000, the loss=348.0409336536437, gradient=20.17247252192387\n",
      "Current iteration=46000, the loss=347.63960780241734, gradient=19.921146985607837\n",
      "Current iteration=47000, the loss=347.24816704274997, gradient=19.676261347126196\n",
      "Current iteration=48000, the loss=346.86624077454536, gradient=19.437587688909513\n",
      "Current iteration=49000, the loss=346.4934762240411, gradient=19.20490669761116\n",
      "Current iteration=0, the loss=14339.135724243588, gradient=7704.112865097706\n",
      "Current iteration=1000, the loss=12373.135822962897, gradient=593.1997976867842\n",
      "Current iteration=2000, the loss=12141.870253407513, gradient=393.09652540736914\n",
      "Current iteration=3000, the loss=12023.573591218366, gradient=302.54371747761655\n",
      "Current iteration=4000, the loss=11947.595053173936, gradient=252.48052827675644\n",
      "Current iteration=5000, the loss=11891.729746824854, gradient=222.27747621742793\n",
      "Current iteration=6000, the loss=11846.830426286102, gradient=202.6688348516\n",
      "Current iteration=7000, the loss=11808.619879916225, gradient=188.9641862373443\n",
      "Current iteration=8000, the loss=11774.903384061774, gradient=178.70160492544034\n",
      "Current iteration=9000, the loss=11744.45868007421, gradient=170.5482185915604\n",
      "Current iteration=10000, the loss=11716.552262576213, gradient=163.7561713577319\n",
      "Current iteration=11000, the loss=11690.712778473007, gradient=157.88966842883917\n",
      "Current iteration=12000, the loss=11666.617894763542, gradient=152.6850930106542\n",
      "Current iteration=13000, the loss=11644.034554288932, gradient=147.9771314971728\n",
      "Current iteration=14000, the loss=11622.785839982189, gradient=143.65842003338864\n",
      "Current iteration=15000, the loss=11602.731792451335, gradient=139.65675854676832\n",
      "Current iteration=16000, the loss=11583.757873548335, gradient=135.92184958607226\n",
      "Current iteration=17000, the loss=11565.76778044716, gradient=132.41737349165314\n",
      "Current iteration=18000, the loss=11548.678816708178, gradient=129.11614300474335\n",
      "Current iteration=19000, the loss=11532.41880977872, gradient=125.99708106028552\n",
      "Current iteration=20000, the loss=11516.923988571463, gradient=123.04330176792311\n",
      "Current iteration=21000, the loss=11502.137472231327, gradient=120.24087136042772\n",
      "Current iteration=22000, the loss=11488.008157861794, gradient=117.57799494093801\n",
      "Current iteration=23000, the loss=11474.489875482564, gradient=115.04447360465622\n",
      "Current iteration=24000, the loss=11461.540726866933, gradient=112.63133543306759\n",
      "Current iteration=25000, the loss=11449.122554494215, gradient=110.33057965782235\n",
      "Current iteration=26000, the loss=11437.200505239636, gradient=108.13499537414576\n",
      "Current iteration=27000, the loss=11425.742665026573, gradient=106.0380299816674\n",
      "Current iteration=28000, the loss=11414.719748098494, gradient=104.0336912475759\n",
      "Current iteration=29000, the loss=11404.104829404267, gradient=102.1164724472128\n",
      "Current iteration=30000, the loss=11393.873111789639, gradient=100.28129361429598\n",
      "Current iteration=31000, the loss=11384.001721842615, gradient=98.52345425227416\n",
      "Current iteration=32000, the loss=11374.469529719976, gradient=96.83859437320965\n",
      "Current iteration=33000, the loss=11365.256989319976, gradient=95.2226617271767\n",
      "Current iteration=34000, the loss=11356.345995910411, gradient=93.67188374552371\n",
      "Current iteration=35000, the loss=11347.719758867315, gradient=92.18274316222069\n",
      "Current iteration=36000, the loss=11339.362687589915, gradient=90.75195657426603\n",
      "Current iteration=37000, the loss=11331.260288972604, gradient=89.3764554036701\n",
      "Current iteration=38000, the loss=11323.399075062262, gradient=88.05336886183444\n",
      "Current iteration=39000, the loss=11315.766479727336, gradient=86.78000861316255\n",
      "Current iteration=40000, the loss=11308.35078332651, gradient=85.55385490231723\n",
      "Current iteration=41000, the loss=11301.14104449824, gradient=84.37254395784683\n",
      "Current iteration=42000, the loss=11294.127038304126, gradient=83.23385652004724\n",
      "Current iteration=43000, the loss=11287.299200053796, gradient=82.13570736700125\n",
      "Current iteration=44000, the loss=11280.648574219667, gradient=81.0761357324977\n",
      "Current iteration=45000, the loss=11274.166767919443, gradient=80.0532965248296\n",
      "Current iteration=46000, the loss=11267.845908504343, gradient=79.0654522675895\n",
      "Current iteration=47000, the loss=11261.678604843291, gradient=78.11096569338069\n",
      "Current iteration=48000, the loss=11255.657911938924, gradient=77.18829292943913\n",
      "Current iteration=49000, the loss=11249.777298551177, gradient=76.29597722095491\n"
     ]
    }
   ],
   "source": [
    "w = api.train(y, x, poly=poly, split_method='mass', replace=None, cv=False, cut=cut, \n",
    "               model_func=implementations.reg_logistic_regression, \n",
    "               lambda_=lambda_, initial_w=initial_w, max_iters=n_iters, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y, test_x, test_ids = helpers.load_csv_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(350000, -1.0),\n",
       "             (350001, -1.0),\n",
       "             (350002, -1.0),\n",
       "             (350003, 1.0),\n",
       "             (350004, -1.0),\n",
       "             (350005, -1.0),\n",
       "             (350006, 1.0),\n",
       "             (350007, -1.0),\n",
       "             (350008, -1.0),\n",
       "             (350009, 1.0),\n",
       "             (350010, -1.0),\n",
       "             (350011, -1.0),\n",
       "             (350012, -1.0),\n",
       "             (350013, -1.0),\n",
       "             (350014, -1.0),\n",
       "             (350015, -1.0),\n",
       "             (350016, -1.0),\n",
       "             (350017, -1.0),\n",
       "             (350018, -1.0),\n",
       "             (350019, 1.0),\n",
       "             (350020, -1.0),\n",
       "             (350021, -1.0),\n",
       "             (350022, -1.0),\n",
       "             (350023, 1.0),\n",
       "             (350024, 1.0),\n",
       "             (350025, -1.0),\n",
       "             (350026, -1.0),\n",
       "             (350027, -1.0),\n",
       "             (350028, -1.0),\n",
       "             (350029, -1.0),\n",
       "             (350030, -1.0),\n",
       "             (350031, -1.0),\n",
       "             (350032, -1.0),\n",
       "             (350033, -1.0),\n",
       "             (350034, -1.0),\n",
       "             (350035, -1.0),\n",
       "             (350036, -1.0),\n",
       "             (350037, -1.0),\n",
       "             (350038, -1.0),\n",
       "             (350039, -1.0),\n",
       "             (350040, -1.0),\n",
       "             (350041, -1.0),\n",
       "             (350042, -1.0),\n",
       "             (350043, 1.0),\n",
       "             (350044, 1.0),\n",
       "             (350045, -1.0),\n",
       "             (350046, -1.0),\n",
       "             (350047, 1.0),\n",
       "             (350048, -1.0),\n",
       "             (350049, -1.0),\n",
       "             (350050, 1.0),\n",
       "             (350051, 1.0),\n",
       "             (350052, 1.0),\n",
       "             (350053, 1.0),\n",
       "             (350054, 1.0),\n",
       "             (350055, -1.0),\n",
       "             (350056, -1.0),\n",
       "             (350057, -1.0),\n",
       "             (350058, 1.0),\n",
       "             (350059, -1.0),\n",
       "             (350060, -1.0),\n",
       "             (350061, -1.0),\n",
       "             (350062, 1.0),\n",
       "             (350063, -1.0),\n",
       "             (350064, 1.0),\n",
       "             (350065, -1.0),\n",
       "             (350066, -1.0),\n",
       "             (350067, 1.0),\n",
       "             (350068, 1.0),\n",
       "             (350069, 1.0),\n",
       "             (350070, 1.0),\n",
       "             (350071, -1.0),\n",
       "             (350072, 1.0),\n",
       "             (350073, 1.0),\n",
       "             (350074, -1.0),\n",
       "             (350075, -1.0),\n",
       "             (350076, -1.0),\n",
       "             (350077, 1.0),\n",
       "             (350078, -1.0),\n",
       "             (350079, 1.0),\n",
       "             (350080, -1.0),\n",
       "             (350081, 1.0),\n",
       "             (350082, -1.0),\n",
       "             (350083, 1.0),\n",
       "             (350084, -1.0),\n",
       "             (350085, -1.0),\n",
       "             (350086, -1.0),\n",
       "             (350087, -1.0),\n",
       "             (350088, 1.0),\n",
       "             (350089, 1.0),\n",
       "             (350090, -1.0),\n",
       "             (350091, 1.0),\n",
       "             (350092, -1.0),\n",
       "             (350093, -1.0),\n",
       "             (350094, -1.0),\n",
       "             (350095, 1.0),\n",
       "             (350096, -1.0),\n",
       "             (350097, -1.0),\n",
       "             (350098, -1.0),\n",
       "             (350099, -1.0),\n",
       "             (350100, -1.0),\n",
       "             (350101, -1.0),\n",
       "             (350102, -1.0),\n",
       "             (350103, -1.0),\n",
       "             (350104, -1.0),\n",
       "             (350105, -1.0),\n",
       "             (350106, 1.0),\n",
       "             (350107, -1.0),\n",
       "             (350108, 1.0),\n",
       "             (350109, 1.0),\n",
       "             (350110, -1.0),\n",
       "             (350111, 1.0),\n",
       "             (350112, -1.0),\n",
       "             (350113, -1.0),\n",
       "             (350114, 1.0),\n",
       "             (350115, -1.0),\n",
       "             (350116, 1.0),\n",
       "             (350117, 1.0),\n",
       "             (350118, -1.0),\n",
       "             (350119, -1.0),\n",
       "             (350120, -1.0),\n",
       "             (350121, -1.0),\n",
       "             (350122, -1.0),\n",
       "             (350123, -1.0),\n",
       "             (350124, -1.0),\n",
       "             (350125, -1.0),\n",
       "             (350126, -1.0),\n",
       "             (350127, -1.0),\n",
       "             (350128, -1.0),\n",
       "             (350129, 1.0),\n",
       "             (350130, -1.0),\n",
       "             (350131, -1.0),\n",
       "             (350132, 1.0),\n",
       "             (350133, -1.0),\n",
       "             (350134, -1.0),\n",
       "             (350135, -1.0),\n",
       "             (350136, -1.0),\n",
       "             (350137, -1.0),\n",
       "             (350138, -1.0),\n",
       "             (350139, 1.0),\n",
       "             (350140, -1.0),\n",
       "             (350141, -1.0),\n",
       "             (350142, -1.0),\n",
       "             (350143, -1.0),\n",
       "             (350144, -1.0),\n",
       "             (350145, 1.0),\n",
       "             (350146, -1.0),\n",
       "             (350147, -1.0),\n",
       "             (350148, 1.0),\n",
       "             (350149, -1.0),\n",
       "             (350150, 1.0),\n",
       "             (350151, -1.0),\n",
       "             (350152, 1.0),\n",
       "             (350153, -1.0),\n",
       "             (350154, 1.0),\n",
       "             (350155, -1.0),\n",
       "             (350156, -1.0),\n",
       "             (350157, 1.0),\n",
       "             (350158, 1.0),\n",
       "             (350159, -1.0),\n",
       "             (350160, -1.0),\n",
       "             (350161, -1.0),\n",
       "             (350162, -1.0),\n",
       "             (350163, 1.0),\n",
       "             (350164, -1.0),\n",
       "             (350165, -1.0),\n",
       "             (350166, -1.0),\n",
       "             (350167, -1.0),\n",
       "             (350168, -1.0),\n",
       "             (350169, 1.0),\n",
       "             (350170, -1.0),\n",
       "             (350171, -1.0),\n",
       "             (350172, 1.0),\n",
       "             (350173, -1.0),\n",
       "             (350174, -1.0),\n",
       "             (350175, -1.0),\n",
       "             (350176, 1.0),\n",
       "             (350177, -1.0),\n",
       "             (350178, -1.0),\n",
       "             (350179, 1.0),\n",
       "             (350180, 1.0),\n",
       "             (350181, -1.0),\n",
       "             (350182, -1.0),\n",
       "             (350183, -1.0),\n",
       "             (350184, -1.0),\n",
       "             (350185, -1.0),\n",
       "             (350186, 1.0),\n",
       "             (350187, -1.0),\n",
       "             (350188, -1.0),\n",
       "             (350189, 1.0),\n",
       "             (350190, -1.0),\n",
       "             (350191, -1.0),\n",
       "             (350192, -1.0),\n",
       "             (350193, -1.0),\n",
       "             (350194, -1.0),\n",
       "             (350195, -1.0),\n",
       "             (350196, -1.0),\n",
       "             (350197, 1.0),\n",
       "             (350198, 1.0),\n",
       "             (350199, 1.0),\n",
       "             (350200, -1.0),\n",
       "             (350201, -1.0),\n",
       "             (350202, -1.0),\n",
       "             (350203, 1.0),\n",
       "             (350204, 1.0),\n",
       "             (350205, 1.0),\n",
       "             (350206, -1.0),\n",
       "             (350207, -1.0),\n",
       "             (350208, 1.0),\n",
       "             (350209, -1.0),\n",
       "             (350210, 1.0),\n",
       "             (350211, 1.0),\n",
       "             (350212, -1.0),\n",
       "             (350213, 1.0),\n",
       "             (350214, -1.0),\n",
       "             (350215, 1.0),\n",
       "             (350216, 1.0),\n",
       "             (350217, -1.0),\n",
       "             (350218, 1.0),\n",
       "             (350219, -1.0),\n",
       "             (350220, -1.0),\n",
       "             (350221, -1.0),\n",
       "             (350222, -1.0),\n",
       "             (350223, -1.0),\n",
       "             (350224, 1.0),\n",
       "             (350225, -1.0),\n",
       "             (350226, -1.0),\n",
       "             (350227, -1.0),\n",
       "             (350228, -1.0),\n",
       "             (350229, -1.0),\n",
       "             (350230, -1.0),\n",
       "             (350231, 1.0),\n",
       "             (350232, -1.0),\n",
       "             (350233, -1.0),\n",
       "             (350234, -1.0),\n",
       "             (350235, 1.0),\n",
       "             (350236, -1.0),\n",
       "             (350237, -1.0),\n",
       "             (350238, -1.0),\n",
       "             (350239, -1.0),\n",
       "             (350240, -1.0),\n",
       "             (350241, -1.0),\n",
       "             (350242, -1.0),\n",
       "             (350243, -1.0),\n",
       "             (350244, -1.0),\n",
       "             (350245, 1.0),\n",
       "             (350246, 1.0),\n",
       "             (350247, -1.0),\n",
       "             (350248, 1.0),\n",
       "             (350249, -1.0),\n",
       "             (350250, 1.0),\n",
       "             (350251, 1.0),\n",
       "             (350252, 1.0),\n",
       "             (350253, 1.0),\n",
       "             (350254, -1.0),\n",
       "             (350255, 1.0),\n",
       "             (350256, 1.0),\n",
       "             (350257, -1.0),\n",
       "             (350258, -1.0),\n",
       "             (350259, -1.0),\n",
       "             (350260, 1.0),\n",
       "             (350261, 1.0),\n",
       "             (350262, 1.0),\n",
       "             (350263, 1.0),\n",
       "             (350264, -1.0),\n",
       "             (350265, -1.0),\n",
       "             (350266, 1.0),\n",
       "             (350267, -1.0),\n",
       "             (350268, 1.0),\n",
       "             (350269, -1.0),\n",
       "             (350270, -1.0),\n",
       "             (350271, -1.0),\n",
       "             (350272, -1.0),\n",
       "             (350273, -1.0),\n",
       "             (350274, -1.0),\n",
       "             (350275, -1.0),\n",
       "             (350276, 1.0),\n",
       "             (350277, -1.0),\n",
       "             (350278, -1.0),\n",
       "             (350279, -1.0),\n",
       "             (350280, 1.0),\n",
       "             (350281, -1.0),\n",
       "             (350282, -1.0),\n",
       "             (350283, -1.0),\n",
       "             (350284, -1.0),\n",
       "             (350285, 1.0),\n",
       "             (350286, -1.0),\n",
       "             (350287, -1.0),\n",
       "             (350288, -1.0),\n",
       "             (350289, -1.0),\n",
       "             (350290, -1.0),\n",
       "             (350291, -1.0),\n",
       "             (350292, -1.0),\n",
       "             (350293, -1.0),\n",
       "             (350294, 1.0),\n",
       "             (350295, -1.0),\n",
       "             (350296, -1.0),\n",
       "             (350297, -1.0),\n",
       "             (350298, 1.0),\n",
       "             (350299, -1.0),\n",
       "             (350300, -1.0),\n",
       "             (350301, -1.0),\n",
       "             (350302, -1.0),\n",
       "             (350303, -1.0),\n",
       "             (350304, -1.0),\n",
       "             (350305, -1.0),\n",
       "             (350306, -1.0),\n",
       "             (350307, -1.0),\n",
       "             (350308, -1.0),\n",
       "             (350309, 1.0),\n",
       "             (350310, 1.0),\n",
       "             (350311, -1.0),\n",
       "             (350312, -1.0),\n",
       "             (350313, 1.0),\n",
       "             (350314, 1.0),\n",
       "             (350315, -1.0),\n",
       "             (350316, -1.0),\n",
       "             (350317, -1.0),\n",
       "             (350318, -1.0),\n",
       "             (350319, -1.0),\n",
       "             (350320, -1.0),\n",
       "             (350321, 1.0),\n",
       "             (350322, 1.0),\n",
       "             (350323, -1.0),\n",
       "             (350324, -1.0),\n",
       "             (350325, -1.0),\n",
       "             (350326, -1.0),\n",
       "             (350327, -1.0),\n",
       "             (350328, -1.0),\n",
       "             (350329, -1.0),\n",
       "             (350330, 1.0),\n",
       "             (350331, -1.0),\n",
       "             (350332, -1.0),\n",
       "             (350333, -1.0),\n",
       "             (350334, 1.0),\n",
       "             (350335, -1.0),\n",
       "             (350336, -1.0),\n",
       "             (350337, -1.0),\n",
       "             (350338, -1.0),\n",
       "             (350339, 1.0),\n",
       "             (350340, -1.0),\n",
       "             (350341, -1.0),\n",
       "             (350342, -1.0),\n",
       "             (350343, 1.0),\n",
       "             (350344, -1.0),\n",
       "             (350345, -1.0),\n",
       "             (350346, 1.0),\n",
       "             (350347, -1.0),\n",
       "             (350348, -1.0),\n",
       "             (350349, 1.0),\n",
       "             (350350, -1.0),\n",
       "             (350351, -1.0),\n",
       "             (350352, -1.0),\n",
       "             (350353, -1.0),\n",
       "             (350354, 1.0),\n",
       "             (350355, -1.0),\n",
       "             (350356, 1.0),\n",
       "             (350357, -1.0),\n",
       "             (350358, -1.0),\n",
       "             (350359, -1.0),\n",
       "             (350360, -1.0),\n",
       "             (350361, 1.0),\n",
       "             (350362, -1.0),\n",
       "             (350363, 1.0),\n",
       "             (350364, 1.0),\n",
       "             (350365, 1.0),\n",
       "             (350366, -1.0),\n",
       "             (350367, 1.0),\n",
       "             (350368, 1.0),\n",
       "             (350369, -1.0),\n",
       "             (350370, 1.0),\n",
       "             (350371, -1.0),\n",
       "             (350372, -1.0),\n",
       "             (350373, 1.0),\n",
       "             (350374, -1.0),\n",
       "             (350375, 1.0),\n",
       "             (350376, 1.0),\n",
       "             (350377, -1.0),\n",
       "             (350378, -1.0),\n",
       "             (350379, -1.0),\n",
       "             (350380, 1.0),\n",
       "             (350381, 1.0),\n",
       "             (350382, 1.0),\n",
       "             (350383, -1.0),\n",
       "             (350384, -1.0),\n",
       "             (350385, -1.0),\n",
       "             (350386, 1.0),\n",
       "             (350387, -1.0),\n",
       "             (350388, 1.0),\n",
       "             (350389, 1.0),\n",
       "             (350390, 1.0),\n",
       "             (350391, 1.0),\n",
       "             (350392, -1.0),\n",
       "             (350393, -1.0),\n",
       "             (350394, -1.0),\n",
       "             (350395, -1.0),\n",
       "             (350396, -1.0),\n",
       "             (350397, -1.0),\n",
       "             (350398, -1.0),\n",
       "             (350399, -1.0),\n",
       "             (350400, 1.0),\n",
       "             (350401, 1.0),\n",
       "             (350402, -1.0),\n",
       "             (350403, -1.0),\n",
       "             (350404, 1.0),\n",
       "             (350405, -1.0),\n",
       "             (350406, -1.0),\n",
       "             (350407, 1.0),\n",
       "             (350408, 1.0),\n",
       "             (350409, -1.0),\n",
       "             (350410, -1.0),\n",
       "             (350411, 1.0),\n",
       "             (350412, 1.0),\n",
       "             (350413, 1.0),\n",
       "             (350414, 1.0),\n",
       "             (350415, -1.0),\n",
       "             (350416, 1.0),\n",
       "             (350417, -1.0),\n",
       "             (350418, -1.0),\n",
       "             (350419, 1.0),\n",
       "             (350420, 1.0),\n",
       "             (350421, 1.0),\n",
       "             (350422, 1.0),\n",
       "             (350423, -1.0),\n",
       "             (350424, -1.0),\n",
       "             (350425, -1.0),\n",
       "             (350426, -1.0),\n",
       "             (350427, -1.0),\n",
       "             (350428, -1.0),\n",
       "             (350429, 1.0),\n",
       "             (350430, -1.0),\n",
       "             (350431, -1.0),\n",
       "             (350432, -1.0),\n",
       "             (350433, 1.0),\n",
       "             (350434, -1.0),\n",
       "             (350435, -1.0),\n",
       "             (350436, -1.0),\n",
       "             (350437, -1.0),\n",
       "             (350438, -1.0),\n",
       "             (350439, -1.0),\n",
       "             (350440, -1.0),\n",
       "             (350441, 1.0),\n",
       "             (350442, 1.0),\n",
       "             (350443, 1.0),\n",
       "             (350444, -1.0),\n",
       "             (350445, -1.0),\n",
       "             (350446, -1.0),\n",
       "             (350447, -1.0),\n",
       "             (350448, 1.0),\n",
       "             (350449, -1.0),\n",
       "             (350450, -1.0),\n",
       "             (350451, -1.0),\n",
       "             (350452, -1.0),\n",
       "             (350453, 1.0),\n",
       "             (350454, -1.0),\n",
       "             (350455, -1.0),\n",
       "             (350456, -1.0),\n",
       "             (350457, 1.0),\n",
       "             (350458, 1.0),\n",
       "             (350459, -1.0),\n",
       "             (350460, 1.0),\n",
       "             (350461, 1.0),\n",
       "             (350462, 1.0),\n",
       "             (350463, 1.0),\n",
       "             (350464, 1.0),\n",
       "             (350465, -1.0),\n",
       "             (350466, -1.0),\n",
       "             (350467, 1.0),\n",
       "             (350468, 1.0),\n",
       "             (350469, 1.0),\n",
       "             (350470, -1.0),\n",
       "             (350471, -1.0),\n",
       "             (350472, 1.0),\n",
       "             (350473, -1.0),\n",
       "             (350474, 1.0),\n",
       "             (350475, -1.0),\n",
       "             (350476, 1.0),\n",
       "             (350477, -1.0),\n",
       "             (350478, -1.0),\n",
       "             (350479, 1.0),\n",
       "             (350480, -1.0),\n",
       "             (350481, -1.0),\n",
       "             (350482, -1.0),\n",
       "             (350483, 1.0),\n",
       "             (350484, -1.0),\n",
       "             (350485, 1.0),\n",
       "             (350486, 1.0),\n",
       "             (350487, -1.0),\n",
       "             (350488, -1.0),\n",
       "             (350489, -1.0),\n",
       "             (350490, -1.0),\n",
       "             (350491, -1.0),\n",
       "             (350492, 1.0),\n",
       "             (350493, -1.0),\n",
       "             (350494, -1.0),\n",
       "             (350495, -1.0),\n",
       "             (350496, -1.0),\n",
       "             (350497, 1.0),\n",
       "             (350498, 1.0),\n",
       "             (350499, 1.0),\n",
       "             (350500, 1.0),\n",
       "             (350501, 1.0),\n",
       "             (350502, -1.0),\n",
       "             (350503, -1.0),\n",
       "             (350504, -1.0),\n",
       "             (350505, -1.0),\n",
       "             (350506, -1.0),\n",
       "             (350507, 1.0),\n",
       "             (350508, -1.0),\n",
       "             (350509, 1.0),\n",
       "             (350510, 1.0),\n",
       "             (350511, -1.0),\n",
       "             (350512, -1.0),\n",
       "             (350513, 1.0),\n",
       "             (350514, -1.0),\n",
       "             (350515, -1.0),\n",
       "             (350516, -1.0),\n",
       "             (350517, -1.0),\n",
       "             (350518, -1.0),\n",
       "             (350519, -1.0),\n",
       "             (350520, -1.0),\n",
       "             (350521, -1.0),\n",
       "             (350522, -1.0),\n",
       "             (350523, 1.0),\n",
       "             (350524, -1.0),\n",
       "             (350525, 1.0),\n",
       "             (350526, -1.0),\n",
       "             (350527, -1.0),\n",
       "             (350528, 1.0),\n",
       "             (350529, -1.0),\n",
       "             (350530, -1.0),\n",
       "             (350531, -1.0),\n",
       "             (350532, 1.0),\n",
       "             (350533, 1.0),\n",
       "             (350534, -1.0),\n",
       "             (350535, -1.0),\n",
       "             (350536, -1.0),\n",
       "             (350537, -1.0),\n",
       "             (350538, 1.0),\n",
       "             (350539, -1.0),\n",
       "             (350540, -1.0),\n",
       "             (350541, 1.0),\n",
       "             (350542, -1.0),\n",
       "             (350543, -1.0),\n",
       "             (350544, -1.0),\n",
       "             (350545, -1.0),\n",
       "             (350546, -1.0),\n",
       "             (350547, -1.0),\n",
       "             (350548, -1.0),\n",
       "             (350549, -1.0),\n",
       "             (350550, -1.0),\n",
       "             (350551, -1.0),\n",
       "             (350552, 1.0),\n",
       "             (350553, -1.0),\n",
       "             (350554, -1.0),\n",
       "             (350555, 1.0),\n",
       "             (350556, 1.0),\n",
       "             (350557, 1.0),\n",
       "             (350558, -1.0),\n",
       "             (350559, 1.0),\n",
       "             (350560, 1.0),\n",
       "             (350561, 1.0),\n",
       "             (350562, -1.0),\n",
       "             (350563, -1.0),\n",
       "             (350564, 1.0),\n",
       "             (350565, -1.0),\n",
       "             (350566, -1.0),\n",
       "             (350567, -1.0),\n",
       "             (350568, -1.0),\n",
       "             (350569, -1.0),\n",
       "             (350570, 1.0),\n",
       "             (350571, 1.0),\n",
       "             (350572, 1.0),\n",
       "             (350573, -1.0),\n",
       "             (350574, -1.0),\n",
       "             (350575, -1.0),\n",
       "             (350576, 1.0),\n",
       "             (350577, -1.0),\n",
       "             (350578, -1.0),\n",
       "             (350579, -1.0),\n",
       "             (350580, 1.0),\n",
       "             (350581, 1.0),\n",
       "             (350582, 1.0),\n",
       "             (350583, 1.0),\n",
       "             (350584, 1.0),\n",
       "             (350585, -1.0),\n",
       "             (350586, 1.0),\n",
       "             (350587, -1.0),\n",
       "             (350588, -1.0),\n",
       "             (350589, -1.0),\n",
       "             (350590, -1.0),\n",
       "             (350591, -1.0),\n",
       "             (350592, 1.0),\n",
       "             (350593, -1.0),\n",
       "             (350594, 1.0),\n",
       "             (350595, -1.0),\n",
       "             (350596, -1.0),\n",
       "             (350597, -1.0),\n",
       "             (350598, -1.0),\n",
       "             (350599, 1.0),\n",
       "             (350600, -1.0),\n",
       "             (350601, 1.0),\n",
       "             (350602, -1.0),\n",
       "             (350603, -1.0),\n",
       "             (350604, 1.0),\n",
       "             (350605, 1.0),\n",
       "             (350606, 1.0),\n",
       "             (350607, -1.0),\n",
       "             (350608, -1.0),\n",
       "             (350609, 1.0),\n",
       "             (350610, -1.0),\n",
       "             (350611, 1.0),\n",
       "             (350612, 1.0),\n",
       "             (350613, -1.0),\n",
       "             (350614, -1.0),\n",
       "             (350615, -1.0),\n",
       "             (350616, -1.0),\n",
       "             (350617, -1.0),\n",
       "             (350618, 1.0),\n",
       "             (350619, -1.0),\n",
       "             (350620, -1.0),\n",
       "             (350621, -1.0),\n",
       "             (350622, 1.0),\n",
       "             (350623, -1.0),\n",
       "             (350624, -1.0),\n",
       "             (350625, -1.0),\n",
       "             (350626, 1.0),\n",
       "             (350627, -1.0),\n",
       "             (350628, 1.0),\n",
       "             (350629, -1.0),\n",
       "             (350630, -1.0),\n",
       "             (350631, 1.0),\n",
       "             (350632, -1.0),\n",
       "             (350633, 1.0),\n",
       "             (350634, -1.0),\n",
       "             (350635, -1.0),\n",
       "             (350636, -1.0),\n",
       "             (350637, -1.0),\n",
       "             (350638, -1.0),\n",
       "             (350639, 1.0),\n",
       "             (350640, -1.0),\n",
       "             (350641, -1.0),\n",
       "             (350642, 1.0),\n",
       "             (350643, 1.0),\n",
       "             (350644, 1.0),\n",
       "             (350645, -1.0),\n",
       "             (350646, -1.0),\n",
       "             (350647, -1.0),\n",
       "             (350648, 1.0),\n",
       "             (350649, -1.0),\n",
       "             (350650, 1.0),\n",
       "             (350651, -1.0),\n",
       "             (350652, -1.0),\n",
       "             (350653, 1.0),\n",
       "             (350654, -1.0),\n",
       "             (350655, -1.0),\n",
       "             (350656, -1.0),\n",
       "             (350657, -1.0),\n",
       "             (350658, 1.0),\n",
       "             (350659, -1.0),\n",
       "             (350660, -1.0),\n",
       "             (350661, -1.0),\n",
       "             (350662, 1.0),\n",
       "             (350663, 1.0),\n",
       "             (350664, 1.0),\n",
       "             (350665, -1.0),\n",
       "             (350666, 1.0),\n",
       "             (350667, -1.0),\n",
       "             (350668, -1.0),\n",
       "             (350669, -1.0),\n",
       "             (350670, 1.0),\n",
       "             (350671, -1.0),\n",
       "             (350672, -1.0),\n",
       "             (350673, 1.0),\n",
       "             (350674, -1.0),\n",
       "             (350675, -1.0),\n",
       "             (350676, -1.0),\n",
       "             (350677, -1.0),\n",
       "             (350678, -1.0),\n",
       "             (350679, -1.0),\n",
       "             (350680, 1.0),\n",
       "             (350681, -1.0),\n",
       "             (350682, -1.0),\n",
       "             (350683, -1.0),\n",
       "             (350684, -1.0),\n",
       "             (350685, 1.0),\n",
       "             (350686, 1.0),\n",
       "             (350687, 1.0),\n",
       "             (350688, -1.0),\n",
       "             (350689, -1.0),\n",
       "             (350690, -1.0),\n",
       "             (350691, -1.0),\n",
       "             (350692, -1.0),\n",
       "             (350693, 1.0),\n",
       "             (350694, -1.0),\n",
       "             (350695, 1.0),\n",
       "             (350696, -1.0),\n",
       "             (350697, -1.0),\n",
       "             (350698, -1.0),\n",
       "             (350699, -1.0),\n",
       "             (350700, 1.0),\n",
       "             (350701, -1.0),\n",
       "             (350702, 1.0),\n",
       "             (350703, -1.0),\n",
       "             (350704, 1.0),\n",
       "             (350705, -1.0),\n",
       "             (350706, -1.0),\n",
       "             (350707, 1.0),\n",
       "             (350708, -1.0),\n",
       "             (350709, 1.0),\n",
       "             (350710, -1.0),\n",
       "             (350711, 1.0),\n",
       "             (350712, 1.0),\n",
       "             (350713, -1.0),\n",
       "             (350714, -1.0),\n",
       "             (350715, -1.0),\n",
       "             (350716, -1.0),\n",
       "             (350717, -1.0),\n",
       "             (350718, -1.0),\n",
       "             (350719, -1.0),\n",
       "             (350720, 1.0),\n",
       "             (350721, -1.0),\n",
       "             (350722, -1.0),\n",
       "             (350723, -1.0),\n",
       "             (350724, 1.0),\n",
       "             (350725, -1.0),\n",
       "             (350726, 1.0),\n",
       "             (350727, 1.0),\n",
       "             (350728, -1.0),\n",
       "             (350729, 1.0),\n",
       "             (350730, 1.0),\n",
       "             (350731, -1.0),\n",
       "             (350732, -1.0),\n",
       "             (350733, -1.0),\n",
       "             (350734, -1.0),\n",
       "             (350735, -1.0),\n",
       "             (350736, -1.0),\n",
       "             (350737, -1.0),\n",
       "             (350738, -1.0),\n",
       "             (350739, 1.0),\n",
       "             (350740, -1.0),\n",
       "             (350741, -1.0),\n",
       "             (350742, -1.0),\n",
       "             (350743, 1.0),\n",
       "             (350744, -1.0),\n",
       "             (350745, -1.0),\n",
       "             (350746, -1.0),\n",
       "             (350747, 1.0),\n",
       "             (350748, -1.0),\n",
       "             (350749, -1.0),\n",
       "             (350750, -1.0),\n",
       "             (350751, 1.0),\n",
       "             (350752, 1.0),\n",
       "             (350753, 1.0),\n",
       "             (350754, -1.0),\n",
       "             (350755, -1.0),\n",
       "             (350756, 1.0),\n",
       "             (350757, -1.0),\n",
       "             (350758, -1.0),\n",
       "             (350759, 1.0),\n",
       "             (350760, -1.0),\n",
       "             (350761, 1.0),\n",
       "             (350762, -1.0),\n",
       "             (350763, -1.0),\n",
       "             (350764, -1.0),\n",
       "             (350765, -1.0),\n",
       "             (350766, 1.0),\n",
       "             (350767, -1.0),\n",
       "             (350768, -1.0),\n",
       "             (350769, -1.0),\n",
       "             (350770, -1.0),\n",
       "             (350771, -1.0),\n",
       "             (350772, -1.0),\n",
       "             (350773, -1.0),\n",
       "             (350774, -1.0),\n",
       "             (350775, 1.0),\n",
       "             (350776, -1.0),\n",
       "             (350777, -1.0),\n",
       "             (350778, 1.0),\n",
       "             (350779, -1.0),\n",
       "             (350780, -1.0),\n",
       "             (350781, 1.0),\n",
       "             (350782, 1.0),\n",
       "             (350783, 1.0),\n",
       "             (350784, 1.0),\n",
       "             (350785, 1.0),\n",
       "             (350786, 1.0),\n",
       "             (350787, 1.0),\n",
       "             (350788, -1.0),\n",
       "             (350789, -1.0),\n",
       "             (350790, -1.0),\n",
       "             (350791, 1.0),\n",
       "             (350792, 1.0),\n",
       "             (350793, -1.0),\n",
       "             (350794, -1.0),\n",
       "             (350795, -1.0),\n",
       "             (350796, -1.0),\n",
       "             (350797, -1.0),\n",
       "             (350798, -1.0),\n",
       "             (350799, 1.0),\n",
       "             (350800, -1.0),\n",
       "             (350801, 1.0),\n",
       "             (350802, -1.0),\n",
       "             (350803, 1.0),\n",
       "             (350804, -1.0),\n",
       "             (350805, -1.0),\n",
       "             (350806, -1.0),\n",
       "             (350807, 1.0),\n",
       "             (350808, 1.0),\n",
       "             (350809, -1.0),\n",
       "             (350810, -1.0),\n",
       "             (350811, -1.0),\n",
       "             (350812, -1.0),\n",
       "             (350813, 1.0),\n",
       "             (350814, -1.0),\n",
       "             (350815, 1.0),\n",
       "             (350816, 1.0),\n",
       "             (350817, -1.0),\n",
       "             (350818, 1.0),\n",
       "             (350819, -1.0),\n",
       "             (350820, 1.0),\n",
       "             (350821, 1.0),\n",
       "             (350822, -1.0),\n",
       "             (350823, -1.0),\n",
       "             (350824, -1.0),\n",
       "             (350825, -1.0),\n",
       "             (350826, 1.0),\n",
       "             (350827, 1.0),\n",
       "             (350828, 1.0),\n",
       "             (350829, 1.0),\n",
       "             (350830, 1.0),\n",
       "             (350831, -1.0),\n",
       "             (350832, -1.0),\n",
       "             (350833, -1.0),\n",
       "             (350834, -1.0),\n",
       "             (350835, 1.0),\n",
       "             (350836, 1.0),\n",
       "             (350837, 1.0),\n",
       "             (350838, -1.0),\n",
       "             (350839, 1.0),\n",
       "             (350840, -1.0),\n",
       "             (350841, 1.0),\n",
       "             (350842, -1.0),\n",
       "             (350843, -1.0),\n",
       "             (350844, -1.0),\n",
       "             (350845, -1.0),\n",
       "             (350846, -1.0),\n",
       "             (350847, -1.0),\n",
       "             (350848, 1.0),\n",
       "             (350849, -1.0),\n",
       "             (350850, -1.0),\n",
       "             (350851, -1.0),\n",
       "             (350852, -1.0),\n",
       "             (350853, 1.0),\n",
       "             (350854, -1.0),\n",
       "             (350855, -1.0),\n",
       "             (350856, 1.0),\n",
       "             (350857, 1.0),\n",
       "             (350858, -1.0),\n",
       "             (350859, -1.0),\n",
       "             (350860, -1.0),\n",
       "             (350861, -1.0),\n",
       "             (350862, -1.0),\n",
       "             (350863, -1.0),\n",
       "             (350864, 1.0),\n",
       "             (350865, 1.0),\n",
       "             (350866, 1.0),\n",
       "             (350867, -1.0),\n",
       "             (350868, -1.0),\n",
       "             (350869, 1.0),\n",
       "             (350870, -1.0),\n",
       "             (350871, -1.0),\n",
       "             (350872, 1.0),\n",
       "             (350873, -1.0),\n",
       "             (350874, -1.0),\n",
       "             (350875, -1.0),\n",
       "             (350876, -1.0),\n",
       "             (350877, -1.0),\n",
       "             (350878, -1.0),\n",
       "             (350879, -1.0),\n",
       "             (350880, -1.0),\n",
       "             (350881, -1.0),\n",
       "             (350882, -1.0),\n",
       "             (350883, -1.0),\n",
       "             (350884, 1.0),\n",
       "             (350885, -1.0),\n",
       "             (350886, 1.0),\n",
       "             (350887, 1.0),\n",
       "             (350888, -1.0),\n",
       "             (350889, -1.0),\n",
       "             (350890, 1.0),\n",
       "             (350891, 1.0),\n",
       "             (350892, -1.0),\n",
       "             (350893, -1.0),\n",
       "             (350894, -1.0),\n",
       "             (350895, 1.0),\n",
       "             (350896, -1.0),\n",
       "             (350897, 1.0),\n",
       "             (350898, -1.0),\n",
       "             (350899, -1.0),\n",
       "             (350900, -1.0),\n",
       "             (350901, 1.0),\n",
       "             (350902, -1.0),\n",
       "             (350903, 1.0),\n",
       "             (350904, -1.0),\n",
       "             (350905, -1.0),\n",
       "             (350906, -1.0),\n",
       "             (350907, -1.0),\n",
       "             (350908, -1.0),\n",
       "             (350909, -1.0),\n",
       "             (350910, 1.0),\n",
       "             (350911, -1.0),\n",
       "             (350912, 1.0),\n",
       "             (350913, -1.0),\n",
       "             (350914, 1.0),\n",
       "             (350915, -1.0),\n",
       "             (350916, 1.0),\n",
       "             (350917, -1.0),\n",
       "             (350918, 1.0),\n",
       "             (350919, -1.0),\n",
       "             (350920, -1.0),\n",
       "             (350921, 1.0),\n",
       "             (350922, 1.0),\n",
       "             (350923, -1.0),\n",
       "             (350924, 1.0),\n",
       "             (350925, 1.0),\n",
       "             (350926, 1.0),\n",
       "             (350927, -1.0),\n",
       "             (350928, 1.0),\n",
       "             (350929, 1.0),\n",
       "             (350930, -1.0),\n",
       "             (350931, 1.0),\n",
       "             (350932, -1.0),\n",
       "             (350933, 1.0),\n",
       "             (350934, -1.0),\n",
       "             (350935, -1.0),\n",
       "             (350936, 1.0),\n",
       "             (350937, -1.0),\n",
       "             (350938, -1.0),\n",
       "             (350939, 1.0),\n",
       "             (350940, 1.0),\n",
       "             (350941, 1.0),\n",
       "             (350942, -1.0),\n",
       "             (350943, -1.0),\n",
       "             (350944, 1.0),\n",
       "             (350945, 1.0),\n",
       "             (350946, 1.0),\n",
       "             (350947, -1.0),\n",
       "             (350948, -1.0),\n",
       "             (350949, -1.0),\n",
       "             (350950, -1.0),\n",
       "             (350951, -1.0),\n",
       "             (350952, 1.0),\n",
       "             (350953, -1.0),\n",
       "             (350954, 1.0),\n",
       "             (350955, -1.0),\n",
       "             (350956, -1.0),\n",
       "             (350957, -1.0),\n",
       "             (350958, -1.0),\n",
       "             (350959, -1.0),\n",
       "             (350960, 1.0),\n",
       "             (350961, -1.0),\n",
       "             (350962, -1.0),\n",
       "             (350963, 1.0),\n",
       "             (350964, -1.0),\n",
       "             (350965, -1.0),\n",
       "             (350966, -1.0),\n",
       "             (350967, -1.0),\n",
       "             (350968, 1.0),\n",
       "             (350969, -1.0),\n",
       "             (350970, 1.0),\n",
       "             (350971, -1.0),\n",
       "             (350972, -1.0),\n",
       "             (350973, 1.0),\n",
       "             (350974, -1.0),\n",
       "             (350975, 1.0),\n",
       "             (350976, 1.0),\n",
       "             (350977, -1.0),\n",
       "             (350978, -1.0),\n",
       "             (350979, 1.0),\n",
       "             (350980, -1.0),\n",
       "             (350981, -1.0),\n",
       "             (350982, -1.0),\n",
       "             (350983, 1.0),\n",
       "             (350984, 1.0),\n",
       "             (350985, -1.0),\n",
       "             (350986, 1.0),\n",
       "             (350987, -1.0),\n",
       "             (350988, -1.0),\n",
       "             (350989, -1.0),\n",
       "             (350990, 1.0),\n",
       "             (350991, -1.0),\n",
       "             (350992, 1.0),\n",
       "             (350993, -1.0),\n",
       "             (350994, -1.0),\n",
       "             (350995, -1.0),\n",
       "             (350996, 1.0),\n",
       "             (350997, 1.0),\n",
       "             (350998, -1.0),\n",
       "             (350999, -1.0),\n",
       "             ...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.predict(test_y, test_x, test_ids, 0.43, w, poly=poly, split_method='mass', replace=None, res_to_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from costs import *\n",
    "from models import *\n",
    "from helpers import * \n",
    "from evaluation import *\n",
    "from gradient import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean function modules. Plz don't change them as possible as you can, otherwise plz let me know and specify the changes when you commit \n",
    "\n",
    "1. **models**: 6 model functions\n",
    "2. **costs**: calculate_loss (calculate mse/mae/rmse/log_loss)\n",
    "3. **gradient**: compute_gradient (stoch_gradient, gradient_sigmoid, sigmoid, hessian)\n",
    "4. **helpers**: standardize, build_poly, batch_iter, load_csv_data, load_header, predict_labels, create_csv_submission\n",
    "5. **evaluation**: cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "** Load the training data into feature matrix, class labels, and record ids**\n",
    "\n",
    "We write our own `load_csv_data` function to import csv data, which gives us prediction column, feature matrix and each record ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) method to standardize our feature matrix, i.e. to rescale tx down to [0, 1], so as to avoid complicated computation caused by large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx, mean_tx, std_tx = standardize(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple linear regression with least_square using **normal equations**. Here we don't consider using least squares with gradient descent or stochastic gradient descent for the fact that **optimal w could be derived thoeritically**. We therefore don't bother to estimate the w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.66100353e-01,   1.43944820e-01,  -1.48024702e+00,\n",
       "        -1.95325672e+00,   2.07611316e-01,  -2.05273171e+01,\n",
       "         2.35244629e-01,  -3.64674518e+00,   8.50084408e+01,\n",
       "         7.31821343e-02,   8.27790918e+00,  -4.55995963e+01,\n",
       "         2.41325681e+01,   2.63030240e+01,  -5.27006907e+00,\n",
       "         1.96424301e+00,   6.13727054e-01,  -4.80474078e+00,\n",
       "         5.35500372e+00,   4.57906017e+00,   5.13016525e-01,\n",
       "        -3.60672656e-01,  -5.31315200e-01,  -6.34098601e+01,\n",
       "        -5.33032834e-01,  -7.51674008e-01,   1.42630115e+00,\n",
       "        -1.22661995e+00,  -1.21167317e+00,   1.80519787e-01,\n",
       "        -8.02665771e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "max_iter = 1000\n",
    "loss, w = least_squares(y, tx)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run cross validation 4 times on our train_data to see LS performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose intial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "gamma = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, w = logistic_regression(y, tx, gamma, n_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)\n",
    "test_x = standardize(test_x[:, 2:])  # remove id and prediction columns\n",
    "# could've used load_csv_data\n",
    "create_csv_submission([i for i in range(350000,918238)], predict_labels(test_x, w), 'res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=69254.41425128581, gradient=30406.49864854841\n",
      "Current iteration=1000, the loss=46295.557185637896, gradient=533.4147409117363\n",
      "Current iteration=0, the loss=53749.4049693404, gradient=16274.952917534974\n",
      "Current iteration=1000, the loss=45175.52944325842, gradient=367.3409177277322\n",
      "Current iteration=0, the loss=34920.061809429484, gradient=5432.056411976482\n",
      "Current iteration=1000, the loss=28415.602167595942, gradient=254.52458398249715\n",
      "Current iteration=0, the loss=15362.914109930627, gradient=8018.661264849792\n",
      "Current iteration=1000, the loss=12595.055434771602, gradient=156.40197533975189\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from helpers import *\n",
    "from models import *\n",
    "from evaluation import *\n",
    "from gradient import *\n",
    "from split import *\n",
    "\n",
    "gamma = 0.00001\n",
    "n_iters = 2000\n",
    "\n",
    "y, x, ids = load_csv_data('data/train.csv')\n",
    "split_train = split_jets(y, x)\n",
    "test_y, test_x, test_ids = load_csv_data('data/test.csv')\n",
    "split_test = split_jets(test_y, test_x)\n",
    "\n",
    "ws = []\n",
    "for group in split_train:\n",
    "    sub_y, sub_x, id_indices = group\n",
    "    sub_tx = standardize(sub_x)[0]\n",
    "    loss, w = logistic_regression(sub_y, sub_tx, gamma, n_iters)\n",
    "    ws.append(w)\n",
    "\n",
    "res = {}\n",
    "for index, group in enumerate(split_test):\n",
    "    sub_y, sub_x, id_indices = group\n",
    "    sub_tx = standardize(sub_x)[0]\n",
    "    pred_y = predict_labels(ws[index], sub_tx)\n",
    "    res.update(dict(zip(test_ids[id_indices], pred_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_csv_submission(res.keys(), res.values(), 'hah.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "**Generate predictions and save ouput in csv format for submission**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

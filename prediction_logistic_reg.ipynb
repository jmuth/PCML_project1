{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "from models import *\n",
    "from helpers import * \n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "** Load the training data into feature matrix, class labels, and record ids**\n",
    "\n",
    "We write our own `load_csv_data` function to import csv data, which gives us prediction column, feature matrix and each record ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) method to standardize our feature matrix, i.e. to rescale tx down to [0, 1], so as to avoid complicated computation caused by large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx = standardize(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple linear regression with least_square using **normal equations**. Here we don't consider using least squares with gradient descent or stochastic gradient descent for the fact that **optimal w could be derived thoeritically**. We therefore don't bother to estimate the w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.66100353e-01,   1.43944820e-01,  -1.48024702e+00,\n",
       "        -1.95325672e+00,   2.07611316e-01,  -2.05273171e+01,\n",
       "         2.35244629e-01,  -3.64674518e+00,   8.50084408e+01,\n",
       "         7.31821343e-02,   8.27790918e+00,  -4.55995963e+01,\n",
       "         2.41325681e+01,   2.63030240e+01,  -5.27006907e+00,\n",
       "         1.96424301e+00,   6.13727054e-01,  -4.80474078e+00,\n",
       "         5.35500372e+00,   4.57906017e+00,   5.13016525e-01,\n",
       "        -3.60672656e-01,  -5.31315200e-01,  -6.34098601e+01,\n",
       "        -5.33032834e-01,  -7.51674008e-01,   1.42630115e+00,\n",
       "        -1.22661995e+00,  -1.21167317e+00,   1.80519787e-01,\n",
       "        -8.02665771e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "max_iter = 1000\n",
    "loss, w = least_squares(y, tx)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run cross validation 4 times on our train_data to see LS performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ws, losses, accs = cross_validation(y, tx, 4, 0, least_squares, method='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.32400000000000001,\n",
       "  0.32319999999999999,\n",
       "  0.33760000000000001,\n",
       "  0.32240000000000002],\n",
       " [array([  6.17887006e-01,   1.44753238e-01,  -1.44017078e+00,\n",
       "          -2.02554857e+00,   2.49075218e-01,  -2.73717186e+01,\n",
       "           2.86539824e-01,  -4.34507327e+00,   8.68566499e+01,\n",
       "           4.98385802e-02,   1.25955746e+01,  -4.44435798e+01,\n",
       "           2.48005245e+01,   3.18972854e+01,  -9.54590940e+00,\n",
       "           5.93871885e-01,   2.13091547e+00,  -9.05394092e+00,\n",
       "           7.30194088e+00,   5.30407990e+00,   4.67990396e-01,\n",
       "           1.98184629e+00,  -4.89507554e-01,  -6.36371398e+01,\n",
       "          -7.09165322e-01,  -8.93778255e-01,   1.75881540e+00,\n",
       "          -9.69905164e-01,  -8.08825410e-01,   1.37861988e+00,\n",
       "          -1.23612166e+01]),\n",
       "  array([  6.32526280e-01,   1.62908598e-01,  -1.52801347e+00,\n",
       "          -1.89714031e+00,   1.98243687e-01,  -1.78317055e+01,\n",
       "           2.55967696e-01,  -2.70397158e+00,   8.09544603e+01,\n",
       "           1.12667856e-01,   7.79990260e+00,  -4.26609703e+01,\n",
       "           2.19585494e+01,   2.18505335e+01,  -4.71034850e+00,\n",
       "           2.51463615e+00,   2.78222015e-02,  -4.64327426e+00,\n",
       "           5.35417392e+00,   4.17733730e+00,   6.09704833e-01,\n",
       "          -3.72592219e+00,  -5.38328495e-01,  -5.71380702e+01,\n",
       "          -5.15849505e-01,  -2.61018763e-01,   9.16100180e-01,\n",
       "          -1.26159380e+00,  -2.84846940e-01,   4.65523772e-02,\n",
       "          -7.54193431e+00]),\n",
       "  array([  0.6589244 ,   0.12573217,  -1.57467118,  -1.81968744,\n",
       "           0.17934816, -22.34665465,   0.24361697,  -5.13938497,\n",
       "          81.9160628 ,   0.15608643,  13.38011078, -30.79123809,\n",
       "          25.27286961,  29.05124605,  -9.98664104,   2.57430899,\n",
       "          -0.82781497, -10.43353903,   3.95326379,   4.21544149,\n",
       "           0.48266958,   0.09775806,  -0.51806529, -63.70804518,\n",
       "          -0.40743571,   0.6835812 ,  -0.13205047,  -1.10613083,\n",
       "          -1.78605977,   1.24842773, -13.27593793]),\n",
       "  array([  7.56829328e-01,   1.39414127e-01,  -1.40992166e+00,\n",
       "          -2.13420770e+00,   2.41837253e-01,  -1.44606705e+01,\n",
       "           1.54242530e-01,  -2.07609023e+00,   9.39427891e+01,\n",
       "          -1.17173654e-02,  -9.47357626e-01,  -6.57028061e+01,\n",
       "           2.37255266e+01,   2.25811578e+01,   3.44221298e+00,\n",
       "           3.10509309e+00,   1.05723643e+00,   5.26547876e+00,\n",
       "           4.41343249e+00,   4.66305941e+00,   4.72742869e-01,\n",
       "           4.24633806e-01,  -5.70965448e-01,  -7.21482339e+01,\n",
       "          -5.03970415e-01,  -2.81689502e+00,   3.45089414e+00,\n",
       "          -1.60843004e+00,  -2.53113156e+00,  -1.93345768e+00,\n",
       "           1.36750153e+00])],\n",
       " [2708.1289464363012,\n",
       "  2701.8818066953954,\n",
       "  2697.6723946245265,\n",
       "  2701.1830192436564])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs, ws, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose intial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iters = 2000\n",
    "gamma = 0.000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = logistic_regression(y, tx, gamma, n_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)\n",
    "test_x = standardize(test_x[:, 2:])  # remove id and prediction columns\n",
    "# could've used load_csv_data\n",
    "create_csv_submission([i for i in range(350000,918238)], log_reg_predict(test_x, w), 'res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "**Generate predictions and save ouput in csv format for submission**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

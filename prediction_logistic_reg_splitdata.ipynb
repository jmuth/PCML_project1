{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "from models import *\n",
    "from helpers import * \n",
    "from evaluation import *\n",
    "from split_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "** Load the training data into feature matrix, class labels, and record ids**\n",
    "\n",
    "We write our own `load_csv_data` function to import csv data, which gives us prediction column, feature matrix and each record ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split into 6 distinct datasets**\n",
    "According to our exploration, we can distinct 3 different dataset based on number of jets each experiments contains. Then each of them can be split again into 2 different datasets based on whether they have a measurable mass or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet0, jet1, jet23, y0, y1, y23 = split_on_jets(y, tx)\n",
    "jet0_nomass, jet0, y0_nomass, y0 = split_on_mass(y0, jet0)\n",
    "jet1_nomass, jet1, y1_nomass, y1 = split_on_mass(y1, jet1)\n",
    "jet23_nomass, jet23, y23_nomass, y23 = split_on_mass(y23, jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep the features without NaN in each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet0_nomass, selected_jet0_nomass = select_features_without_nan(jet0_nomass)\n",
    "jet0, selected_jet0 = select_features_without_nan(jet0)\n",
    "jet1_nomass, selected_jet1_nomass = select_features_without_nan(jet1_nomass)\n",
    "jet1, selected_jet1 = select_features_without_nan(jet1)\n",
    "jet23_nomass, selected_jet23_nomass = select_features_without_nan(jet23_nomass)\n",
    "jet23, selected_jet23 = select_features_without_nan(jet23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do: remove more features ! some are flat, some are even unique !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Standardization of values**\n",
    "We use [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) method to standardize our feature matrix, i.e. to rescale tx down to [0, 1], so as to avoid complicated computation caused by large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx = standardize(tx)\n",
    "sdt_jet0_nomass, min0_nomass, range0_nomass = standardize(jet0_nomass)\n",
    "sdt_jet0, min0, range0 = standardize(jet0)\n",
    "sdt_jet1_nomass, min1_nomass, range1_nomass = standardize(jet1_nomass)\n",
    "sdt_jet1, min1, range1 = standardize(jet1)\n",
    "sdt_jet23_nomass, min23_nomass, range23_nomass = standardize(jet23_nomass)\n",
    "sdt_jet23, min23, range23 = standardize(jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple linear regression with least_square using **normal equations**. Here we don't consider using least squares with gradient descent or stochastic gradient descent for the fact that **optimal w could be derived thoeritically**. We therefore don't bother to estimate the w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run cross validation 4 times on our train_data to see LS performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose intial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iters = 2000\n",
    "gamma = 0.000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0_nomass\n",
      "Current iteration=0, the loss=18107.08379776745, gradient=14.979627418817616\n",
      "Current iteration=100, the loss=18923.411273579743, gradient=1.077527815046155\n",
      "Current iteration=200, the loss=18922.119312667215, gradient=0.918928181652207\n",
      "Current iteration=300, the loss=18920.14834788276, gradient=0.7955475238607953\n",
      "Current iteration=400, the loss=18917.775532430078, gradient=0.6967303614411418\n",
      "Current iteration=500, the loss=18915.202810020794, gradient=0.6154599464281553\n",
      "Current iteration=600, the loss=18912.573013530553, gradient=0.5471194813382345\n",
      "Current iteration=700, the loss=18909.98309712258, gradient=0.48864242862344115\n",
      "Current iteration=800, the loss=18907.495785800133, gradient=0.4379508011387841\n",
      "Current iteration=900, the loss=18905.148996923162, gradient=0.3935946157851658\n",
      "Current iteration=1000, the loss=18902.963106326482, gradient=0.35452515368357557\n",
      "Current iteration=1100, the loss=18900.946384251096, gradient=0.3199540474469154\n",
      "Current iteration=1200, the loss=18899.09897463743, gradient=0.2892660587592147\n",
      "Current iteration=1300, the loss=18897.41575810133, gradient=0.2619649075234421\n",
      "Current iteration=1400, the loss=18895.88838010308, gradient=0.237639263821163\n",
      "Current iteration=1500, the loss=18894.506665512137, gradient=0.2159409985826377\n",
      "Current iteration=1600, the loss=18893.25958808032, gradient=0.19657090019443124\n",
      "Current iteration=1700, the loss=18892.135920618886, gradient=0.17926896927330405\n",
      "Current iteration=1800, the loss=18891.124658463035, gradient=0.1638075560170817\n",
      "Current iteration=1900, the loss=18890.2152836445, gradient=0.14998629615494988\n",
      "Final loss=18889.40566734785\n",
      "w0\n",
      "Current iteration=0, the loss=51147.33045351836, gradient=72.90404686155327\n",
      "Current iteration=100, the loss=55152.64058659032, gradient=4.728878447315115\n",
      "Current iteration=200, the loss=54606.786542033136, gradient=3.6235279474677147\n",
      "Current iteration=300, the loss=54212.09830286332, gradient=2.861579486911678\n",
      "Current iteration=400, the loss=53920.60318845821, gradient=2.317218102848628\n",
      "Current iteration=500, the loss=53701.72021500665, gradient=1.9208041856627267\n",
      "Current iteration=600, the loss=53534.96566449089, gradient=1.627342972156871\n",
      "Current iteration=700, the loss=53406.19729783035, gradient=1.405915927731353\n",
      "Current iteration=800, the loss=53305.44509805643, gradient=1.2350773800795525\n",
      "Current iteration=900, the loss=53225.57054854279, gradient=1.1000940565128303\n",
      "Current iteration=1000, the loss=53161.40066927598, gradient=0.9909563871900471\n",
      "Current iteration=1100, the loss=53109.150452882706, gradient=0.9008949695372148\n",
      "Current iteration=1200, the loss=53066.02801103456, gradient=0.8253065004933056\n",
      "Current iteration=1300, the loss=53029.95937467528, gradient=0.761009238576072\n",
      "Current iteration=1400, the loss=52999.39379041525, gradient=0.7057464536959364\n",
      "Current iteration=1500, the loss=52973.16435892013, gradient=0.6578648208854941\n",
      "Current iteration=1600, the loss=52950.38738324249, gradient=0.6161102157064083\n",
      "Current iteration=1700, the loss=52930.389160897335, gradient=0.5794994851997165\n",
      "Current iteration=1800, the loss=52912.6524362987, gradient=0.5472401131094826\n",
      "Current iteration=1900, the loss=52896.77705127735, gradient=0.5186794992916784\n",
      "Final loss=52882.5873583308\n",
      "w1_nomass\n",
      "Current iteration=0, the loss=5241.578979394306, gradient=32.128825132346805\n",
      "Current iteration=100, the loss=5528.041426938657, gradient=2.2713793187345996\n",
      "Current iteration=200, the loss=5517.510134607969, gradient=1.6160924960433563\n",
      "Current iteration=300, the loss=5510.509642355212, gradient=1.2211729271411016\n",
      "Current iteration=400, the loss=5505.606242683331, gradient=0.9618081452918907\n",
      "Current iteration=500, the loss=5502.039638696112, gradient=0.7775784465106116\n",
      "Current iteration=600, the loss=5499.3723757640755, gradient=0.6391761794443576\n",
      "Current iteration=700, the loss=5497.335932810403, gradient=0.5314947346772644\n",
      "Current iteration=800, the loss=5495.756595412895, gradient=0.4459158011561204\n",
      "Current iteration=900, the loss=5494.516956304491, gradient=0.37696758151164084\n",
      "Current iteration=1000, the loss=5493.5347039944645, gradient=0.3208701837065595\n",
      "Current iteration=1100, the loss=5492.750362609851, gradient=0.2748659624282519\n",
      "Current iteration=1200, the loss=5492.119900582275, gradient=0.23687875541768916\n",
      "Current iteration=1300, the loss=5491.6100945697945, gradient=0.20531781780098696\n",
      "Current iteration=1400, the loss=5491.19550851462, gradient=0.17895150112770355\n",
      "Current iteration=1500, the loss=5490.8564527267945, gradient=0.15681897601382444\n",
      "Current iteration=1600, the loss=5490.577558917285, gradient=0.1381655137249276\n",
      "Current iteration=1700, the loss=5490.346756406469, gradient=0.12239388007493783\n",
      "Current iteration=1800, the loss=5490.154518875693, gradient=0.10902744807593043\n",
      "Current iteration=1900, the loss=5489.993299551692, gradient=0.09768210931709356\n",
      "Final loss=5489.858356099866\n",
      "w1\n",
      "Current iteration=0, the loss=48507.82598994609, gradient=133.89079051245764\n",
      "Current iteration=100, the loss=51723.14055652816, gradient=3.0625515482943704\n",
      "Current iteration=200, the loss=51469.727929528686, gradient=2.4019434843335326\n",
      "Current iteration=300, the loss=51286.07045682593, gradient=1.936742821622469\n",
      "Current iteration=400, the loss=51149.78175953611, gradient=1.6093279398151366\n",
      "Current iteration=500, the loss=51046.848209686636, gradient=1.3772829559692144\n",
      "Current iteration=600, the loss=50967.83761434286, gradient=1.2098189412246008\n",
      "Current iteration=700, the loss=50906.22323992908, gradient=1.0853273383081017\n",
      "Current iteration=800, the loss=50857.41202759077, gradient=0.9892302454886273\n",
      "Current iteration=900, the loss=50818.129603520065, gradient=0.9120301363089142\n",
      "Current iteration=1000, the loss=50786.01660294321, gradient=0.8476853471013427\n",
      "Current iteration=1100, the loss=50759.35733327627, gradient=0.792392341810983\n",
      "Current iteration=1200, the loss=50736.89394343813, gradient=0.74374868935032\n",
      "Current iteration=1300, the loss=50717.6971966434, gradient=0.7002120996813882\n",
      "Current iteration=1400, the loss=50701.075489332274, gradient=0.6607645902361688\n",
      "Current iteration=1500, the loss=50686.51016029452, gradient=0.6247089660044309\n",
      "Current iteration=1600, the loss=50673.6091277582, gradient=0.5915469809359607\n",
      "Current iteration=1700, the loss=50662.07344890114, gradient=0.5609066207161573\n",
      "Current iteration=1800, the loss=50651.673072503734, gradient=0.5324984993441195\n",
      "Current iteration=1900, the loss=50642.22917807097, gradient=0.5060893998639399\n",
      "Final loss=50633.68388723592\n",
      "w23_nomass\n",
      "Current iteration=0, the loss=3069.948862699998, gradient=105.42502342582894\n",
      "Current iteration=100, the loss=3170.0590789183457, gradient=2.4132146810221005\n",
      "Current iteration=200, the loss=3164.3190070789983, gradient=1.6143367287427153\n",
      "Current iteration=300, the loss=3161.1606517964315, gradient=1.2171007641460818\n",
      "Current iteration=400, the loss=3159.1278217074446, gradient=0.9549665992963768\n",
      "Current iteration=500, the loss=3157.6908728161975, gradient=0.7672228486727937\n",
      "Current iteration=600, the loss=3156.627581243956, gradient=0.6292131303128271\n",
      "Current iteration=700, the loss=3155.8245838577377, gradient=0.5262545135840444\n",
      "Current iteration=800, the loss=3155.212791118003, gradient=0.4484589769526668\n",
      "Current iteration=900, the loss=3154.7450462216493, gradient=0.3889113534066784\n",
      "Current iteration=1000, the loss=3154.387229563845, gradient=0.34269032096593066\n",
      "Current iteration=1100, the loss=3154.1139408235117, gradient=0.3062547637199785\n",
      "Current iteration=1200, the loss=3153.90598142632, gradient=0.27703926777399185\n",
      "Current iteration=1300, the loss=3153.7487005145363, gradient=0.2531793120098763\n",
      "Current iteration=1400, the loss=3153.6308378172075, gradient=0.23331911027347804\n",
      "Current iteration=1500, the loss=3153.5436883842785, gradient=0.21647375765054067\n",
      "Current iteration=1600, the loss=3153.4804891752974, gradient=0.20192859742871255\n",
      "Current iteration=1700, the loss=3153.4359633841254, gradient=0.18916520392615446\n",
      "Current iteration=1800, the loss=3153.405978853892, gradient=0.1778069660403305\n",
      "Current iteration=1900, the loss=3153.387289977519, gradient=0.1675792886548521\n",
      "Final loss=3153.377404495167\n",
      "w23\n",
      "Current iteration=0, the loss=47213.02705666011, gradient=342.65295459263336\n",
      "Current iteration=100, the loss=47441.34643355982, gradient=3.4397665729576867\n",
      "Current iteration=200, the loss=47181.11783235369, gradient=2.0845797947155678\n",
      "Current iteration=300, the loss=47045.9752184256, gradient=1.5934904343149516\n",
      "Current iteration=400, the loss=46964.362187389444, gradient=1.3164659003863166\n",
      "Current iteration=500, the loss=46908.80105383837, gradient=1.1264655905747287\n",
      "Current iteration=600, the loss=46868.246836237944, gradient=0.9865272501935062\n",
      "Current iteration=700, the loss=46837.47993307065, gradient=0.8795550767062116\n",
      "Current iteration=800, the loss=46813.57994624428, gradient=0.7954529147259772\n",
      "Current iteration=900, the loss=46794.69467519799, gradient=0.7276954288735707\n",
      "Current iteration=1000, the loss=46779.5573028899, gradient=0.6719042345852051\n",
      "Current iteration=1100, the loss=46767.26465948404, gradient=0.6250723794289347\n",
      "Current iteration=1200, the loss=46757.157655907795, gradient=0.5850919445199331\n",
      "Current iteration=1300, the loss=46748.748253149475, gradient=0.5504547767858953\n",
      "Current iteration=1400, the loss=46741.67119487611, gradient=0.5200599901591535\n",
      "Current iteration=1500, the loss=46735.65064414028, gradient=0.4930887689256551\n",
      "Current iteration=1600, the loss=46730.476535958136, gradient=0.46892180798264654\n",
      "Current iteration=1700, the loss=46725.987564823765, gradient=0.44708387102493785\n",
      "Current iteration=1800, the loss=46722.058824637104, gradient=0.4272057462022264\n",
      "Current iteration=1900, the loss=46718.59276264332, gradient=0.40899752965106334\n",
      "Final loss=46715.54162255749\n"
     ]
    }
   ],
   "source": [
    "print(\"w0_nomass\")\n",
    "loss0_nomass, w0_nomass = logistic_regression(y0_nomass, jet0_nomass, gamma, n_iters)\n",
    "print(\"w0\")\n",
    "loss0, w0 = logistic_regression(y0, jet0, gamma, n_iters)\n",
    "\n",
    "print(\"w1_nomass\")\n",
    "loss1_nomass, w1_nomass = logistic_regression(y1_nomass, jet1_nomass, gamma, n_iters)\n",
    "print(\"w1\")\n",
    "loss1_nomass, w1 = logistic_regression(y1, jet1, gamma, n_iters)\n",
    "\n",
    "print(\"w23_nomass\")\n",
    "loss23_nomass, w23_nomass = logistic_regression(y23_nomass, jet23_nomass, gamma, n_iters)\n",
    "print(\"w23\")\n",
    "loss23, w23 = logistic_regression(y23, jet23, gamma, n_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = test_x[:, 0]\n",
    "testset = test_x[:, 2:] # remove id and prediction columns\n",
    "\n",
    "jet0_nm_ids = np.where((testset[:,22] == 0) & (testset[:,0] == -999.))[0]\n",
    "jet0_ids = np.where((testset[:,22] == 0) & (testset[:,0] != -999.))[0]\n",
    "jet1_nm_ids = np.where((testset[:,22] == 1) & (testset[:,0] == -999.))[0]\n",
    "jet1_ids = np.where((testset[:,22] == 1) & (testset[:,0] != -999.))[0]\n",
    "jet23_nm_ids = np.where(((testset[:,22] == 2) | (testset[:,22] == 3)) \\\n",
    "                        & (testset[:,0] == -999.))[0]\n",
    "jet23_ids = np.where(((testset[:,22] == 2) | (testset[:,22] == 3)) \\\n",
    "                        & (testset[:,0] != -999.))[0]\n",
    "\n",
    "jet0_test_nm = testset[jet0_nm_ids][:, selected_jet0_nomass]\n",
    "jet0_test = testset[jet0_ids][:, selected_jet0]\n",
    "jet1_test_nm = testset[jet1_nm_ids][:, selected_jet1_nomass]\n",
    "jet1_test = testset[jet1_ids][:, selected_jet1]\n",
    "jet23_test_nm = testset[jet23_nm_ids][:, selected_jet23_nomass]\n",
    "jet23_test = testset[jet23_ids][:, selected_jet23]\n",
    "\n",
    "higgs_jet0_nm = np.where(log_reg_predict(jet0_test_nm, w0_nomass) == 1)[0]\n",
    "higgs_jet0 = np.where(log_reg_predict(jet0_test, w0) == 1)[0]\n",
    "higgs_jet1_nm = np.where(log_reg_predict(jet1_test_nm, w1_nomass) == 1)[0]\n",
    "higgs_jet1 = np.where(log_reg_predict(jet1_test, w1) == 1)[0]\n",
    "higgs_jet23_nm = np.where(log_reg_predict(jet23_test_nm, w23_nomass) == 1)[0]\n",
    "higgs_jet23 = np.where(log_reg_predict(jet23_test, w23) == 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8628"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(higgs_jet23_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve the id's of higgs boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "higgs = itertools.chain(ids[jet0_nm_ids][higgs_jet0_nm], ids[jet0_ids][higgs_jet0] \\\n",
    "               , ids[jet1_nm_ids][higgs_jet1_nm], ids[jet1_ids][higgs_jet1]\\\n",
    "               , ids[jet23_nm_ids][higgs_jet23_nm], ids[jet23_ids][higgs_jet23])\n",
    "\n",
    "higgs_arr = list([int(i) for i in higgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-3f0d66cf669e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m350000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m918238\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhiggs_arr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(350000,918238):\n",
    "    if i in higgs_arr:\n",
    "        res.append([i, 1])\n",
    "    else:\n",
    "        res.append([i, -1])\n",
    "        \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = apply_right_model(testset, ids, w0_nomass, w0, w1_nomass, ...\n",
    "                               w1, w23_nomass, w23, selected_jet0_nomass, ...\n",
    "                              selected_jet0, selected_jet1_nomass, selected_jet1, ...\n",
    "                              selected_jet23_nomass, selected_jet23)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"higgs: \", np.count_nonzero(prediction == 1))\n",
    "print(\"non-higgs: \", np.count_nonzero(prediction == -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)\n",
    "test_x = standardize(test_x[:, 2:])  # remove id and prediction columns\n",
    "# could've used load_csv_data\n",
    "create_csv_submission([i for i in range(350000,918238)], log_reg_predict(test_x, w), 'res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_jet0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "**Generate predictions and save ouput in csv format for submission**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

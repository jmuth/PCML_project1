{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "from models import *\n",
    "from helpers import * \n",
    "from evaluation import *\n",
    "from split_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "** Load the training data into feature matrix, class labels, and record ids**\n",
    "\n",
    "We write our own `load_csv_data` function to import csv data, which gives us prediction column, feature matrix and each record ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split into 6 distinct datasets**\n",
    "According to our exploration, we can distinct 3 different dataset based on number of jets each experiments contains. Then each of them can be split again into 2 different datasets based on whether they have a measurable mass or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet0, jet1, jet23, y0, y1, y23 = split_on_jets(y, tx)\n",
    "jet0_nomass, jet0, y0_nomass, y0 = split_on_mass(y0, jet0)\n",
    "jet1_nomass, jet1, y1_nomass, y1 = split_on_mass(y1, jet1)\n",
    "jet23_nomass, jet23, y23_nomass, y23 = split_on_mass(y23, jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep the features without NaN in each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet0_nomass, selected_jet0_nomass = select_features_without_nan(jet0_nomass)\n",
    "jet0, selected_jet0 = select_features_without_nan(jet0)\n",
    "jet1_nomass, selected_jet1_nomass = select_features_without_nan(jet1_nomass)\n",
    "jet1, selected_jet1 = select_features_without_nan(jet1)\n",
    "jet23_nomass, selected_jet23_nomass = select_features_without_nan(jet23_nomass)\n",
    "jet23, selected_jet23 = select_features_without_nan(jet23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do: remove more features ! some are flat, some are even unique !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Standardization of values**\n",
    "We use [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) method to standardize our feature matrix, i.e. to rescale tx down to [0, 1], so as to avoid complicated computation caused by large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx = standardize(tx)\n",
    "sdt_jet0_nomass, min0_nomass, range0_nomass = standardize(jet0_nomass)\n",
    "sdt_jet0, min0, range0 = standardize(jet0)\n",
    "sdt_jet1_nomass, min1_nomass, range1_nomass = standardize(jet1_nomass)\n",
    "sdt_jet1, min1, range1 = standardize(jet1)\n",
    "sdt_jet23_nomass, min23_nomass, range23_nomass = standardize(jet23_nomass)\n",
    "sdt_jet23, min23, range23 = standardize(jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple linear regression with least_square using **normal equations**. Here we don't consider using least squares with gradient descent or stochastic gradient descent for the fact that **optimal w could be derived thoeritically**. We therefore don't bother to estimate the w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run cross validation 4 times on our train_data to see LS performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose intial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "gamma = 0.000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0_nomass\n",
      "Current iteration=0, the loss=18107.08379776745, gradient=14.979627418817616\n",
      "Current iteration=100, the loss=18923.411273579743, gradient=1.077527815046155\n",
      "Current iteration=200, the loss=18922.119312667215, gradient=0.918928181652207\n",
      "Current iteration=300, the loss=18920.14834788276, gradient=0.7955475238607953\n",
      "Current iteration=400, the loss=18917.775532430078, gradient=0.6967303614411418\n",
      "Current iteration=500, the loss=18915.202810020794, gradient=0.6154599464281553\n",
      "Current iteration=600, the loss=18912.573013530553, gradient=0.5471194813382345\n",
      "Current iteration=700, the loss=18909.98309712258, gradient=0.48864242862344115\n",
      "Current iteration=800, the loss=18907.495785800133, gradient=0.4379508011387841\n",
      "Current iteration=900, the loss=18905.148996923162, gradient=0.3935946157851658\n",
      "Current iteration=1000, the loss=18902.963106326482, gradient=0.35452515368357557\n",
      "Current iteration=1100, the loss=18900.946384251096, gradient=0.3199540474469154\n",
      "Current iteration=1200, the loss=18899.09897463743, gradient=0.2892660587592147\n",
      "Current iteration=1300, the loss=18897.41575810133, gradient=0.2619649075234421\n",
      "Current iteration=1400, the loss=18895.88838010308, gradient=0.237639263821163\n",
      "Current iteration=1500, the loss=18894.506665512137, gradient=0.2159409985826377\n",
      "Current iteration=1600, the loss=18893.25958808032, gradient=0.19657090019443124\n",
      "Current iteration=1700, the loss=18892.135920618886, gradient=0.17926896927330405\n",
      "Current iteration=1800, the loss=18891.124658463035, gradient=0.1638075560170817\n",
      "Current iteration=1900, the loss=18890.2152836445, gradient=0.14998629615494988\n",
      "Current iteration=2000, the loss=18889.397918452694, gradient=0.13762821491516913\n",
      "Current iteration=2100, the loss=18888.663403272803, gradient=0.12657661512589188\n",
      "Current iteration=2200, the loss=18888.003323528323, gradient=0.11669251289574785\n",
      "Current iteration=2300, the loss=18887.410003265948, gradient=0.10785247259970245\n",
      "Current iteration=2400, the loss=18886.876477669673, gradient=0.09994674612610495\n",
      "Current iteration=2500, the loss=18886.396453027573, gradient=0.09287765375676367\n",
      "Current iteration=2600, the loss=18885.964259991306, gradient=0.0865581640653691\n",
      "Current iteration=2700, the loss=18885.574804065316, gradient=0.08091064278516082\n",
      "Current iteration=2800, the loss=18885.223515920898, gradient=0.07586574862254505\n",
      "Current iteration=2900, the loss=18884.90630319107, gradient=0.07136145918231804\n",
      "Current iteration=3000, the loss=18884.61950475081, gradient=0.0673422135235454\n",
      "Current iteration=3100, the loss=18884.35984803974, gradient=0.06375815998477023\n",
      "Current iteration=3200, the loss=18884.12440968243, gradient=0.060564499186481165\n",
      "Current iteration=3300, the loss=18883.91057946108, gradient=0.05772091280909769\n",
      "Current iteration=3400, the loss=18883.71602756675, gradient=0.055191069072746936\n",
      "Current iteration=3500, the loss=18883.53867497586, gradient=0.05294219599747678\n",
      "Current iteration=3600, the loss=18883.37666675385, gradient=0.05094471365945258\n",
      "Current iteration=3700, the loss=18883.22834806554, gradient=0.049171916905658114\n",
      "Current iteration=3800, the loss=18883.092242665105, gradient=0.04759970042713306\n",
      "Current iteration=3900, the loss=18882.967033641948, gradient=0.046206318747585115\n",
      "Current iteration=4000, the loss=18882.851546207727, gradient=0.04497217453988483\n",
      "Current iteration=4100, the loss=18882.744732323245, gradient=0.0438796296783057\n",
      "Current iteration=4200, the loss=18882.645656978206, gradient=0.04291283448881334\n",
      "Current iteration=4300, the loss=18882.553485952776, gradient=0.04205757168937806\n",
      "Current iteration=4400, the loss=18882.46747490483, gradient=0.04130111244572106\n",
      "Current iteration=4500, the loss=18882.38695964183, gradient=0.04063208275575739\n",
      "Current iteration=4600, the loss=18882.3113474501, gradient=0.040040338994389864\n",
      "Current iteration=4700, the loss=18882.2401093673, gradient=0.039516851898463025\n",
      "Current iteration=4800, the loss=18882.172773295566, gradient=0.039053598566284264\n",
      "Current iteration=4900, the loss=18882.10891786391, gradient=0.03864346221440748\n",
      "Current iteration=5000, the loss=18882.048166958033, gradient=0.03828013950748586\n",
      "Current iteration=5100, the loss=18881.99018484461, gradient=0.0379580552851888\n",
      "Current iteration=5200, the loss=18881.934671825027, gradient=0.037672284479451056\n",
      "Current iteration=5300, the loss=18881.88136036071, gradient=0.03741848096614045\n",
      "Current iteration=5400, the loss=18881.83001161836, gradient=0.03719281304216489\n",
      "Current iteration=5500, the loss=18881.78041238931, gradient=0.036991905171487\n",
      "Current iteration=5600, the loss=18881.73237234204, gradient=0.036812785606626264\n",
      "Current iteration=5700, the loss=18881.68572157149, gradient=0.03665283946807588\n",
      "Current iteration=5800, the loss=18881.64030841288, gradient=0.036509766852595986\n",
      "Current iteration=5900, the loss=18881.595997491117, gradient=0.036381545541260545\n",
      "Current iteration=6000, the loss=18881.552667980213, gradient=0.03626639788751793\n",
      "Current iteration=6100, the loss=18881.510212049896, gradient=0.03616276148219412\n",
      "Current iteration=6200, the loss=18881.468533479012, gradient=0.036069263214304856\n",
      "Current iteration=6300, the loss=18881.427546417755, gradient=0.035984696371869865\n",
      "Current iteration=6400, the loss=18881.3871742825, gradient=0.03590800045414524\n",
      "Current iteration=6500, the loss=18881.347348768923, gradient=0.035838243394533655\n",
      "Current iteration=6600, the loss=18881.308008970664, gradient=0.03577460592103253\n",
      "Current iteration=6700, the loss=18881.269100592115, gradient=0.035716367807699954\n",
      "Current iteration=6800, the loss=18881.230575245165, gradient=0.03566289579585593\n",
      "Current iteration=6900, the loss=18881.192389820946, gradient=0.03561363298727675\n",
      "Current iteration=7000, the loss=18881.154505928378, gradient=0.03556808953334827\n",
      "Current iteration=7100, the loss=18881.11688939252, gradient=0.035525834463959426\n",
      "Current iteration=7200, the loss=18881.079509806146, gradient=0.035486488517879104\n",
      "Current iteration=7300, the loss=18881.042340128944, gradient=0.03544971785251159\n",
      "Current iteration=7400, the loss=18881.00535632921, gradient=0.03541522852539606\n",
      "Current iteration=7500, the loss=18880.968537063505, gradient=0.035382761652709684\n",
      "Current iteration=7600, the loss=18880.93186339019, gradient=0.03535208916147989\n",
      "Current iteration=7700, the loss=18880.89531851333, gradient=0.03532301006235595\n",
      "Current iteration=7800, the loss=18880.85888755353, gradient=0.035295347178737245\n",
      "Current iteration=7900, the loss=18880.822557343134, gradient=0.035268944275953125\n",
      "Current iteration=8000, the loss=18880.78631624288, gradient=0.03524366354112866\n",
      "Current iteration=8100, the loss=18880.75015397799, gradient=0.03521938337048313\n",
      "Current iteration=8200, the loss=18880.71406149149, gradient=0.03519599642615326\n",
      "Current iteration=8300, the loss=18880.678030812993, gradient=0.03517340792934481\n",
      "Current iteration=8400, the loss=18880.64205494129, gradient=0.035151534160722364\n",
      "Current iteration=8500, the loss=18880.606127739324, gradient=0.035130301142573594\n",
      "Current iteration=8600, the loss=18880.57024384018, gradient=0.03510964348042124\n",
      "Current iteration=8700, the loss=18880.534398563002, gradient=0.03508950334454518\n",
      "Current iteration=8800, the loss=18880.49858783775, gradient=0.03506982957428375\n",
      "Current iteration=8900, the loss=18880.462808137843, gradient=0.03505057689011067\n",
      "Current iteration=9000, the loss=18880.427056419958, gradient=0.03503170520033564\n",
      "Current iteration=9100, the loss=18880.391330070117, gradient=0.03501317899090485\n",
      "Current iteration=9200, the loss=18880.355626855464, gradient=0.03499496678819426\n",
      "Current iteration=9300, the loss=18880.319944881157, gradient=0.03497704068593614\n",
      "Current iteration=9400, the loss=18880.28428255181, gradient=0.034959375928508604\n",
      "Current iteration=9500, the loss=18880.248638536934, gradient=0.03494195054376931\n",
      "Current iteration=9600, the loss=18880.213011740194, gradient=0.034924745019454966\n",
      "Current iteration=9700, the loss=18880.177401271776, gradient=0.03490774201789534\n",
      "Current iteration=9800, the loss=18880.141806423755, gradient=0.03489092612443257\n",
      "Current iteration=9900, the loss=18880.106226648124, gradient=0.03487428362550237\n",
      "Final loss=18880.07101711665\n",
      "w0\n",
      "Current iteration=0, the loss=51147.33045351836, gradient=72.90404686155327\n",
      "Current iteration=100, the loss=55152.64058659032, gradient=4.728878447315115\n",
      "Current iteration=200, the loss=54606.786542033136, gradient=3.6235279474677147\n",
      "Current iteration=300, the loss=54212.09830286332, gradient=2.861579486911678\n",
      "Current iteration=400, the loss=53920.60318845821, gradient=2.317218102848628\n",
      "Current iteration=500, the loss=53701.72021500665, gradient=1.9208041856627267\n",
      "Current iteration=600, the loss=53534.96566449089, gradient=1.627342972156871\n",
      "Current iteration=700, the loss=53406.19729783035, gradient=1.405915927731353\n",
      "Current iteration=800, the loss=53305.44509805643, gradient=1.2350773800795525\n",
      "Current iteration=900, the loss=53225.57054854279, gradient=1.1000940565128303\n",
      "Current iteration=1000, the loss=53161.40066927598, gradient=0.9909563871900471\n",
      "Current iteration=1100, the loss=53109.150452882706, gradient=0.9008949695372148\n",
      "Current iteration=1200, the loss=53066.02801103456, gradient=0.8253065004933056\n",
      "Current iteration=1300, the loss=53029.95937467528, gradient=0.761009238576072\n",
      "Current iteration=1400, the loss=52999.39379041525, gradient=0.7057464536959364\n",
      "Current iteration=1500, the loss=52973.16435892013, gradient=0.6578648208854941\n",
      "Current iteration=1600, the loss=52950.38738324249, gradient=0.6161102157064083\n",
      "Current iteration=1700, the loss=52930.389160897335, gradient=0.5794994851997165\n",
      "Current iteration=1800, the loss=52912.6524362987, gradient=0.5472401131094826\n",
      "Current iteration=1900, the loss=52896.77705127735, gradient=0.5186794992916784\n",
      "Current iteration=2000, the loss=52882.45091303054, gradient=0.4932722869527866\n",
      "Current iteration=2100, the loss=52869.42849630141, gradient=0.4705585781150277\n",
      "Current iteration=2200, the loss=52857.514869126506, gradient=0.4501486908581967\n",
      "Current iteration=2300, the loss=52846.55378146017, gradient=0.43171187095452784\n",
      "Current iteration=2400, the loss=52836.41875091257, gradient=0.4149674497194042\n",
      "Current iteration=2500, the loss=52827.00636531687, gradient=0.3996775879895969\n",
      "Current iteration=2600, the loss=52818.23122924752, gradient=0.38564112505761855\n",
      "Current iteration=2700, the loss=52810.0221328893, gradient=0.37268826540241934\n",
      "Current iteration=2800, the loss=52802.319132327684, gradient=0.3606759516212138\n",
      "Current iteration=2900, the loss=52795.07131149524, gradient=0.34948383082847573\n",
      "Current iteration=3000, the loss=52788.23505565138, gradient=0.33901074957686655\n",
      "Current iteration=3100, the loss=52781.77271017725, gradient=0.32917172429848085\n",
      "Current iteration=3200, the loss=52775.65153083844, gradient=0.3198953390022961\n",
      "Current iteration=3300, the loss=52769.842855569674, gradient=0.31112152408449795\n",
      "Current iteration=3400, the loss=52764.32144551169, gradient=0.3027996718154076\n",
      "Current iteration=3500, the loss=52759.0649561231, gradient=0.29488704623243633\n",
      "Current iteration=3600, the loss=52754.053508903045, gradient=0.28734744798222694\n",
      "Current iteration=3700, the loss=52749.26934148114, gradient=0.2801500979986119\n",
      "Current iteration=3800, the loss=52744.696519209436, gradient=0.2732687075560433\n",
      "Current iteration=3900, the loss=52740.32069540754, gradient=0.2666807059797903\n",
      "Current iteration=4000, the loss=52736.12891041735, gradient=0.2603666009455186\n",
      "Current iteration=4100, the loss=52732.109421883535, gradient=0.2543094497358011\n",
      "Current iteration=4200, the loss=52728.251560375924, gradient=0.24849442296290195\n",
      "Current iteration=4300, the loss=52724.54560575808, gradient=0.24290844507831982\n",
      "Current iteration=4400, the loss=52720.982680684916, gradient=0.23753989846176243\n",
      "Current iteration=4500, the loss=52717.55465835868, gradient=0.23237838002633723\n",
      "Current iteration=4600, the loss=52714.25408224774, gradient=0.227414501115769\n",
      "Current iteration=4700, the loss=52711.0740959151, gradient=0.22263972303254612\n",
      "Current iteration=4800, the loss=52708.008381449035, gradient=0.21804622185465392\n",
      "Current iteration=4900, the loss=52705.051105258935, gradient=0.2136267773044938\n",
      "Current iteration=5000, the loss=52702.19687021211, gradient=0.2093746813563229\n",
      "Current iteration=5100, the loss=52699.44067325782, gradient=0.2052836630352366\n",
      "Current iteration=5200, the loss=52696.777867821, gradient=0.20134782649561256\n",
      "Current iteration=5300, the loss=52694.204130358135, gradient=0.19756159999097625\n",
      "Current iteration=5400, the loss=52691.71543055835, gradient=0.19391969377893897\n",
      "Current iteration=5500, the loss=52689.308004746126, gradient=0.19041706535947564\n",
      "Current iteration=5600, the loss=52686.978332104365, gradient=0.1870488907358215\n",
      "Current iteration=5700, the loss=52684.723113387, gradient=0.18381054062549382\n",
      "Current iteration=5800, the loss=52682.53925183435, gradient=0.18069756074386756\n",
      "Current iteration=5900, the loss=52680.42383604063, gradient=0.1777056554418921\n",
      "Current iteration=6000, the loss=52678.37412455429, gradient=0.17483067410947103\n",
      "Current iteration=6100, the loss=52676.3875320191, gradient=0.17206859986195397\n",
      "Current iteration=6200, the loss=52674.46161668653, gradient=0.16941554011349616\n",
      "Current iteration=6300, the loss=52672.5940691501, gradient=0.16686771871134304\n",
      "Current iteration=6400, the loss=52670.78270217011, gradient=0.16442146936235913\n",
      "Current iteration=6500, the loss=52669.02544147191, gradient=0.1620732301297349\n",
      "Current iteration=6600, the loss=52667.32031741441, gradient=0.1598195388158242\n",
      "Current iteration=6700, the loss=52665.66545743709, gradient=0.15765702907803786\n",
      "Current iteration=6800, the loss=52664.05907920419, gradient=0.15558242715010245\n",
      "Current iteration=6900, the loss=52662.49948437349, gradient=0.15359254906169875\n",
      "Current iteration=7000, the loss=52660.98505292527, gradient=0.15168429826658855\n",
      "Current iteration=7100, the loss=52659.514237994386, gradient=0.14985466360340807\n",
      "Current iteration=7200, the loss=52658.085561153785, gradient=0.1481007175249958\n",
      "Current iteration=7300, the loss=52656.69760810434, gradient=0.14641961454185404\n",
      "Current iteration=7400, the loss=52655.34902473007, gradient=0.1448085898335758\n",
      "Current iteration=7500, the loss=52654.03851348245, gradient=0.14326495798897124\n",
      "Current iteration=7600, the loss=52652.76483006147, gradient=0.1417861118415996\n",
      "Current iteration=7700, the loss=52651.526780364315, gradient=0.14036952137247627\n",
      "Current iteration=7800, the loss=52650.32321767564, gradient=0.13901273265618538\n",
      "Current iteration=7900, the loss=52649.1530400765, gradient=0.13771336683047555\n",
      "Current iteration=8000, the loss=52648.01518805095, gradient=0.13646911907283682\n",
      "Current iteration=8100, the loss=52646.90864227168, gradient=0.13527775757052996\n",
      "Current iteration=8200, the loss=52645.832421548075, gradient=0.13413712247324167\n",
      "Current iteration=8300, the loss=52644.78558092173, gradient=0.1330451248198554\n",
      "Current iteration=8400, the loss=52643.76720989596, gradient=0.13199974543295834\n",
      "Current iteration=8500, the loss=52642.776430787264, gradient=0.13099903377651265\n",
      "Current iteration=8600, the loss=52641.81239718784, gradient=0.13004110677379707\n",
      "Current iteration=8700, the loss=52640.87429252955, gradient=0.1291241475841101\n",
      "Current iteration=8800, the loss=52639.961328740486, gradient=0.12824640433798304\n",
      "Current iteration=8900, the loss=52639.07274498606, gradient=0.12740618883171342\n",
      "Current iteration=9000, the loss=52638.207806487975, gradient=0.12660187518292879\n",
      "Current iteration=9100, the loss=52637.36580341421, gradient=0.12583189844963327\n",
      "Current iteration=9200, the loss=52636.546049834426, gradient=0.12509475321581723\n",
      "Current iteration=9300, the loss=52635.74788273546, gradient=0.12438899214716102\n",
      "Current iteration=9400, the loss=52634.97066109245, gradient=0.12371322452078215\n",
      "Current iteration=9500, the loss=52634.21376499096, gradient=0.12306611473317167\n",
      "Current iteration=9600, the loss=52633.47659479648, gradient=0.1224463807906812\n",
      "Current iteration=9700, the loss=52632.75857036758, gradient=0.12185279278698015\n",
      "Current iteration=9800, the loss=52632.05913030976, gradient=0.1212841713719129\n",
      "Current iteration=9900, the loss=52631.377731266955, gradient=0.1207393862161303\n",
      "Final loss=52630.720401074606\n",
      "w1_nomass\n",
      "Current iteration=0, the loss=5241.578979394306, gradient=32.128825132346805\n",
      "Current iteration=100, the loss=5528.041426938657, gradient=2.2713793187345996\n",
      "Current iteration=200, the loss=5517.510134607969, gradient=1.6160924960433563\n",
      "Current iteration=300, the loss=5510.509642355212, gradient=1.2211729271411016\n",
      "Current iteration=400, the loss=5505.606242683331, gradient=0.9618081452918907\n",
      "Current iteration=500, the loss=5502.039638696112, gradient=0.7775784465106116\n",
      "Current iteration=600, the loss=5499.3723757640755, gradient=0.6391761794443576\n",
      "Current iteration=700, the loss=5497.335932810403, gradient=0.5314947346772644\n",
      "Current iteration=800, the loss=5495.756595412895, gradient=0.4459158011561204\n",
      "Current iteration=900, the loss=5494.516956304491, gradient=0.37696758151164084\n",
      "Current iteration=1000, the loss=5493.5347039944645, gradient=0.3208701837065595\n",
      "Current iteration=1100, the loss=5492.750362609851, gradient=0.2748659624282519\n",
      "Current iteration=1200, the loss=5492.119900582275, gradient=0.23687875541768916\n",
      "Current iteration=1300, the loss=5491.6100945697945, gradient=0.20531781780098696\n",
      "Current iteration=1400, the loss=5491.19550851462, gradient=0.17895150112770355\n",
      "Current iteration=1500, the loss=5490.8564527267945, gradient=0.15681897601382444\n",
      "Current iteration=1600, the loss=5490.577558917285, gradient=0.1381655137249276\n",
      "Current iteration=1700, the loss=5490.346756406469, gradient=0.12239388007493783\n",
      "Current iteration=1800, the loss=5490.154518875693, gradient=0.10902744807593043\n",
      "Current iteration=1900, the loss=5489.993299551692, gradient=0.09768210931709356\n",
      "Current iteration=2000, the loss=5489.857101411758, gradient=0.0880448892935893\n",
      "Current iteration=2100, the loss=5489.741146462303, gradient=0.0798577072540047\n",
      "Current iteration=2200, the loss=5489.641619113187, gradient=0.07290510331596192\n",
      "Current iteration=2300, the loss=5489.555465795067, gradient=0.06700503722583298\n",
      "Current iteration=2400, the loss=5489.480237752508, gradient=0.06200206738805942\n",
      "Current iteration=2500, the loss=5489.413967263165, gradient=0.05776236152829259\n",
      "Current iteration=2600, the loss=5489.3550698991585, gradient=0.05417008833869547\n",
      "Current iteration=2700, the loss=5489.302267174217, gradient=0.05112481116114629\n",
      "Current iteration=2800, the loss=5489.25452520638, gradient=0.0485395669823778\n",
      "Current iteration=2900, the loss=5489.211005998093, gradient=0.04633937737281642\n",
      "Current iteration=3000, the loss=5489.171028678656, gradient=0.044460004698708665\n",
      "Current iteration=3100, the loss=5489.134038627173, gradient=0.042846832266900635\n",
      "Current iteration=3200, the loss=5489.099582839055, gradient=0.04145380365748024\n",
      "Current iteration=3300, the loss=5489.0672902460465, gradient=0.04024239866539358\n",
      "Current iteration=3400, the loss=5489.036855971506, gradient=0.03918064924924968\n",
      "Current iteration=3500, the loss=5489.008028715767, gradient=0.03824221066207919\n",
      "Current iteration=3600, the loss=5488.98060063431, gradient=0.03740550461135375\n",
      "Current iteration=3700, the loss=5488.954399203524, gradient=0.036652947272511255\n",
      "Current iteration=3800, the loss=5488.929280673302, gradient=0.035970268812329814\n",
      "Current iteration=3900, the loss=5488.905124788049, gradient=0.03534592504870916\n",
      "Current iteration=4000, the loss=5488.881830522961, gradient=0.034770597115880646\n",
      "Current iteration=4100, the loss=5488.85931263406, gradient=0.034236771844196136\n",
      "Current iteration=4200, the loss=5488.837498861358, gradient=0.033738393869939945\n",
      "Current iteration=4300, the loss=5488.816327657067, gradient=0.03327057993833968\n",
      "Current iteration=4400, the loss=5488.795746336488, gradient=0.03282938609569744\n",
      "Current iteration=4500, the loss=5488.775709569763, gradient=0.03241161917517132\n",
      "Current iteration=4600, the loss=5488.756178148943, gradient=0.032014684930475594\n",
      "Current iteration=4700, the loss=5488.737117977846, gradient=0.03163646620083015\n",
      "Current iteration=4800, the loss=5488.718499242499, gradient=0.03127522549707412\n",
      "Current iteration=4900, the loss=5488.700295728273, gradient=0.030929527325833046\n",
      "Current iteration=5000, the loss=5488.682484256352, gradient=0.030598176388950987\n",
      "Current iteration=5100, the loss=5488.665044217553, gradient=0.030280168501514968\n",
      "Current iteration=5200, the loss=5488.647957185654, gradient=0.029974651667655536\n",
      "Current iteration=5300, the loss=5488.631206595847, gradient=0.02968089524843824\n",
      "Current iteration=5400, the loss=5488.614777476633, gradient=0.02939826556317957\n",
      "Current iteration=5500, the loss=5488.598656225676, gradient=0.02912620659704121\n",
      "Current iteration=5600, the loss=5488.582830421876, gradient=0.028864224756088127\n",
      "Current iteration=5700, the loss=5488.567288667399, gradient=0.02861187682694713\n",
      "Current iteration=5800, the loss=5488.552020454499, gradient=0.028368760471357606\n",
      "Current iteration=5900, the loss=5488.537016052922, gradient=0.028134506724245133\n",
      "Current iteration=6000, the loss=5488.522266414448, gradient=0.027908774074214396\n",
      "Current iteration=6100, the loss=5488.50776309172, gradient=0.027691243793038965\n",
      "Current iteration=6200, the loss=5488.4934981690385, gradient=0.027481616250387815\n",
      "Current iteration=6300, the loss=5488.479464203152, gradient=0.02727960800521641\n",
      "Current iteration=6400, the loss=5488.465654172482, gradient=0.027084949509033722\n",
      "Current iteration=6500, the loss=5488.452061433441, gradient=0.026897383290862047\n",
      "Current iteration=6600, the loss=5488.438679682725, gradient=0.026716662521109477\n",
      "Current iteration=6700, the loss=5488.425502924695, gradient=0.026542549873233685\n",
      "Current iteration=6800, the loss=5488.41252544303, gradient=0.026374816619190486\n",
      "Current iteration=6900, the loss=5488.3997417760675, gradient=0.026213241908185008\n",
      "Current iteration=7000, the loss=5488.387146695241, gradient=0.02605761218892312\n",
      "Current iteration=7100, the loss=5488.374735186191, gradient=0.025907720744005772\n",
      "Current iteration=7200, the loss=5488.362502432148, gradient=0.025763367311759065\n",
      "Current iteration=7300, the loss=5488.35044379929, gradient=0.025624357776063318\n",
      "Current iteration=7400, the loss=5488.338554823767, gradient=0.025490503908890336\n",
      "Current iteration=7500, the loss=5488.326831200193, gradient=0.025361623153546647\n",
      "Current iteration=7600, the loss=5488.315268771388, gradient=0.02523753843920212\n",
      "Current iteration=7700, the loss=5488.303863519207, gradient=0.025118078019330907\n",
      "Current iteration=7800, the loss=5488.29261155632, gradient=0.025003075328307752\n",
      "Current iteration=7900, the loss=5488.281509118803, gradient=0.02489236885167561\n",
      "Current iteration=8000, the loss=5488.270552559466, gradient=0.024785802006605108\n",
      "Current iteration=8100, the loss=5488.2597383417915, gradient=0.024683223029864827\n",
      "Current iteration=8200, the loss=5488.249063034425, gradient=0.024584484871239248\n",
      "Current iteration=8300, the loss=5488.23852330616, gradient=0.024489445090836122\n",
      "Current iteration=8400, the loss=5488.228115921336, gradient=0.024397965759099926\n",
      "Current iteration=8500, the loss=5488.217837735621, gradient=0.02430991335866667\n",
      "Current iteration=8600, the loss=5488.20768569212, gradient=0.02422515868741996\n",
      "Current iteration=8700, the loss=5488.197656817786, gradient=0.024143576762312477\n",
      "Current iteration=8800, the loss=5488.187748220081, gradient=0.02406504672364556\n",
      "Current iteration=8900, the loss=5488.177957083888, gradient=0.023989451739624055\n",
      "Current iteration=9000, the loss=5488.168280668625, gradient=0.023916678911087307\n",
      "Current iteration=9100, the loss=5488.1587163055365, gradient=0.02384661917637816\n",
      "Current iteration=9200, the loss=5488.1492613951705, gradient=0.023779167216370994\n",
      "Current iteration=9300, the loss=5488.139913404997, gradient=0.023714221359706297\n",
      "Current iteration=9400, the loss=5488.130669867171, gradient=0.02365168348831238\n",
      "Current iteration=9500, the loss=5488.121528376406, gradient=0.02359145894331049\n",
      "Current iteration=9600, the loss=5488.112486587991, gradient=0.023533456431409933\n",
      "Current iteration=9700, the loss=5488.10354221587, gradient=0.023477587931907705\n",
      "Current iteration=9800, the loss=5488.094693030854, gradient=0.023423768604413413\n",
      "Current iteration=9900, the loss=5488.085936858891, gradient=0.02337191669740895\n",
      "Final loss=5488.077357789149\n",
      "w1\n",
      "Current iteration=0, the loss=48507.82598994609, gradient=133.89079051245764\n",
      "Current iteration=100, the loss=51723.14055652816, gradient=3.0625515482943704\n",
      "Current iteration=200, the loss=51469.727929528686, gradient=2.4019434843335326\n",
      "Current iteration=300, the loss=51286.07045682593, gradient=1.936742821622469\n",
      "Current iteration=400, the loss=51149.78175953611, gradient=1.6093279398151366\n",
      "Current iteration=500, the loss=51046.848209686636, gradient=1.3772829559692144\n",
      "Current iteration=600, the loss=50967.83761434286, gradient=1.2098189412246008\n",
      "Current iteration=700, the loss=50906.22323992908, gradient=1.0853273383081017\n",
      "Current iteration=800, the loss=50857.41202759077, gradient=0.9892302454886273\n",
      "Current iteration=900, the loss=50818.129603520065, gradient=0.9120301363089142\n",
      "Current iteration=1000, the loss=50786.01660294321, gradient=0.8476853471013427\n",
      "Current iteration=1100, the loss=50759.35733327627, gradient=0.792392341810983\n",
      "Current iteration=1200, the loss=50736.89394343813, gradient=0.74374868935032\n",
      "Current iteration=1300, the loss=50717.6971966434, gradient=0.7002120996813882\n",
      "Current iteration=1400, the loss=50701.075489332274, gradient=0.6607645902361688\n",
      "Current iteration=1500, the loss=50686.51016029452, gradient=0.6247089660044309\n",
      "Current iteration=1600, the loss=50673.6091277582, gradient=0.5915469809359607\n",
      "Current iteration=1700, the loss=50662.07344890114, gradient=0.5609066207161573\n",
      "Current iteration=1800, the loss=50651.673072503734, gradient=0.5324984993441195\n",
      "Current iteration=1900, the loss=50642.22917807097, gradient=0.5060893998639399\n",
      "Current iteration=2000, the loss=50633.6012603724, gradient=0.4814859077377742\n",
      "Current iteration=2100, the loss=50625.67764842796, gradient=0.4585240160069444\n",
      "Current iteration=2200, the loss=50618.36851944926, gradient=0.43706230096023113\n",
      "Current iteration=2300, the loss=50611.60073109259, gradient=0.41697726730187284\n",
      "Current iteration=2400, the loss=50605.31398275666, gradient=0.39816004126844323\n",
      "Current iteration=2500, the loss=50599.457951010045, gradient=0.3805139255522021\n",
      "Current iteration=2600, the loss=50593.99014100089, gradient=0.36395252442662906\n",
      "Current iteration=2700, the loss=50588.87426564399, gradient=0.3483982608452836\n",
      "Current iteration=2800, the loss=50584.07901507604, gradient=0.33378117388229833\n",
      "Current iteration=2900, the loss=50579.5771157007, gradient=0.3200379244550742\n",
      "Current iteration=3000, the loss=50575.344604954065, gradient=0.3071109611602007\n",
      "Current iteration=3100, the loss=50571.36026747099, gradient=0.2949478127819728\n",
      "Current iteration=3200, the loss=50567.60519261079, gradient=0.28350048335611017\n",
      "Current iteration=3300, the loss=50564.062423747615, gradient=0.272724931763155\n",
      "Current iteration=3400, the loss=50560.71667738407, gradient=0.2625806219517702\n",
      "Current iteration=3500, the loss=50557.55411576718, gradient=0.2530301327936026\n",
      "Current iteration=3600, the loss=50554.56216081686, gradient=0.2440388186890051\n",
      "Current iteration=3700, the loss=50551.72934022448, gradient=0.23557451364113605\n",
      "Current iteration=3800, the loss=50549.045158828965, gradient=0.22760727275691628\n",
      "Current iteration=3900, the loss=50546.499990045006, gradient=0.22010914611882573\n",
      "Current iteration=4000, the loss=50544.08498335755, gradient=0.21305398076673726\n",
      "Current iteration=4100, the loss=50541.791984820215, gradient=0.20641724717815615\n",
      "Current iteration=4200, the loss=50539.61346818748, gradient=0.2001758871683401\n",
      "Current iteration=4300, the loss=50537.542474830596, gradient=0.19430818057142468\n",
      "Current iteration=4400, the loss=50535.57256098073, gradient=0.18879362842613415\n",
      "Current iteration=4500, the loss=50533.69775114184, gradient=0.18361285068891545\n",
      "Current iteration=4600, the loss=50531.912496744866, gradient=0.17874749674378\n",
      "Current iteration=4700, the loss=50530.21163929049, gradient=0.1741801671816275\n",
      "Current iteration=4800, the loss=50528.59037736552, gradient=0.16989434549013654\n",
      "Current iteration=4900, the loss=50527.04423702413, gradient=0.16587433843566127\n",
      "Current iteration=5000, the loss=50525.56904511058, gradient=0.16210522403705158\n",
      "Current iteration=5100, the loss=50524.16090516661, gradient=0.15857280613325095\n",
      "Current iteration=5200, the loss=50522.81617562152, gradient=0.15526357463637885\n",
      "Current iteration=5300, the loss=50521.53145000644, gradient=0.15216467064339279\n",
      "Current iteration=5400, the loss=50520.303538970344, gradient=0.14926385565517322\n",
      "Current iteration=5500, the loss=50519.129453905356, gradient=0.14654948422404723\n",
      "Current iteration=5600, the loss=50518.00639201324, gradient=0.14401047942064088\n",
      "Current iteration=5700, the loss=50516.93172266637, gradient=0.1416363105792774\n",
      "Current iteration=5800, the loss=50515.90297493348, gradient=0.13941697284797666\n",
      "Current iteration=5900, the loss=50514.917826156285, gradient=0.1373429681342964\n",
      "Current iteration=6000, the loss=50513.974091475495, gradient=0.1354052871011676\n",
      "Current iteration=6100, the loss=50513.069714216035, gradient=0.13359539192681616\n",
      "Current iteration=6200, the loss=50512.20275705151, gradient=0.13190519959912364\n",
      "Current iteration=6300, the loss=50511.371393875546, gradient=0.13032706556651608\n",
      "Current iteration=6400, the loss=50510.5739023162, gradient=0.12885376761425357\n",
      "Current iteration=6500, the loss=50509.80865683522, gradient=0.12747848987620294\n",
      "Current iteration=6600, the loss=50509.07412236043, gradient=0.12619480692772975\n",
      "Current iteration=6700, the loss=50508.368848404396, gradient=0.12499666793507326\n",
      "Current iteration=6800, the loss=50507.69146362701, gradient=0.12387838086069283\n",
      "Current iteration=6900, the loss=50507.040670804185, gradient=0.12283459674284433\n",
      "Current iteration=7000, the loss=50506.41524216764, gradient=0.12186029408149388\n",
      "Current iteration=7100, the loss=50505.81401508506, gradient=0.12095076337215978\n",
      "Current iteration=7200, the loss=50505.23588805196, gradient=0.12010159183490153\n",
      "Current iteration=7300, the loss=50504.679816969874, gradient=0.11930864838809982\n",
      "Current iteration=7400, the loss=50504.14481168703, gradient=0.11856806891646365\n",
      "Current iteration=7500, the loss=50503.62993278103, gradient=0.11787624188041941\n",
      "Current iteration=7600, the loss=50503.1342885634, gradient=0.11722979431021077\n",
      "Current iteration=7700, the loss=50502.65703228922, gradient=0.11662557822313428\n",
      "Current iteration=7800, the loss=50502.19735955518, gradient=0.11606065749672033\n",
      "Current iteration=7900, the loss=50501.75450587171, gradient=0.11553229522475758\n",
      "Current iteration=8000, the loss=50501.32774439598, gradient=0.1150379415769943\n",
      "Current iteration=8100, the loss=50500.91638381317, gradient=0.1145752221775159\n",
      "Current iteration=8200, the loss=50500.51976635528, gradient=0.11414192701117513\n",
      "Current iteration=8300, the loss=50500.13726594685, gradient=0.11373599986231128\n",
      "Current iteration=8400, the loss=50499.76828646858, gradient=0.11335552828530925\n",
      "Current iteration=8500, the loss=50499.41226013007, gradient=0.11299873410239979\n",
      "Current iteration=8600, the loss=50499.068645943844, gradient=0.11266396442051306\n",
      "Current iteration=8700, the loss=50498.73692829339, gradient=0.11234968315592951\n",
      "Current iteration=8800, the loss=50498.41661558865, gradient=0.11205446305294352\n",
      "Current iteration=8900, the loss=50498.10723900287, gradient=0.11177697818071029\n",
      "Current iteration=9000, the loss=50497.80835128493, gradient=0.11151599689084533\n",
      "Current iteration=9100, the loss=50497.51952564224, gradient=0.11127037521716075\n",
      "Current iteration=9200, the loss=50497.240354689384, gradient=0.11103905069810135\n",
      "Current iteration=9300, the loss=50496.97044945789, gradient=0.11082103660193802\n",
      "Current iteration=9400, the loss=50496.70943846333, gradient=0.11061541653453946\n",
      "Current iteration=9500, the loss=50496.45696682574, gradient=0.11042133940955651\n",
      "Current iteration=9600, the loss=50496.21269543997, gradient=0.11023801476104707\n",
      "Current iteration=9700, the loss=50495.976300192866, gradient=0.11006470837893079\n",
      "Current iteration=9800, the loss=50495.747471224015, gradient=0.10990073824816365\n",
      "Current iteration=9900, the loss=50495.52591222753, gradient=0.10974547077309868\n",
      "Final loss=50495.31345183324\n",
      "w23_nomass\n",
      "Current iteration=0, the loss=3069.948862699998, gradient=105.42502342582894\n",
      "Current iteration=100, the loss=3170.0590789183457, gradient=2.4132146810221005\n",
      "Current iteration=200, the loss=3164.3190070789983, gradient=1.6143367287427153\n",
      "Current iteration=300, the loss=3161.1606517964315, gradient=1.2171007641460818\n",
      "Current iteration=400, the loss=3159.1278217074446, gradient=0.9549665992963768\n",
      "Current iteration=500, the loss=3157.6908728161975, gradient=0.7672228486727937\n",
      "Current iteration=600, the loss=3156.627581243956, gradient=0.6292131303128271\n",
      "Current iteration=700, the loss=3155.8245838577377, gradient=0.5262545135840444\n",
      "Current iteration=800, the loss=3155.212791118003, gradient=0.4484589769526668\n",
      "Current iteration=900, the loss=3154.7450462216493, gradient=0.3889113534066784\n",
      "Current iteration=1000, the loss=3154.387229563845, gradient=0.34269032096593066\n",
      "Current iteration=1100, the loss=3154.1139408235117, gradient=0.3062547637199785\n",
      "Current iteration=1200, the loss=3153.90598142632, gradient=0.27703926777399185\n",
      "Current iteration=1300, the loss=3153.7487005145363, gradient=0.2531793120098763\n",
      "Current iteration=1400, the loss=3153.6308378172075, gradient=0.23331911027347804\n",
      "Current iteration=1500, the loss=3153.5436883842785, gradient=0.21647375765054067\n",
      "Current iteration=1600, the loss=3153.4804891752974, gradient=0.20192859742871255\n",
      "Current iteration=1700, the loss=3153.4359633841254, gradient=0.18916520392615446\n",
      "Current iteration=1800, the loss=3153.405978853892, gradient=0.1778069660403305\n",
      "Current iteration=1900, the loss=3153.387289977519, gradient=0.1675792886548521\n",
      "Current iteration=2000, the loss=3153.377341267786, gradient=0.15828069243770368\n",
      "Current iteration=2100, the loss=3153.374116868942, gradient=0.14976197907371155\n",
      "Current iteration=2200, the loss=3153.3760245637386, gradient=0.14191130606128233\n",
      "Current iteration=2300, the loss=3153.3818058733905, gradient=0.1346435485197732\n",
      "Current iteration=2400, the loss=3153.390466031074, gradient=0.12789274390217767\n",
      "Current iteration=2500, the loss=3153.401219188999, gradient=0.12160673801757126\n",
      "Current iteration=2600, the loss=3153.413445371223, gradient=0.11574339429453703\n",
      "Current iteration=2700, the loss=3153.426656531828, gradient=0.11026790873157667\n",
      "Current iteration=2800, the loss=3153.440469706273, gradient=0.10515090476970915\n",
      "Current iteration=2900, the loss=3153.4545857130092, gradient=0.10036707740217632\n",
      "Current iteration=3000, the loss=3153.4687722155118, gradient=0.0958942238163005\n",
      "Current iteration=3100, the loss=3153.4828502223604, gradient=0.09171254614303401\n",
      "Current iteration=3200, the loss=3153.496683306933, gradient=0.08780414601246184\n",
      "Current iteration=3300, the loss=3153.51016898468, gradient=0.08415265464595179\n",
      "Current iteration=3400, the loss=3153.523231806572, gradient=0.08074295909894603\n",
      "Current iteration=3500, the loss=3153.5358178208235, gradient=0.07756099710715753\n",
      "Current iteration=3600, the loss=3153.5478901277984, gradient=0.07459360127713745\n",
      "Current iteration=3700, the loss=3153.559425309948, gradient=0.07182837915490489\n",
      "Current iteration=3800, the loss=3153.5704105633477, gradient=0.06925361974581126\n",
      "Current iteration=3900, the loss=3153.580841392582, gradient=0.06685821986642113\n",
      "Current iteration=4000, the loss=3153.590719758603, gradient=0.06463162565097974\n",
      "Current iteration=4100, the loss=3153.600052591199, gradient=0.06256378586854627\n",
      "Current iteration=4200, the loss=3153.608850595273, gradient=0.06064511461408074\n",
      "Current iteration=4300, the loss=3153.617127294051, gradient=0.058866461546793585\n",
      "Current iteration=4400, the loss=3153.624898263519, gradient=0.057219088254596324\n",
      "Current iteration=4500, the loss=3153.6321805212547, gradient=0.05569464959102362\n",
      "Current iteration=4600, the loss=3153.6389920399984, gradient=0.054285179008833555\n",
      "Current iteration=4700, the loss=3153.6453513619845, gradient=0.052983077037240325\n",
      "Current iteration=4800, the loss=3153.6512772947094, gradient=0.05178110214191936\n",
      "Current iteration=4900, the loss=3153.656788672472, gradient=0.05067236328515201\n",
      "Current iteration=5000, the loss=3153.661904171021, gradient=0.04965031357821872\n",
      "Current iteration=5100, the loss=3153.6666421650616, gradient=0.04870874449506371\n",
      "Current iteration=5200, the loss=3153.671020620292, gradient=0.04784178019728733\n",
      "Current iteration=5300, the loss=3153.6750570132426, gradient=0.04704387160512191\n",
      "Current iteration=5400, the loss=3153.6787682734357, gradient=0.04630978993485662\n",
      "Current iteration=5500, the loss=3153.6821707434287, gradient=0.04563461950719597\n",
      "Current iteration=5600, the loss=3153.6852801531386, gradient=0.045013749709933944\n",
      "Current iteration=5700, the loss=3153.6881116055165, gradient=0.04444286606929007\n",
      "Current iteration=5800, the loss=3153.690679571204, gradient=0.04391794044506198\n",
      "Current iteration=5900, the loss=3153.692997890233, gradient=0.04343522041410782\n",
      "Current iteration=6000, the loss=3153.6950797792174, gradient=0.042991217944084184\n",
      "Current iteration=6100, the loss=3153.6969378427566, gradient=0.042582697485175146\n",
      "Current iteration=6200, the loss=3153.69858408804, gradient=0.042206663622709875\n",
      "Current iteration=6300, the loss=3153.7000299418164, gradient=0.04186034843938445\n",
      "Current iteration=6400, the loss=3153.701286269049, gradient=0.041541198734015426\n",
      "Current iteration=6500, the loss=3153.702363392746, gradient=0.04124686323593182\n",
      "Current iteration=6600, the loss=3153.703271114508, gradient=0.04097517994200815\n",
      "Current iteration=6700, the loss=3153.704018735465, gradient=0.04072416368840327\n",
      "Current iteration=6800, the loss=3153.704615077329, gradient=0.040491994052597194\n",
      "Current iteration=6900, the loss=3153.7050685033373, gradient=0.04027700366439484\n",
      "Current iteration=7000, the loss=3153.705386938932, gradient=0.040077666987983894\n",
      "Current iteration=7100, the loss=3153.705577892038, gradient=0.03989258962151435\n",
      "Current iteration=7200, the loss=3153.7056484728473, gradient=0.039720498146399635\n",
      "Final loss=3153.7056491805597\n",
      "w23\n",
      "Current iteration=0, the loss=47213.02705666011, gradient=342.65295459263336\n",
      "Current iteration=100, the loss=47441.34643355982, gradient=3.4397665729576867\n",
      "Current iteration=200, the loss=47181.11783235369, gradient=2.0845797947155678\n",
      "Current iteration=300, the loss=47045.9752184256, gradient=1.5934904343149516\n",
      "Current iteration=400, the loss=46964.362187389444, gradient=1.3164659003863166\n",
      "Current iteration=500, the loss=46908.80105383837, gradient=1.1264655905747287\n",
      "Current iteration=600, the loss=46868.246836237944, gradient=0.9865272501935062\n",
      "Current iteration=700, the loss=46837.47993307065, gradient=0.8795550767062116\n",
      "Current iteration=800, the loss=46813.57994624428, gradient=0.7954529147259772\n",
      "Current iteration=900, the loss=46794.69467519799, gradient=0.7276954288735707\n",
      "Current iteration=1000, the loss=46779.5573028899, gradient=0.6719042345852051\n",
      "Current iteration=1100, the loss=46767.26465948404, gradient=0.6250723794289347\n",
      "Current iteration=1200, the loss=46757.157655907795, gradient=0.5850919445199331\n",
      "Current iteration=1300, the loss=46748.748253149475, gradient=0.5504547767858953\n",
      "Current iteration=1400, the loss=46741.67119487611, gradient=0.5200599901591535\n",
      "Current iteration=1500, the loss=46735.65064414028, gradient=0.4930887689256551\n",
      "Current iteration=1600, the loss=46730.476535958136, gradient=0.46892180798264654\n",
      "Current iteration=1700, the loss=46725.987564823765, gradient=0.44708387102493785\n",
      "Current iteration=1800, the loss=46722.058824637104, gradient=0.4272057462022264\n",
      "Current iteration=1900, the loss=46718.59276264332, gradient=0.40899752965106334\n",
      "Current iteration=2000, the loss=46715.51251821329, gradient=0.39222943637910107\n",
      "Current iteration=2100, the loss=46712.756990423346, gradient=0.37671773483662335\n",
      "Current iteration=2200, the loss=46710.2771661443, gradient=0.36231425881845997\n",
      "Current iteration=2300, the loss=46708.03337178865, gradient=0.3488984788853423\n",
      "Current iteration=2400, the loss=46705.99320498237, gradient=0.33637144537142144\n",
      "Current iteration=2500, the loss=46704.12996895485, gradient=0.3246511249827036\n",
      "Current iteration=2600, the loss=46702.42148026569, gradient=0.31366879003106957\n",
      "Current iteration=2700, the loss=46700.849155041484, gradient=0.3033662114346336\n",
      "Current iteration=2800, the loss=46699.39730396267, gradient=0.2936934703682209\n",
      "Current iteration=2900, the loss=46698.052584495665, gradient=0.2846072488396135\n",
      "Current iteration=3000, the loss=46696.80357219832, gradient=0.2760694925797623\n",
      "Current iteration=3100, the loss=46695.6404227005, gradient=0.2680463642675713\n",
      "Current iteration=3200, the loss=46694.55460314484, gradient=0.26050742370717833\n",
      "Current iteration=3300, the loss=46693.53867717119, gradient=0.25342498576579525\n",
      "Current iteration=3400, the loss=46692.58613144656, gradient=0.24677361778509962\n",
      "Current iteration=3500, the loss=46691.691234652484, gradient=0.24052974659710416\n",
      "Current iteration=3600, the loss=46690.848922008605, gradient=0.23467135179061352\n",
      "Current iteration=3700, the loss=46690.0547000318, gradient=0.22917772692380514\n",
      "Current iteration=3800, the loss=46689.30456744777, gradient=0.22402929429474533\n",
      "Current iteration=3900, the loss=46688.59494908938, gradient=0.21920746192082863\n",
      "Current iteration=4000, the loss=46687.92264031153, gradient=0.2146945137389478\n",
      "Current iteration=4100, the loss=46687.284759982686, gradient=0.21047352587558202\n",
      "Current iteration=4200, the loss=46686.678710517765, gradient=0.2065283032705885\n",
      "Current iteration=4300, the loss=46686.10214373075, gradient=0.20284333206453567\n",
      "Current iteration=4400, the loss=46685.552931525956, gradient=0.19940374404952527\n",
      "Current iteration=4500, the loss=46685.02914063706, gradient=0.1961952901938735\n",
      "Current iteration=4600, the loss=46684.529010769664, gradient=0.19320432082425454\n",
      "Current iteration=4700, the loss=46684.05093562107, gradient=0.19041777051711845\n",
      "Current iteration=4800, the loss=46683.59344634328, gradient=0.18782314613818563\n",
      "Current iteration=4900, the loss=46683.15519709025, gradient=0.18540851679198878\n",
      "Current iteration=5000, the loss=46682.73495234954, gradient=0.1831625047154495\n",
      "Current iteration=5100, the loss=46682.33157580836, gradient=0.18107427637925275\n",
      "Current iteration=5200, the loss=46681.944020542185, gradient=0.1791335332549265\n",
      "Current iteration=5300, the loss=46681.5713203483, gradient=0.17733050186876578\n",
      "Current iteration=5400, the loss=46681.21258207239, gradient=0.17565592289962303\n",
      "Current iteration=5500, the loss=46680.86697879916, gradient=0.17410103918916167\n",
      "Current iteration=5600, the loss=46680.53374379661, gradient=0.17265758262292516\n",
      "Current iteration=5700, the loss=46680.21216511933, gradient=0.17131775991082093\n",
      "Current iteration=5800, the loss=46679.90158078894, gradient=0.1700742373487619\n",
      "Current iteration=5900, the loss=46679.60137448161, gradient=0.16892012468121195\n",
      "Current iteration=6000, the loss=46679.31097166175, gradient=0.1678489582096433\n",
      "Current iteration=6100, the loss=46679.029836108806, gradient=0.166854683306302\n",
      "Current iteration=6200, the loss=46678.75746679149, gradient=0.16593163649823675\n",
      "Current iteration=6300, the loss=46678.49339504946, gradient=0.16507452728512773\n",
      "Current iteration=6400, the loss=46678.23718204736, gradient=0.16427841984756586\n",
      "Current iteration=6500, the loss=46677.98841647096, gradient=0.16353871479169854\n",
      "Current iteration=6600, the loss=46677.74671243864, gradient=0.16285113106269017\n",
      "Current iteration=6700, the loss=46677.51170760456, gradient=0.16221168814438947\n",
      "Current iteration=6800, the loss=46677.28306143325, gradient=0.161616688646762\n",
      "Current iteration=6900, the loss=46677.06045362737, gradient=0.16106270136675072\n",
      "Current iteration=7000, the loss=46676.843582692396, gradient=0.16054654489276463\n",
      "Current iteration=7100, the loss=46676.63216462481, gradient=0.1600652718083843\n",
      "Current iteration=7200, the loss=46676.42593171056, gradient=0.1596161535373274\n",
      "Current iteration=7300, the loss=46676.22463142316, gradient=0.159196665859454\n",
      "Current iteration=7400, the loss=46676.028025411724, gradient=0.15880447511664258\n",
      "Current iteration=7500, the loss=46675.83588856998, gradient=0.15843742511778441\n",
      "Current iteration=7600, the loss=46675.64800817867, gradient=0.15809352474389432\n",
      "Current iteration=7700, the loss=46675.464183114425, gradient=0.15777093624733993\n",
      "Current iteration=7800, the loss=46675.28422311893, gradient=0.15746796423338955\n",
      "Current iteration=7900, the loss=46675.107948123165, gradient=0.15718304530754487\n",
      "Current iteration=8000, the loss=46674.93518762112, gradient=0.15691473836837846\n",
      "Current iteration=8100, the loss=46674.76578008954, gradient=0.15666171552269767\n",
      "Current iteration=8200, the loss=46674.599572448926, gradient=0.15642275359773636\n",
      "Current iteration=8300, the loss=46674.436419562655, gradient=0.15619672622358308\n",
      "Current iteration=8400, the loss=46674.276183770926, gradient=0.15598259645815207\n",
      "Current iteration=8500, the loss=46674.11873445668, gradient=0.15577940992654127\n",
      "Current iteration=8600, the loss=46673.96394764087, gradient=0.15558628844656927\n",
      "Current iteration=8700, the loss=46673.81170560455, gradient=0.15540242411255403\n",
      "Current iteration=8800, the loss=46673.6618965361, gradient=0.15522707380990222\n",
      "Current iteration=8900, the loss=46673.51441420118, gradient=0.1550595541338144\n",
      "Current iteration=9000, the loss=46673.369157633904, gradient=0.15489923668628486\n",
      "Current iteration=9100, the loss=46673.226030847574, gradient=0.15474554372657123\n",
      "Current iteration=9200, the loss=46673.084942563495, gradient=0.15459794415138411\n",
      "Current iteration=9300, the loss=46672.9458059567, gradient=0.15445594978217939\n",
      "Current iteration=9400, the loss=46672.80853841701, gradient=0.15431911193809048\n",
      "Current iteration=9500, the loss=46672.67306132485, gradient=0.15418701827421064\n",
      "Current iteration=9600, the loss=46672.5392998403, gradient=0.15405928986608933\n",
      "Current iteration=9700, the loss=46672.407182704614, gradient=0.15393557852246348\n",
      "Current iteration=9800, the loss=46672.27664205363, gradient=0.15381556430934656\n",
      "Current iteration=9900, the loss=46672.1476132418, gradient=0.15369895326968996\n",
      "Final loss=46672.02130347854\n"
     ]
    }
   ],
   "source": [
    "print(\"w0_nomass\")\n",
    "loss0_nomass, w0_nomass = logistic_regression(y0_nomass, jet0_nomass, gamma, n_iters)\n",
    "print(\"w0\")\n",
    "loss0, w0 = logistic_regression(y0, jet0, gamma, n_iters)\n",
    "\n",
    "print(\"w1_nomass\")\n",
    "loss1_nomass, w1_nomass = logistic_regression(y1_nomass, jet1_nomass, gamma, n_iters)\n",
    "print(\"w1\")\n",
    "loss1_nomass, w1 = logistic_regression(y1, jet1, gamma, n_iters)\n",
    "\n",
    "print(\"w23_nomass\")\n",
    "loss23_nomass, w23_nomass = logistic_regression(y23_nomass, jet23_nomass, gamma, n_iters)\n",
    "print(\"w23\")\n",
    "loss23, w23 = logistic_regression(y23, jet23, gamma, n_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = test_x[:, 0]\n",
    "testset = test_x[:, 2:] # remove id and prediction columns\n",
    "\n",
    "y = []\n",
    "\n",
    "for x_t in testset:\n",
    "    x = np.array([x_t])\n",
    "    if isJet0_nomass(x):\n",
    "        pred = x[:,selected_jet0_nomass]\n",
    "        pred, _ , _ = standardize(pred, min0_nomass, range0_nomass)\n",
    "        y.append(log_reg_predict(pred, w0_nomass))\n",
    "    elif isJet0(x):\n",
    "        pred = x[:, selected_jet0]\n",
    "        pred, _ , _  = standardize(pred, min0, range0)\n",
    "        y.append(log_reg_predict(pred, w0))\n",
    "    elif isJet1_nomass(x):\n",
    "        pred = x[:, selected_jet1_nomass]\n",
    "        pred, _ , _  = standardize(pred,min1_nomass, range1_nomass)\n",
    "        y.append(log_reg_predict(pred, w1_nomass))\n",
    "    elif isJet1(x):\n",
    "        pred = x[:, selected_jet1]\n",
    "        pred, _ , _  = standardize(pred, min1, range1)\n",
    "        y.append(log_reg_predict(pred, w1))\n",
    "    elif isJet23_nomass(x):\n",
    "        pred= x[:, selected_jet23_nomass]\n",
    "        pred, _ , _  = standardize(pred, min23_nomass, range23_nomass)\n",
    "        y.append(log_reg_predict(pred, w23_nomass))\n",
    "    else:\n",
    "        pred= x[:, selected_jet23]\n",
    "        pred, _ , _  = standardize(pred, min23, range23)\n",
    "        y.append(log_reg_predict(pred, w23))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y, 'data/split_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = test_x[:, 0]\n",
    "testset = test_x[:, 2:] # remove id and prediction columns\n",
    "\n",
    "jet0_nm_ids = np.where((testset[:,22] == 0) & (testset[:,0] == -999.))[0]\n",
    "jet0_ids = np.where((testset[:,22] == 0) & (testset[:,0] != -999.))[0]\n",
    "jet1_nm_ids = np.where((testset[:,22] == 1) & (testset[:,0] == -999.))[0]\n",
    "jet1_ids = np.where((testset[:,22] == 1) & (testset[:,0] != -999.))[0]\n",
    "jet23_nm_ids = np.where(((testset[:,22] == 2) | (testset[:,22] == 3)) \\\n",
    "                        & (testset[:,0] == -999.))[0]\n",
    "jet23_ids = np.where(((testset[:,22] == 2) | (testset[:,22] == 3)) \\\n",
    "                        & (testset[:,0] != -999.))[0]\n",
    "\n",
    "jet0_test_nm = testset[jet0_nm_ids][:, selected_jet0_nomass]\n",
    "jet0_test = testset[jet0_ids][:, selected_jet0]\n",
    "jet1_test_nm = testset[jet1_nm_ids][:, selected_jet1_nomass]\n",
    "jet1_test = testset[jet1_ids][:, selected_jet1]\n",
    "jet23_test_nm = testset[jet23_nm_ids][:, selected_jet23_nomass]\n",
    "jet23_test = testset[jet23_ids][:, selected_jet23]\n",
    "\n",
    "higgs_jet0_nm = np.where(log_reg_predict(jet0_test_nm, w0_nomass) == 1)[0]\n",
    "higgs_jet0 = np.where(log_reg_predict(jet0_test, w0) == 1)[0]\n",
    "higgs_jet1_nm = np.where(log_reg_predict(jet1_test_nm, w1_nomass) == 1)[0]\n",
    "higgs_jet1 = np.where(log_reg_predict(jet1_test, w1) == 1)[0]\n",
    "higgs_jet23_nm = np.where(log_reg_predict(jet23_test_nm, w23_nomass) == 1)[0]\n",
    "higgs_jet23 = np.where(log_reg_predict(jet23_test, w23) == 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve the id's of higgs boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "higgs = itertools.chain(ids[jet0_nm_ids][higgs_jet0_nm], ids[jet0_ids][higgs_jet0] \\\n",
    "               , ids[jet1_nm_ids][higgs_jet1_nm], ids[jet1_ids][higgs_jet1]\\\n",
    "               , ids[jet23_nm_ids][higgs_jet23_nm], ids[jet23_ids][higgs_jet23])\n",
    "\n",
    "higgs_arr = list([int(i) for i in higgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(350000,918238):\n",
    "    if i in higgs_arr:\n",
    "        res.append([i, 1])\n",
    "    else:\n",
    "        res.append([i, -1])\n",
    "        \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = apply_right_model(testset, ids, w0_nomass, w0, w1_nomass, ...\n",
    "                               w1, w23_nomass, w23, selected_jet0_nomass, ...\n",
    "                              selected_jet0, selected_jet1_nomass, selected_jet1, ...\n",
    "                              selected_jet23_nomass, selected_jet23)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"higgs: \", np.count_nonzero(prediction == 1))\n",
    "print(\"non-higgs: \", np.count_nonzero(prediction == -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)\n",
    "test_x = standardize(test_x[:, 2:])  # remove id and prediction columns\n",
    "# could've used load_csv_data\n",
    "create_csv_submission([i for i in range(350000,918238)], log_reg_predict(test_x, w), 'res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_jet0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "**Generate predictions and save ouput in csv format for submission**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

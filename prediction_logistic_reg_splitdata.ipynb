{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "from models import *\n",
    "from helpers import * \n",
    "from evaluation import *\n",
    "from split_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "** Load the training data into feature matrix, class labels, and record ids**\n",
    "\n",
    "We write our own `load_csv_data` function to import csv data, which gives us prediction column, feature matrix and each record ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split into 6 distinct datasets**\n",
    "According to our exploration, we can distinct 3 different dataset based on number of jets each experiments contains. Then each of them can be split again into 2 different datasets based on whether they have a measurable mass or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jet0, jet1, jet23, y0, y1, y23 = split_on_jets(y, tx)\n",
    "jet0_nomass, jet0, y0_nomass, y0 = split_on_mass(y0, jet0)\n",
    "jet1_nomass, jet1, y1_nomass, y1 = split_on_mass(y1, jet1)\n",
    "jet23_nomass, jet23, y23_nomass, y23 = split_on_mass(y23, jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Standardization of values**\n",
    "We use [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) method to standardize our feature matrix, i.e. to rescale tx down to [0, 1], so as to avoid complicated computation caused by large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx = standardize(tx)\n",
    "jet0_nomass = standardize(jet0_nomass)\n",
    "jet0 = standardize(jet0)\n",
    "jet1_nomass = standardize(jet1_nomass)\n",
    "jet1 = standardize(jet1)\n",
    "jet23_nomass = standardize(jet23_nomass)\n",
    "jet23 = standardize(jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a simple linear regression with least_square using **normal equations**. Here we don't consider using least squares with gradient descent or stochastic gradient descent for the fact that **optimal w could be derived thoeritically**. We therefore don't bother to estimate the w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run cross validation 4 times on our train_data to see LS performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose intial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iters = 2000\n",
    "gamma = 0.000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0_nomass\n",
      "Current iteration=0, the loss=18107.08379776745, gradient=0.24041936232171168\n",
      "Current iteration=100, the loss=18110.495402926295, gradient=0.2392022137219361\n",
      "Current iteration=200, the loss=18113.890301224466, gradient=0.23799122887558502\n",
      "Current iteration=300, the loss=18117.268571521756, gradient=0.23678637657804344\n",
      "Current iteration=400, the loss=18120.63029233654, gradient=0.23558762578271833\n",
      "Current iteration=500, the loss=18123.97554184695, gradient=0.23439494560024096\n",
      "Current iteration=600, the loss=18127.30439789197, gradient=0.2332083052976695\n",
      "Current iteration=700, the loss=18130.616937972623, gradient=0.23202767429769758\n",
      "Current iteration=800, the loss=18133.913239253096, gradient=0.23085302217786627\n",
      "Current iteration=900, the loss=18137.1933785619, gradient=0.2296843186697798\n",
      "Current iteration=1000, the loss=18140.457432393, gradient=0.22852153365832648\n",
      "Current iteration=1100, the loss=18143.705476906947, gradient=0.22736463718090183\n",
      "Current iteration=1200, the loss=18146.937587932065, gradient=0.22621359942663752\n",
      "Current iteration=1300, the loss=18150.15384096556, gradient=0.22506839073563148\n",
      "Current iteration=1400, the loss=18153.35431117467, gradient=0.22392898159818528\n",
      "Current iteration=1500, the loss=18156.539073397806, gradient=0.22279534265404347\n",
      "Current iteration=1600, the loss=18159.708202145717, gradient=0.2216674446916358\n",
      "Current iteration=1700, the loss=18162.86177160259, gradient=0.22054525864732605\n",
      "Current iteration=1800, the loss=18165.99985562724, gradient=0.2194287556046623\n",
      "Current iteration=1900, the loss=18169.12252775421, gradient=0.21831790679363208\n",
      "Final loss=18172.19886354743\n",
      "w0\n",
      "Current iteration=0, the loss=51147.33045351836, gradient=1.343197217037293\n",
      "Current iteration=100, the loss=51168.838234444374, gradient=1.3363161244628317\n",
      "Current iteration=200, the loss=51190.28645693719, gradient=1.3294702844882607\n",
      "Current iteration=300, the loss=51211.67490858485, gradient=1.322659516517348\n",
      "Current iteration=400, the loss=51233.00338328651, gradient=1.3158836408790735\n",
      "Current iteration=500, the loss=51254.27168116914, gradient=1.3091424788228916\n",
      "Current iteration=600, the loss=51275.479608505106, gradient=1.3024358525140207\n",
      "Current iteration=700, the loss=51296.62697763058, gradient=1.2957635850287446\n",
      "Current iteration=800, the loss=51317.71360686484, gradient=1.2891255003497442\n",
      "Current iteration=900, the loss=51338.73932043046, gradient=1.2825214233614637\n",
      "Current iteration=1000, the loss=51359.70394837436, gradient=1.2759511798454837\n",
      "Current iteration=1100, the loss=51380.60732648955, gradient=1.2694145964759211\n",
      "Current iteration=1200, the loss=51401.44929623803, gradient=1.26291150081487\n",
      "Current iteration=1300, the loss=51422.22970467416, gradient=1.2564417213078376\n",
      "Current iteration=1400, the loss=51442.94840436905, gradient=1.250005087279231\n",
      "Current iteration=1500, the loss=51463.60525333586, gradient=1.2436014289278468\n",
      "Current iteration=1600, the loss=51484.20011495558, gradient=1.2372305773223944\n",
      "Current iteration=1700, the loss=51504.73285790401, gradient=1.230892364397041\n",
      "Current iteration=1800, the loss=51525.203356079204, gradient=1.2245866229469735\n",
      "Current iteration=1900, the loss=51545.61148852995, gradient=1.2183131866239956\n",
      "Final loss=51565.75399252825\n",
      "w1_nomass\n",
      "Current iteration=0, the loss=5241.578979394306, gradient=0.38818236722804383\n",
      "Current iteration=100, the loss=5243.093177419943, gradient=0.38610652763636727\n",
      "Current iteration=200, the loss=5244.599729269215, gradient=0.3840417911672859\n",
      "Current iteration=300, the loss=5246.098671011609, gradient=0.38198809844593046\n",
      "Current iteration=400, the loss=5247.590038575115, gradient=0.3799453904150131\n",
      "Current iteration=500, the loss=5249.073867746428, gradient=0.3779136083331271\n",
      "Current iteration=600, the loss=5250.550194171166, gradient=0.37589269377305906\n",
      "Current iteration=700, the loss=5252.019053354095, gradient=0.3738825886201124\n",
      "Current iteration=800, the loss=5253.480480659346, gradient=0.3718832350704276\n",
      "Current iteration=900, the loss=5254.934511310647, gradient=0.3698945756293283\n",
      "Current iteration=1000, the loss=5256.381180391556, gradient=0.36791655310966376\n",
      "Current iteration=1100, the loss=5257.820522845695, gradient=0.3659491106301651\n",
      "Current iteration=1200, the loss=5259.252573476994, gradient=0.3639921916138096\n",
      "Current iteration=1300, the loss=5260.677366949936, gradient=0.36204573978619414\n",
      "Current iteration=1400, the loss=5262.094937789804, gradient=0.3601096991739178\n",
      "Current iteration=1500, the loss=5263.5053203829375, gradient=0.35818401410296713\n",
      "Current iteration=1600, the loss=5264.908548976988, gradient=0.3562686291971238\n",
      "Current iteration=1700, the loss=5266.304657681176, gradient=0.3543634893763646\n",
      "Current iteration=1800, the loss=5267.693680466562, gradient=0.3524685398552817\n",
      "Current iteration=1900, the loss=5269.075651166312, gradient=0.3505837261415055\n",
      "Final loss=5270.436888583597\n",
      "w1\n",
      "Current iteration=0, the loss=48507.82598994609, gradient=1.6331733745291879\n",
      "Current iteration=100, the loss=48524.678945028325, gradient=1.6243270734696165\n",
      "Current iteration=200, the loss=48541.51561720398, gradient=1.6155286923558618\n",
      "Current iteration=300, the loss=48558.33528397391, gradient=1.6067779716232213\n",
      "Current iteration=400, the loss=48575.13723535843, gradient=1.5980746531130516\n",
      "Current iteration=500, the loss=48591.92077374236, gradient=1.589418480065133\n",
      "Current iteration=600, the loss=48608.68521372169, gradient=1.5808091971101101\n",
      "Current iteration=700, the loss=48625.429881951924, gradient=1.5722465502619494\n",
      "Current iteration=800, the loss=48642.15411699783, gradient=1.563730286910447\n",
      "Current iteration=900, the loss=48658.85726918499, gradient=1.5552601558137806\n",
      "Current iteration=1000, the loss=48675.538700452715, gradient=1.546835907091091\n",
      "Current iteration=1100, the loss=48692.19778420864, gradient=1.538457292215116\n",
      "Current iteration=1200, the loss=48708.83390518488, gradient=1.5301240640048577\n",
      "Current iteration=1300, the loss=48725.446459295505, gradient=1.5218359766182858\n",
      "Current iteration=1400, the loss=48742.03485349574, gradient=1.5135927855450906\n",
      "Current iteration=1500, the loss=48758.59850564264, gradient=1.5053942475994668\n",
      "Current iteration=1600, the loss=48775.13684435712, gradient=1.497240120912939\n",
      "Current iteration=1700, the loss=48791.64930888753, gradient=1.4891301649272273\n",
      "Current iteration=1800, the loss=48808.13534897479, gradient=1.4810641403871476\n",
      "Current iteration=1900, the loss=48824.594424718794, gradient=1.4730418093335595\n",
      "Final loss=48840.86182843885\n",
      "w23_nomass\n",
      "Current iteration=0, the loss=3069.948862699998, gradient=0.4858069173125286\n",
      "Current iteration=100, the loss=3071.01614881252, gradient=0.48310052919756896\n",
      "Current iteration=200, the loss=3072.0778944529166, gradient=0.4804093314384033\n",
      "Current iteration=300, the loss=3073.1341257859535, gradient=0.4777332394072218\n",
      "Current iteration=400, the loss=3074.184868883722, gradient=0.4750721689511354\n",
      "Current iteration=500, the loss=3075.2301497255617, gradient=0.47242603638953035\n",
      "Current iteration=600, the loss=3076.2699941979936, gradient=0.46979475851143576\n",
      "Current iteration=700, the loss=3077.304428094652, gradient=0.467178252572905\n",
      "Current iteration=800, the loss=3078.3334771162354, gradient=0.4645764362944137\n",
      "Current iteration=900, the loss=3079.3571668704494, gradient=0.4619892278582708\n",
      "Current iteration=1000, the loss=3080.3755228719706, gradient=0.45941654590604547\n",
      "Current iteration=1100, the loss=3081.388570542405, gradient=0.4568583095360067\n",
      "Current iteration=1200, the loss=3082.396335210262, gradient=0.45431443830057955\n",
      "Current iteration=1300, the loss=3083.3988421109266, gradient=0.45178485220381204\n",
      "Current iteration=1400, the loss=3084.3961163866434, gradient=0.4492694716988607\n",
      "Current iteration=1500, the loss=3085.388183086508, gradient=0.44676821768548775\n",
      "Current iteration=1600, the loss=3086.375067166453, gradient=0.4442810115075721\n",
      "Current iteration=1700, the loss=3087.356793489258, gradient=0.44180777495063317\n",
      "Current iteration=1800, the loss=3088.3333868245477, gradient=0.4393484302393732\n",
      "Current iteration=1900, the loss=3089.3048718488053, gradient=0.43690290003522897\n",
      "Final loss=3090.261634216388\n",
      "w23\n",
      "Current iteration=0, the loss=47213.02705666011, gradient=0.8005159752682665\n",
      "Current iteration=100, the loss=47214.41279517024, gradient=0.7997510382413252\n",
      "Current iteration=200, the loss=47215.80020623036, gradient=0.7989868482697551\n",
      "Current iteration=300, the loss=47217.18928220107, gradient=0.7982234046376919\n",
      "Current iteration=400, the loss=47218.580015461725, gradient=0.7974607066299649\n",
      "Current iteration=500, the loss=47219.97239841033, gradient=0.7966987535321045\n",
      "Current iteration=600, the loss=47221.36642346363, gradient=0.7959375446303336\n",
      "Current iteration=700, the loss=47222.76208305692, gradient=0.7951770792115687\n",
      "Current iteration=800, the loss=47224.159369644105, gradient=0.7944173565634277\n",
      "Current iteration=900, the loss=47225.558275697644, gradient=0.7936583759742122\n",
      "Current iteration=1000, the loss=47226.9587937085, gradient=0.7929001367329206\n",
      "Current iteration=1100, the loss=47228.360916186095, gradient=0.7921426381292455\n",
      "Current iteration=1200, the loss=47229.7646356583, gradient=0.791385879453566\n",
      "Current iteration=1300, the loss=47231.16994467139, gradient=0.7906298599969593\n",
      "Current iteration=1400, the loss=47232.57683578997, gradient=0.7898745790511859\n",
      "Current iteration=1500, the loss=47233.985301597, gradient=0.7891200359086977\n",
      "Current iteration=1600, the loss=47235.39533469369, gradient=0.7883662298626379\n",
      "Current iteration=1700, the loss=47236.806927699545, gradient=0.7876131602068348\n",
      "Current iteration=1800, the loss=47238.22007325223, gradient=0.7868608262358039\n",
      "Current iteration=1900, the loss=47239.634764007606, gradient=0.7861092272447506\n",
      "Final loss=47241.03682276485\n"
     ]
    }
   ],
   "source": [
    "print(\"w0_nomass\")\n",
    "loss0_nomass, w0_nomass = logistic_regression(y0_nomass, jet0_nomass, gamma, n_iters)\n",
    "print(\"w0\")\n",
    "loss0, w0 = logistic_regression(y0, jet0, gamma, n_iters)\n",
    "\n",
    "print(\"w1_nomass\")\n",
    "loss1_nomass, w1_nomass = logistic_regression(y1_nomass, jet1_nomass, gamma, n_iters)\n",
    "print(\"w1\")\n",
    "loss1_nomass, w1 = logistic_regression(y1, jet1, gamma, n_iters)\n",
    "\n",
    "print(\"w23_nomass\")\n",
    "loss23_nomass, w23_nomass = logistic_regression(y23_nomass, jet23_nomass, gamma, n_iters)\n",
    "print(\"w23\")\n",
    "loss23, w23 = logistic_regression(y23, jet23, gamma, n_iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation(y, tx, 4, 0, logistic_regression, 0.000003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = test_x[:, 0]\n",
    "testset = standardize(test_x[:, 2:])  # remove id and prediction columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.50000000e+05,   1.00000000e+00],\n",
       "       [  3.50001000e+05,   1.00000000e+00],\n",
       "       [  3.50002000e+05,   1.00000000e+00],\n",
       "       ..., \n",
       "       [  9.18235000e+05,   1.00000000e+00],\n",
       "       [  9.18236000e+05,   1.00000000e+00],\n",
       "       [  9.18237000e+05,   1.00000000e+00]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = apply_right_model(testset, ids, w0_nomass, w0, w1_nomass, w1, w23_nomass, w23)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higgs:  568238\n",
      "non-higgs:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"higgs: \", np.count_nonzero(prediction == 1))\n",
    "print(\"non-higgs: \", np.count_nonzero(prediction == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)\n",
    "test_x = standardize(test_x[:, 2:])  # remove id and prediction columns\n",
    "# could've used load_csv_data\n",
    "create_csv_submission([i for i in range(350000,918238)], log_reg_predict(test_x, w), 'res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "**Generate predictions and save ouput in csv format for submission**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

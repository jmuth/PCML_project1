{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from costs import *\n",
    "from models import *\n",
    "from helpers import * \n",
    "from evaluation import *\n",
    "from split_data import *\n",
    "from split_features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "** Load the training data into feature matrix, class labels, and record ids**\n",
    "\n",
    "We write our own `load_csv_data` function to import csv data, which gives us prediction column, feature matrix and each record ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tx, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into trainset and testset in order to test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = split_data(tx, y, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225000, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx = x_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into 6 distinct datasets\n",
    "\n",
    "According to our exploration, we can distinct 3 different dataset based on number of jets each experiments contains. Then each of them can be split again into 2 different datasets based on whether they have a measurable mass or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet0, jet1, jet23, y0, y1, y23 = split_on_jets(y, tx)\n",
    "jet0_nomass, jet0, y0_nomass, y0 = split_on_mass(y0, jet0)\n",
    "jet1_nomass, jet1, y1_nomass, y1 = split_on_mass(y1, jet1)\n",
    "jet23_nomass, jet23, y23_nomass, y23 = split_on_mass(y23, jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep the features without NaN in each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet0_nomass, selected_jet0_nomass = select_features_without_nan(jet0_nomass)\n",
    "jet0, selected_jet0 = select_features_without_nan(jet0)\n",
    "jet1_nomass, selected_jet1_nomass = select_features_without_nan(jet1_nomass)\n",
    "jet1, selected_jet1 = select_features_without_nan(jet1)\n",
    "jet23_nomass, selected_jet23_nomass = select_features_without_nan(jet23_nomass)\n",
    "jet23, selected_jet23 = select_features_without_nan(jet23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: remove more features ! some are flat, some are even unique !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAFwCAYAAAD9gEnDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYEjkAAAAACAwJHIAAAAAEJj/DyG6pXY0nAvVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131ed08d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot_five_number(jet0_nomass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of values\n",
    "We use [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) method to standardize our feature matrix, i.e. to rescale tx down to [0, 1], so as to avoid complicated computation caused by large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tx = standardize(tx)\n",
    "std_jet0_nomass, min0_nomass, range0_nomass = standardize(jet0_nomass)\n",
    "std_jet0, min0, range0 = standardize(jet0)\n",
    "std_jet1_nomass, min1_nomass, range1_nomass = standardize(jet1_nomass)\n",
    "std_jet1, min1, range1 = standardize(jet1)\n",
    "std_jet23_nomass, min23_nomass, range23_nomass = standardize(jet23_nomass)\n",
    "std_jet23, min23, range23 = standardize(jet23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose intial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "gamma = 0.000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0_nomass\n",
      "Current iteration=0, the loss=16320.150366283911, gradient=0.07172256268592511\n",
      "Current iteration=500, the loss=16321.472419235455, gradient=0.07156450444979548\n",
      "Current iteration=1000, the loss=16322.79163101393, gradient=0.0714068007823273\n",
      "Current iteration=1500, the loss=16324.10800749434, gradient=0.07124945090170577\n",
      "Current iteration=2000, the loss=16325.421554540579, gradient=0.07109245402786779\n",
      "Current iteration=2500, the loss=16326.732278005476, gradient=0.07093580938249835\n",
      "Current iteration=3000, the loss=16328.040183730782, gradient=0.07077951618902632\n",
      "Current iteration=3500, the loss=16329.345277547192, gradient=0.0706235736726209\n",
      "Current iteration=4000, the loss=16330.647565274374, gradient=0.07046798106018756\n",
      "Current iteration=4500, the loss=16331.94705272098, gradient=0.07031273758036433\n",
      "Current iteration=5000, the loss=16333.243745684658, gradient=0.07015784246351797\n",
      "Current iteration=5500, the loss=16334.537649952066, gradient=0.07000329494173996\n",
      "Current iteration=6000, the loss=16335.828771298893, gradient=0.06984909424884299\n",
      "Current iteration=6500, the loss=16337.11711548987, gradient=0.06969523962035688\n",
      "Current iteration=7000, the loss=16338.402688278793, gradient=0.06954173029352487\n",
      "Current iteration=7500, the loss=16339.685495408534, gradient=0.06938856550729994\n",
      "Current iteration=8000, the loss=16340.96554261106, gradient=0.06923574450234095\n",
      "Current iteration=8500, the loss=16342.242835607447, gradient=0.06908326652100877\n",
      "Current iteration=9000, the loss=16343.517380107893, gradient=0.06893113080736268\n",
      "Current iteration=9500, the loss=16344.789181811737, gradient=0.06877933660715652\n",
      "Final loss=16346.055711006153\n",
      "w0\n",
      "Current iteration=0, the loss=46031.21111380541, gradient=0.42243943012781965\n",
      "Current iteration=500, the loss=46040.56115700013, gradient=0.4213471414768498\n",
      "Current iteration=1000, the loss=46049.89810246041, gradient=0.4202577042493188\n",
      "Current iteration=1500, the loss=46059.22192491174, gradient=0.4191711110683676\n",
      "Current iteration=2000, the loss=46068.53259944887, gradient=0.41808735457638135\n",
      "Current iteration=2500, the loss=46077.830101533225, gradient=0.41700642743494004\n",
      "Current iteration=3000, the loss=46087.11440699057, gradient=0.4159283223247689\n",
      "Current iteration=3500, the loss=46096.38549200859, gradient=0.41485303194568807\n",
      "Current iteration=4000, the loss=46105.64333313434, gradient=0.41378054901656436\n",
      "Current iteration=4500, the loss=46114.887907271994, gradient=0.41271086627525966\n",
      "Current iteration=5000, the loss=46124.119191680395, gradient=0.4116439764785852\n",
      "Current iteration=5500, the loss=46133.33716397063, gradient=0.4105798724022486\n",
      "Current iteration=6000, the loss=46142.54180210378, gradient=0.409518546840809\n",
      "Current iteration=6500, the loss=46151.73308438844, gradient=0.40845999260762383\n",
      "Current iteration=7000, the loss=46160.910989478485, gradient=0.4074042025348037\n",
      "Current iteration=7500, the loss=46170.07549637068, gradient=0.4063511694731625\n",
      "Current iteration=8000, the loss=46179.226584402364, gradient=0.40530088629216876\n",
      "Current iteration=8500, the loss=46188.3642332492, gradient=0.40425334587989764\n",
      "Current iteration=9000, the loss=46197.48842292286, gradient=0.4032085411429824\n",
      "Current iteration=9500, the loss=46206.59913376871, gradient=0.4021664650065678\n",
      "Final loss=46215.678165522026\n",
      "w1_nomass\n",
      "Current iteration=0, the loss=4721.718593974347, gradient=0.1335938358288998\n",
      "Current iteration=500, the loss=4722.506755208606, gradient=0.13317653375188113\n",
      "Current iteration=1000, the loss=4723.292572113913, gradient=0.13276055599733452\n",
      "Current iteration=1500, the loss=4724.07605118242, gradient=0.1323458984265123\n",
      "Current iteration=2000, the loss=4724.857198891425, gradient=0.13193255691378802\n",
      "Current iteration=2500, the loss=4725.636021703382, gradient=0.13152052734661632\n",
      "Current iteration=3000, the loss=4726.412526065913, gradient=0.13110980562549124\n",
      "Current iteration=3500, the loss=4727.186718411827, gradient=0.13070038766390552\n",
      "Current iteration=4000, the loss=4727.9586051591195, gradient=0.1302922693883102\n",
      "Current iteration=4500, the loss=4728.728192710998, gradient=0.12988544673807392\n",
      "Current iteration=5000, the loss=4729.495487455889, gradient=0.1294799156654416\n",
      "Current iteration=5500, the loss=4730.260495767447, gradient=0.1290756721354955\n",
      "Current iteration=6000, the loss=4731.023224004574, gradient=0.12867271212611409\n",
      "Current iteration=6500, the loss=4731.783678511433, gradient=0.12827103162793219\n",
      "Current iteration=7000, the loss=4732.541865617453, gradient=0.12787062664430118\n",
      "Current iteration=7500, the loss=4733.297791637351, gradient=0.12747149319124898\n",
      "Current iteration=8000, the loss=4734.051462871142, gradient=0.1270736272974403\n",
      "Current iteration=8500, the loss=4734.802885604154, gradient=0.1266770250041372\n",
      "Current iteration=9000, the loss=4735.55206610704, gradient=0.12628168236515974\n",
      "Current iteration=9500, the loss=4736.299010635796, gradient=0.1258875954468461\n",
      "Final loss=4737.042238223309\n",
      "w1\n",
      "Current iteration=0, the loss=43632.22872188743, gradient=0.6148890348711071\n",
      "Current iteration=500, the loss=43641.29982281323, gradient=0.6126259839001562\n",
      "Current iteration=1000, the loss=43650.36951868014, gradient=0.6103713143801993\n",
      "Current iteration=1500, the loss=43659.437574419615, gradient=0.6081249954443158\n",
      "Current iteration=2000, the loss=43668.503757566505, gradient=0.6058869963398301\n",
      "Current iteration=2500, the loss=43677.56783823756, gradient=0.6036572864278907\n",
      "Current iteration=3000, the loss=43686.6295891101, gradient=0.6014358351830517\n",
      "Current iteration=3500, the loss=43695.68878540091, gradient=0.5992226121928524\n",
      "Current iteration=4000, the loss=43704.7452048452, gradient=0.5970175871574035\n",
      "Current iteration=4500, the loss=43713.79862767574, gradient=0.5948207298889706\n",
      "Current iteration=5000, the loss=43722.848836602105, gradient=0.5926320103115619\n",
      "Current iteration=5500, the loss=43731.895616790134, gradient=0.5904513984605144\n",
      "Current iteration=6000, the loss=43740.93875584149, gradient=0.5882788644820867\n",
      "Current iteration=6500, the loss=43749.978043773364, gradient=0.5861143786330476\n",
      "Current iteration=7000, the loss=43759.0132729983, gradient=0.5839579112802696\n",
      "Current iteration=7500, the loss=43768.04423830419, gradient=0.5818094329003239\n",
      "Current iteration=8000, the loss=43777.07073683448, gradient=0.5796689140790756\n",
      "Current iteration=8500, the loss=43786.092568068314, gradient=0.5775363255112809\n",
      "Current iteration=9000, the loss=43795.10953380107, gradient=0.5754116380001846\n",
      "Current iteration=9500, the loss=43804.12143812481, gradient=0.5732948224571228\n",
      "Final loss=43813.11007948258\n",
      "w23_nomass\n",
      "Current iteration=0, the loss=2763.577808892502, gradient=0.23957116365259692\n",
      "Current iteration=500, the loss=2764.4961793906914, gradient=0.23842898318719416\n",
      "Current iteration=1000, the loss=2765.410322890264, gradient=0.2372926876762374\n",
      "Current iteration=1500, the loss=2766.320256410268, gradient=0.23616224868038457\n",
      "Current iteration=2000, the loss=2767.225996924813, gradient=0.23503763790355575\n",
      "Current iteration=2500, the loss=2768.1275613629045, gradient=0.23391882719220786\n",
      "Current iteration=3000, the loss=2769.0249666082855, gradient=0.23280578853461456\n",
      "Current iteration=3500, the loss=2769.91822949927, gradient=0.23169849406014695\n",
      "Current iteration=4000, the loss=2770.807366828599, gradient=0.23059691603856014\n",
      "Current iteration=4500, the loss=2771.6923953432843, gradient=0.22950102687928034\n",
      "Current iteration=5000, the loss=2772.57333174447, gradient=0.22841079913069778\n",
      "Current iteration=5500, the loss=2773.4501926872904, gradient=0.22732620547946122\n",
      "Current iteration=6000, the loss=2774.3229947807317, gradient=0.2262472187497757\n",
      "Current iteration=6500, the loss=2775.191754587505, gradient=0.2251738119027047\n",
      "Current iteration=7000, the loss=2776.056488623916, gradient=0.22410595803547384\n",
      "Current iteration=7500, the loss=2776.917213359741, gradient=0.22304363038077887\n",
      "Current iteration=8000, the loss=2777.7739452181117, gradient=0.22198680230609671\n",
      "Current iteration=8500, the loss=2778.6267005753953, gradient=0.22093544731299822\n",
      "Current iteration=9000, the loss=2779.475495761088, gradient=0.21988953903646644\n",
      "Current iteration=9500, the loss=2780.3203470577023, gradient=0.2188490512442157\n",
      "Final loss=2781.1595927624144\n",
      "w23\n",
      "Current iteration=0, the loss=42489.229021144085, gradient=0.9183366948734534\n",
      "Current iteration=500, the loss=42491.82609656081, gradient=0.9132851451019144\n",
      "Current iteration=1000, the loss=42494.51048015765, gradient=0.90826182616532\n",
      "Current iteration=1500, the loss=42497.28052229401, gradient=0.9032665824561642\n",
      "Current iteration=2000, the loss=42500.1345950181, gradient=0.8982992592345864\n",
      "Current iteration=2500, the loss=42503.07109182243, gradient=0.8933597026235719\n",
      "Current iteration=3000, the loss=42506.08842740152, gradient=0.8884477596041817\n",
      "Current iteration=3500, the loss=42509.185037411946, gradient=0.883563278010805\n",
      "Current iteration=4000, the loss=42512.35937823466, gradient=0.8787061065264437\n",
      "Current iteration=4500, the loss=42515.609926739635, gradient=0.8738760946780162\n",
      "Current iteration=5000, the loss=42518.9351800527, gradient=0.8690730928316917\n",
      "Current iteration=5500, the loss=42522.333655324685, gradient=0.8642969521882506\n",
      "Current iteration=6000, the loss=42525.80388950272, gradient=0.8595475247784676\n",
      "Current iteration=6500, the loss=42529.3444391039, gradient=0.8548246634585224\n",
      "Current iteration=7000, the loss=42532.95387999095, gradient=0.8501282219054351\n",
      "Current iteration=7500, the loss=42536.63080715028, gradient=0.8454580546125293\n",
      "Current iteration=8000, the loss=42540.3738344721, gradient=0.8408140168849144\n",
      "Current iteration=8500, the loss=42544.1815945327, gradient=0.836195964834999\n",
      "Current iteration=9000, the loss=42548.05273837902, gradient=0.8316037553780284\n",
      "Current iteration=9500, the loss=42551.98593531507, gradient=0.8270372462276424\n",
      "Final loss=42555.971825060675\n"
     ]
    }
   ],
   "source": [
    "print(\"w0_nomass\")\n",
    "loss0_nomass, w0_nomass = logistic_regression(y0_nomass, std_jet0_nomass, gamma, n_iters)\n",
    "print(\"w0\")\n",
    "loss0, w0 = logistic_regression(y0, std_jet0, gamma, n_iters)\n",
    "\n",
    "print(\"w1_nomass\")\n",
    "loss1_nomass, w1_nomass = logistic_regression(y1_nomass, std_jet1_nomass, gamma, n_iters)\n",
    "print(\"w1\")\n",
    "loss1_nomass, w1 = logistic_regression(y1, std_jet1, gamma, n_iters)\n",
    "\n",
    "print(\"w23_nomass\")\n",
    "loss23_nomass, w23_nomass = logistic_regression(y23_nomass, std_jet23_nomass, gamma, n_iters)\n",
    "print(\"w23\")\n",
    "loss23, w23 = logistic_regression(y23, std_jet23, gamma, n_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy testing against virtual test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset = x_test\n",
    "\n",
    "y_pred = apply_right_model(testset, \\\n",
    "                  selected_jet0_nomass, selected_jet0, selected_jet1_nomass, selected_jet1, \\\n",
    "                  selected_jet23_nomass, selected_jet23, \\\n",
    "                  min0_nomass, min0, min1_nomass, min1, min23_nomass, min23, \\\n",
    "                  range0_nomass, range0, range1_nomass, range1, range23_nomass, range23, \\\n",
    "                  w0_nomass, w0, w1_nomass, w1, w23_nomass, w23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79532"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output prediction to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = test_x[:, 0]\n",
    "testset = test_x[:, 2:] # remove id and prediction columns\n",
    "\n",
    "y = []\n",
    "\n",
    "for x_t in testset:\n",
    "    x = np.array([x_t])\n",
    "    if isJet0_nomass(x):\n",
    "        pred = x[:,selected_jet0_nomass]\n",
    "        pred, _ , _ = standardize(pred, min0_nomass, range0_nomass)\n",
    "        y.append(log_reg_predict(pred, w0_nomass))\n",
    "    elif isJet0(x):\n",
    "        pred = x[:, selected_jet0]\n",
    "        pred, _ , _  = standardize(pred, min0, range0)\n",
    "        y.append(log_reg_predict(pred, w0))\n",
    "    elif isJet1_nomass(x):\n",
    "        pred = x[:, selected_jet1_nomass]\n",
    "        pred, _ , _  = standardize(pred,min1_nomass, range1_nomass)\n",
    "        y.append(log_reg_predict(pred, w1_nomass))\n",
    "    elif isJet1(x):\n",
    "        pred = x[:, selected_jet1]\n",
    "        pred, _ , _  = standardize(pred, min1, range1)\n",
    "        y.append(log_reg_predict(pred, w1))\n",
    "    elif isJet23_nomass(x):\n",
    "        pred= x[:, selected_jet23_nomass]\n",
    "        pred, _ , _  = standardize(pred, min23_nomass, range23_nomass)\n",
    "        y.append(log_reg_predict(pred, w23_nomass))\n",
    "    else:\n",
    "        pred= x[:, selected_jet23]\n",
    "        pred, _ , _  = standardize(pred, min23, range23)\n",
    "        y.append(log_reg_predict(pred, w23))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y, 'data/split_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = test_x[:, 0]\n",
    "testset = test_x[:, 2:] # remove id and prediction columns\n",
    "\n",
    "jet0_nm_ids = np.where((testset[:,22] == 0) & (testset[:,0] == -999.))[0]\n",
    "jet0_ids = np.where((testset[:,22] == 0) & (testset[:,0] != -999.))[0]\n",
    "jet1_nm_ids = np.where((testset[:,22] == 1) & (testset[:,0] == -999.))[0]\n",
    "jet1_ids = np.where((testset[:,22] == 1) & (testset[:,0] != -999.))[0]\n",
    "jet23_nm_ids = np.where(((testset[:,22] == 2) | (testset[:,22] == 3)) \\\n",
    "                        & (testset[:,0] == -999.))[0]\n",
    "jet23_ids = np.where(((testset[:,22] == 2) | (testset[:,22] == 3)) \\\n",
    "                        & (testset[:,0] != -999.))[0]\n",
    "\n",
    "jet0_test_nm = testset[jet0_nm_ids][:, selected_jet0_nomass]\n",
    "jet0_test = testset[jet0_ids][:, selected_jet0]\n",
    "jet1_test_nm = testset[jet1_nm_ids][:, selected_jet1_nomass]\n",
    "jet1_test = testset[jet1_ids][:, selected_jet1]\n",
    "jet23_test_nm = testset[jet23_nm_ids][:, selected_jet23_nomass]\n",
    "jet23_test = testset[jet23_ids][:, selected_jet23]\n",
    "\n",
    "higgs_jet0_nm = np.where(log_reg_predict(jet0_test_nm, w0_nomass) == 1)[0]\n",
    "higgs_jet0 = np.where(log_reg_predict(jet0_test, w0) == 1)[0]\n",
    "higgs_jet1_nm = np.where(log_reg_predict(jet1_test_nm, w1_nomass) == 1)[0]\n",
    "higgs_jet1 = np.where(log_reg_predict(jet1_test, w1) == 1)[0]\n",
    "higgs_jet23_nm = np.where(log_reg_predict(jet23_test_nm, w23_nomass) == 1)[0]\n",
    "higgs_jet23 = np.where(log_reg_predict(jet23_test, w23) == 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve the id's of higgs boson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "higgs = itertools.chain(ids[jet0_nm_ids][higgs_jet0_nm], ids[jet0_ids][higgs_jet0] \\\n",
    "               , ids[jet1_nm_ids][higgs_jet1_nm], ids[jet1_ids][higgs_jet1]\\\n",
    "               , ids[jet23_nm_ids][higgs_jet23_nm], ids[jet23_ids][higgs_jet23])\n",
    "\n",
    "higgs_arr = list([int(i) for i in higgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(350000,918238):\n",
    "    if i in higgs_arr:\n",
    "        res.append([i, 1])\n",
    "    else:\n",
    "        res.append([i, -1])\n",
    "        \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = apply_right_model(testset, ids, w0_nomass, w0, w1_nomass, ...\n",
    "                               w1, w23_nomass, w23, selected_jet0_nomass, ...\n",
    "                              selected_jet0, selected_jet1_nomass, selected_jet1, ...\n",
    "                              selected_jet23_nomass, selected_jet23)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"higgs: \", np.count_nonzero(prediction == 1))\n",
    "print(\"non-higgs: \", np.count_nonzero(prediction == -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.genfromtxt('data/test.csv', delimiter=',', skip_header=1)\n",
    "test_x = standardize(test_x[:, 2:])  # remove id and prediction columns\n",
    "# could've used load_csv_data\n",
    "create_csv_submission([i for i in range(350000,918238)], log_reg_predict(test_x, w), 'res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_jet0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "**Generate predictions and save ouput in csv format for submission**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
